{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602e667f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:49:52.141673Z",
     "start_time": "2022-07-28T13:49:52.107837Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747276a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T14:23:24.518540Z",
     "start_time": "2022-07-28T14:23:24.341947Z"
    }
   },
   "outputs": [],
   "source": [
    "import gfootball.env as football_env\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "clipping_val = 0.2\n",
    "critic_discount = 0.5\n",
    "entropy_beta = 0.001\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "    \n",
    "state = env.reset()\n",
    "state_dims = env.observation_space.shape\n",
    "\n",
    "state_size = state_dims\n",
    "n_actions = env.action_space.n\n",
    "action_space = n_actions\n",
    "input_shape = env.observation_space.shape\n",
    "\n",
    "dummy_n = np.zeros((1, 1, n_actions))\n",
    "dummy_1 = np.zeros((1, 1, 1))\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='./logs/')\n",
    "\n",
    "\n",
    "target_reached = False\n",
    "best_reward = 0\n",
    "iters = 0\n",
    "max_iters = 50\n",
    "training_batch=1000\n",
    "episode = 0\n",
    "replay_count = 0\n",
    "shuffle = False\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "def get_advantages(values, masks, rewards):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * lmbda * masks[i] * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "\n",
    "def critic_PPO2_loss(values):\n",
    "    def loss(y_true, y_pred):\n",
    "        LOSS_CLIPPING = clipping_val\n",
    "        clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "        v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "        v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "        value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "        #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "        return value_loss\n",
    "    return loss\n",
    "\n",
    "def critic_ppo_loss(y_true, y_pred):\n",
    "    value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "    return value_loss\n",
    "\n",
    "def actor_ppo_loss(y_true, y_pred):\n",
    "    # Defined in https://arxiv.org/abs/1707.06347\n",
    "    advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+action_space], y_true[:, 1+action_space:]\n",
    "    LOSS_CLIPPING = clipping_val\n",
    "    ENTROPY_LOSS = entropy_beta\n",
    "        \n",
    "    prob = actions * y_pred\n",
    "    old_prob = actions * prediction_picks\n",
    "\n",
    "    prob = K.clip(prob, 1e-10, 1.0)\n",
    "    old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "    ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "    p1 = ratio * advantages\n",
    "    p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "    actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "    entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "    entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "    total_loss = actor_loss - entropy\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def actor_ppo_loss_continuous(y_true, y_pred):\n",
    "    advantages, actions, logp_old_ph, = y_true[:, :1], y_true[:, 1:1+action_space], y_true[:, 1+action_space]\n",
    "    LOSS_CLIPPING = clipping_val\n",
    "    logp = gaussian_likelihood(actions, y_pred)\n",
    "\n",
    "    ratio = K.exp(logp - logp_old_ph)\n",
    "\n",
    "    p1 = ratio * advantages\n",
    "    p2 = tf.where(advantages > 0, (1.0 + LOSS_CLIPPING)*advantages, (1.0 - LOSS_CLIPPING)*advantages) # minimum advantage\n",
    "\n",
    "    actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "    return actor_loss\n",
    "\n",
    "def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "    log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "    pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "    return K.sum(pre_sum, axis=1)\n",
    "    \n",
    "def get_common_layer(X_input, model=\"MLP\"):\n",
    "    # Shared CNN layers:\n",
    "    if model==\"CNN\":\n",
    "        X = Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")(X_input)\n",
    "        X = MaxPooling1D(pool_size=2)(X)\n",
    "        X = Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(X)\n",
    "        X = MaxPooling1D(pool_size=2)(X)\n",
    "        X = Flatten()(X)\n",
    "\n",
    "    # Shared LSTM layers:\n",
    "    elif model==\"LSTM\":\n",
    "        X = LSTM(512, return_sequences=True)(X_input)\n",
    "        X = LSTM(256)(X)\n",
    "\n",
    "    # Shared Dense layers:\n",
    "    else:\n",
    "        X = Flatten()(X_input)\n",
    "        X = Dense(512, activation=\"relu\")(X)\n",
    "        \n",
    "    return X\n",
    "\n",
    "def get_model_actor_simple(input_shape=input_shape, n_actions=n_actions, continuous=False):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    X = get_common_layer(X_input)\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "    if continuous:\n",
    "        output = Dense(n_actions,activation=\"tanh\")(X)\n",
    "    else:\n",
    "        output = Dense(n_actions, activation=\"softmax\")(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = output)\n",
    "    model.compile(loss=actor_ppo_loss, optimizer=Adam(lr=0.00025))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_critic_simple(input_shape=input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    V = get_common_layer(X_input)\n",
    "    V = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "    V = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "    V = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "    value = Dense(1, activation=None)(V)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs = value)\n",
    "    model.compile(loss=critic_ppo_loss, optimizer=Adam(lr=0.00025))\n",
    "    return model\n",
    "    \n",
    "actor = get_model_actor_simple()\n",
    "critic = get_model_critic_simple()\n",
    "\n",
    "def act(state):\n",
    "    \"\"\" example:\n",
    "    pred = np.array([0.05, 0.85, 0.1])\n",
    "    action_size = 3\n",
    "    np.random.choice(a, p=pred)\n",
    "    result>>> 1, because it have the highest probability to be taken\n",
    "    \"\"\"\n",
    "    # Use the network to predict the next action to take, using the model\n",
    "    prediction = actor.predict(state)[0]\n",
    "    action = np.random.choice(n_actions, p=prediction)\n",
    "    action_onehot = np.zeros([n_actions])\n",
    "    action_onehot[action] = 1\n",
    "    return action, action_onehot, prediction\n",
    "\n",
    "def discount_rewards(reward):#gaes is better\n",
    "    # Compute the gamma-discounted rewards over an episode\n",
    "    # We apply the discount and normalize it to avoid big variability of rewards\n",
    "    gamma = 0.99    # discount rate\n",
    "    running_add = 0\n",
    "    discounted_r = np.zeros_like(reward)\n",
    "    for i in reversed(range(0,len(reward))):\n",
    "        running_add = running_add * gamma + reward[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "    discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "    return discounted_r\n",
    "\n",
    "def get_gaes(rewards, dones, values, next_values, gamma = 0.99, lamda = 0.9, normalize=True):\n",
    "    deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "    deltas = np.stack(deltas)\n",
    "    gaes = copy.deepcopy(deltas)\n",
    "    for t in reversed(range(len(deltas) - 1)):\n",
    "        gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "    target = gaes + values\n",
    "    if normalize:\n",
    "        gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "    return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "   \n",
    "    limit = 0\n",
    "    while not done:\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "        action = np.argmax(action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        #print('test reward',reward)\n",
    "        limit += 1\n",
    "        if limit > 20:\n",
    "            break\n",
    "    print('testing...', total_reward)\n",
    "    return total_reward\n",
    "\n",
    "def critic_predict(state):\n",
    "    return critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "def replay(states, actions, rewards, predictions, dones, next_states):\n",
    "    # reshape memory to appropriate shape for training\n",
    "    states = np.vstack(states)\n",
    "    next_states = np.vstack(next_states)\n",
    "    actions = np.vstack(actions)\n",
    "    predictions = np.vstack(predictions)\n",
    "\n",
    "    # Get Critic network predictions \n",
    "    values = critic_predict(states)\n",
    "    next_values = critic_predict(next_states)\n",
    "\n",
    "    # Compute discounted rewards and advantages\n",
    "    #discounted_r = self.discount_rewards(rewards)\n",
    "    #advantages = np.vstack(discounted_r - values)\n",
    "    advantages, target = get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "    '''\n",
    "        pylab.plot(advantages,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        '''\n",
    "    # stack everything to numpy array\n",
    "    # pack all advantages, predictions and actions to y_true and when they are received\n",
    "    # in custom PPO loss function we unpack it\n",
    "    y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "    # training Actor and Critic networks\n",
    "    print()\n",
    " \n",
    "    a_loss = actor.fit(states, y_true, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "    print('actor loss',np.mean(a_loss.history['loss']))\n",
    "    print()\n",
    "\n",
    "    c_loss = critic.fit(states, target, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "    print('critic loss',np.mean(c_loss.history['loss']))\n",
    "    print()\n",
    "#     self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "#     self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "#     self.replay_count += 1\n",
    "    #replay_count += 1\n",
    "\n",
    "def run_batch(): # train every self.Training_batch episodes\n",
    "    scores_ = []\n",
    "    episodes_ = []\n",
    "    averages_= [] \n",
    "    episode = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1,state_size[0]])\n",
    "    done, score, SAVING = False, 0, ''\n",
    "    while True:\n",
    "        # Instantiate or reset games memory\n",
    "        states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "        for t in range(training_batch):\n",
    "           # env.render()\n",
    "            # Actor picks an action\n",
    "            action, action_onehot, prediction = act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Memorize (state, action, reward) for training\n",
    "            states.append(state)\n",
    "            next_states.append(np.reshape(next_state, [1, state_size[0]]))\n",
    "            actions.append(action_onehot)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            predictions.append(prediction)\n",
    "            # Update current state\n",
    "            state = np.reshape(next_state, [1, state_size[0]])\n",
    "            score += reward\n",
    "            if done:\n",
    "                episode += 1\n",
    "                SAVING = False\n",
    "                scores_.append(score)\n",
    "                averages_.append(sum(scores_[-50:]) / len(scores_[-50:]))\n",
    "\n",
    "                print('score', averages_[-1])\n",
    "#                     average, SAVING = self.PlotModel(score, self.episode)\n",
    "#                     print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "\n",
    "                state, done, score, SAVING = env.reset(), False, 0, ''\n",
    "                state = np.reshape(state, [1, state_size[0]])\n",
    "   \n",
    "        replay(states, actions, rewards, predictions, dones, next_states)\n",
    "        if episode >= 1000:\n",
    "            break\n",
    "    env.close()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9de909",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-28T14:23:25.315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 17.0\n",
      "score 24.5\n",
      "score 24.0\n",
      "score 22.5\n",
      "score 22.6\n",
      "score 20.833333333333332\n",
      "score 24.285714285714285\n",
      "score 23.75\n",
      "score 23.333333333333332\n",
      "score 21.8\n",
      "score 20.90909090909091\n",
      "score 21.5\n",
      "score 20.923076923076923\n",
      "score 20.928571428571427\n",
      "score 21.533333333333335\n",
      "score 22.25\n",
      "score 21.88235294117647\n",
      "score 22.166666666666668\n",
      "score 21.526315789473685\n",
      "score 21.65\n",
      "score 22.857142857142858\n",
      "score 22.59090909090909\n",
      "score 22.043478260869566\n",
      "score 21.833333333333332\n",
      "score 21.48\n",
      "score 21.615384615384617\n",
      "score 21.59259259259259\n",
      "score 21.678571428571427\n",
      "score 22.03448275862069\n",
      "score 22.133333333333333\n",
      "score 22.483870967741936\n",
      "score 22.6875\n",
      "score 22.272727272727273\n",
      "score 23.294117647058822\n",
      "score 23.057142857142857\n",
      "score 23.77777777777778\n",
      "score 24.135135135135137\n",
      "score 24.07894736842105\n",
      "score 23.974358974358974\n",
      "score 24.2\n",
      "score 23.951219512195124\n",
      "\n",
      "actor loss -0.014927236577868463\n",
      "\n",
      "critic loss 17.086272804641723\n",
      "\n",
      "score 23.857142857142858\n",
      "score 23.72093023255814\n",
      "score 24.068181818181817\n",
      "score 23.8\n",
      "score 24.304347826086957\n",
      "score 24.23404255319149\n",
      "score 24.520833333333332\n",
      "score 25.020408163265305\n",
      "score 24.74\n",
      "score 25.02\n",
      "score 24.92\n",
      "score 24.72\n",
      "score 24.82\n",
      "score 25.08\n",
      "score 25.2\n",
      "score 24.52\n",
      "score 24.56\n",
      "score 25.5\n",
      "score 25.74\n",
      "score 25.82\n",
      "score 25.7\n",
      "score 25.64\n",
      "score 26.26\n",
      "score 26.46\n",
      "score 26.08\n",
      "score 26.14\n",
      "score 26.2\n",
      "score 26.4\n",
      "score 26.2\n",
      "score 25.52\n",
      "score 25.78\n",
      "score 25.96\n",
      "score 25.92\n",
      "score 26.66\n",
      "score 26.66\n",
      "score 26.82\n",
      "score 26.6\n",
      "score 26.2\n",
      "\n",
      "actor loss -0.01834910307088867\n",
      "\n",
      "critic loss 16.10885777301788\n",
      "\n",
      "score 27.66\n",
      "score 27.32\n",
      "score 29.9\n",
      "score 30.16\n",
      "score 30.5\n",
      "score 31.32\n",
      "score 32.94\n",
      "score 33.1\n",
      "score 33.54\n",
      "score 33.4\n",
      "score 33.14\n",
      "score 33.54\n",
      "score 34.94\n",
      "score 35.7\n",
      "score 35.7\n",
      "score 36.86\n",
      "score 36.46\n",
      "score 36.84\n",
      "\n",
      "actor loss -0.01820247613191605\n",
      "\n",
      "critic loss 28.873127776145935\n",
      "\n",
      "score 37.74\n",
      "score 38.26\n",
      "score 39.94\n",
      "score 40.42\n",
      "score 42.4\n",
      "score 43.24\n",
      "score 44.68\n",
      "score 45.92\n",
      "score 46.2\n",
      "score 48.66\n",
      "score 50.14\n",
      "score 49.24\n",
      "\n",
      "actor loss -0.018917338752746586\n",
      "\n",
      "critic loss 22.675390476417544\n",
      "\n",
      "score 56.92\n",
      "score 64.12\n",
      "\n",
      "actor loss -0.0017751393467187882\n",
      "\n",
      "critic loss 38.7073890982151\n",
      "\n",
      "score 69.2\n",
      "score 77.66\n",
      "score 83.66\n",
      "\n",
      "actor loss -0.005769629031419753\n",
      "\n",
      "critic loss 32.578522221183775\n",
      "\n",
      "score 90.26\n",
      "score 94.18\n",
      "score 103.22\n",
      "\n",
      "actor loss 0.0068840931057929985\n",
      "\n",
      "critic loss 48.72145614347458\n",
      "\n",
      "score 108.7\n",
      "score 112.52\n",
      "score 115.94\n",
      "score 120.94\n",
      "\n",
      "actor loss -0.0004444444492459298\n",
      "\n",
      "critic loss 31.296634503078458\n",
      "\n",
      "score 124.94\n",
      "score 127.56\n",
      "score 130.74\n",
      "score 133.28\n",
      "score 136.28\n",
      "score 140.62\n",
      "\n",
      "actor loss -0.004819347937405109\n",
      "\n",
      "critic loss 36.7832798828125\n",
      "\n",
      "score 144.94\n",
      "score 149.2\n",
      "score 151.76\n",
      "score 156.9\n",
      "\n",
      "actor loss -0.004974784564971924\n",
      "\n",
      "critic loss 13.5440404797554\n",
      "\n",
      "score 160.08\n",
      "score 166.2\n",
      "score 172.14\n",
      "\n",
      "actor loss -0.0016843637153506278\n",
      "\n",
      "critic loss 111.50528371682167\n",
      "\n",
      "score 176.96\n",
      "score 182.14\n",
      "\n",
      "actor loss -0.004976830485463143\n",
      "\n",
      "critic loss 35.056373293352124\n",
      "\n",
      "score 189.1\n",
      "score 195.04\n",
      "score 204.78\n",
      "\n",
      "actor loss -0.006373481975495815\n",
      "\n",
      "critic loss 71.97357114529609\n",
      "\n",
      "score 214.38\n",
      "score 223.7\n",
      "\n",
      "actor loss -0.0028828274935483933\n",
      "\n",
      "critic loss 79.24077856025696\n",
      "\n",
      "score 231.9\n",
      "score 240.78\n",
      "\n",
      "actor loss 0.004196130704879761\n",
      "\n",
      "critic loss 63.03842637183667\n",
      "\n",
      "score 250.0\n",
      "score 252.5\n",
      "score 252.34\n",
      "\n",
      "actor loss -0.005964355063438416\n",
      "\n",
      "critic loss 98.83908709144593\n",
      "\n",
      "score 260.9\n",
      "score 269.24\n",
      "score 271.94\n",
      "\n",
      "actor loss -0.0076783592732623225\n",
      "\n",
      "critic loss 27.397663975715638\n",
      "\n",
      "score 280.04\n",
      "score 287.06\n",
      "\n",
      "actor loss 0.0007476045727729796\n",
      "\n",
      "critic loss 56.21795835828782\n",
      "\n",
      "score 294.54\n",
      "score 303.44\n",
      "score 307.22\n",
      "\n",
      "actor loss 0.0006462296754121781\n",
      "\n",
      "critic loss 59.816449474287026\n",
      "\n",
      "score 309.48\n",
      "score 318.84\n",
      "score 323.1\n",
      "\n",
      "actor loss -0.0012680687308311463\n",
      "\n",
      "critic loss 21.715933337879182\n",
      "\n",
      "score 328.48\n",
      "score 332.74\n",
      "\n",
      "actor loss -0.006163840770721435\n",
      "\n",
      "critic loss 255.3690487239838\n",
      "\n",
      "score 334.66\n",
      "score 337.14\n",
      "score 338.74\n",
      "\n",
      "actor loss -0.0032407906115055085\n",
      "\n",
      "critic loss 33.641286824798584\n",
      "\n",
      "score 335.88\n",
      "score 336.44\n",
      "\n",
      "actor loss -0.0014781125783920287\n",
      "\n",
      "critic loss 8.219904427957534\n",
      "\n",
      "score 339.04\n",
      "score 340.56\n",
      "score 335.7\n",
      "\n",
      "actor loss -0.003084516684617847\n",
      "\n",
      "critic loss 49.81768656990528\n",
      "\n",
      "score 338.9\n",
      "score 344.68\n",
      "\n",
      "actor loss -0.0022305566340684894\n",
      "\n",
      "critic loss 59.29263247489929\n",
      "\n",
      "score 350.98\n",
      "score 355.72\n",
      "score 357.44\n",
      "\n",
      "actor loss -0.0021707928270101547\n",
      "\n",
      "critic loss 45.82742354028225\n",
      "\n",
      "score 362.9\n",
      "score 369.42\n",
      "\n",
      "actor loss -0.005081133618950844\n",
      "\n",
      "critic loss 105.51489635857342\n",
      "\n",
      "score 375.88\n",
      "score 382.38\n",
      "\n",
      "actor loss -0.003991889939457178\n",
      "\n",
      "critic loss 61.18555628499985\n",
      "\n",
      "score 385.22\n",
      "score 390.64\n",
      "\n",
      "actor loss -0.0024330958678852764\n",
      "\n",
      "critic loss 53.05505686550141\n",
      "\n",
      "score 396.14\n",
      "score 401.62\n",
      "\n",
      "actor loss 3.4019209444522876e-05\n",
      "\n",
      "critic loss 64.83336155309676\n",
      "\n",
      "score 406.16\n",
      "score 409.82\n",
      "\n",
      "actor loss 0.0023946897034533322\n",
      "\n",
      "critic loss 78.68648628187181\n",
      "\n",
      "score 413.26\n",
      "score 415.84\n",
      "\n",
      "actor loss -0.0014008152604103087\n",
      "\n",
      "critic loss 61.709621712017054\n",
      "\n",
      "score 415.66\n",
      "score 417.16\n",
      "score 419.3\n",
      "\n",
      "actor loss -0.0028865588530898093\n",
      "\n",
      "critic loss 84.32021830921174\n",
      "\n",
      "score 422.48\n",
      "score 421.92\n",
      "\n",
      "actor loss -0.0014764033824205397\n",
      "\n",
      "critic loss 56.775458880043026\n",
      "\n",
      "score 421.92\n",
      "score 421.92\n",
      "\n",
      "actor loss -0.0032781213670969008\n",
      "\n",
      "critic loss 55.518917539429665\n",
      "\n",
      "score 421.92\n",
      "score 421.92\n",
      "\n",
      "actor loss 0.004754891821742058\n",
      "\n",
      "critic loss 55.152534020996086\n",
      "\n",
      "score 421.92\n",
      "score 428.02\n",
      "\n",
      "actor loss -0.0035854598712176085\n",
      "\n",
      "critic loss 64.88559511542319\n",
      "\n",
      "score 437.64\n",
      "score 438.28\n",
      "\n",
      "actor loss 0.0007589941143989562\n",
      "\n",
      "critic loss 46.873314644861225\n",
      "\n",
      "score 438.28\n",
      "score 444.08\n",
      "\n",
      "actor loss 4.281849861145002e-05\n",
      "\n",
      "critic loss 61.77335570201875\n",
      "\n",
      "score 444.08\n",
      "score 445.96\n",
      "\n",
      "actor loss -0.0018689528942108155\n",
      "\n",
      "critic loss 59.23859484093189\n",
      "\n",
      "score 445.96\n",
      "score 445.96\n",
      "\n",
      "actor loss -0.0032177373960614202\n",
      "\n",
      "critic loss 48.63135976662636\n",
      "\n",
      "score 449.66\n",
      "score 455.44\n",
      "\n",
      "actor loss -0.004866350790858269\n",
      "\n",
      "critic loss 46.89480326256752\n",
      "\n",
      "score 455.44\n",
      "score 458.5\n",
      "\n",
      "actor loss -0.001060526479780674\n",
      "\n",
      "critic loss 57.56901134757995\n",
      "\n",
      "score 458.18\n",
      "score 463.48\n",
      "\n",
      "actor loss 0.0010673794028349223\n",
      "\n",
      "critic loss 99.90645414626599\n",
      "\n",
      "score 463.48\n",
      "score 460.54\n",
      "score 457.56\n",
      "score 453.74\n",
      "\n",
      "actor loss -0.0031624448165297513\n",
      "\n",
      "critic loss 63.87729053192138\n",
      "\n",
      "score 449.78\n",
      "score 449.62\n",
      "score 449.34\n",
      "\n",
      "actor loss -0.0059026060149073595\n",
      "\n",
      "critic loss 25.751798895072938\n",
      "\n",
      "score 450.98\n",
      "score 448.8\n",
      "score 447.9\n",
      "\n",
      "actor loss -0.008540057283639907\n",
      "\n",
      "critic loss 22.732313239002227\n",
      "\n",
      "score 443.22\n",
      "score 437.98\n",
      "score 436.76\n",
      "\n",
      "actor loss -0.009995806741714477\n",
      "\n",
      "critic loss 24.591330404186248\n",
      "\n",
      "score 433.46\n",
      "score 430.7\n",
      "score 425.6\n",
      "score 422.2\n",
      "\n",
      "actor loss -0.006398210173845291\n",
      "\n",
      "critic loss 2.5699906334996223\n",
      "\n",
      "score 420.98\n",
      "score 416.6\n",
      "score 411.08\n",
      "\n",
      "actor loss -0.006309859848022461\n",
      "\n",
      "critic loss 1.4487268440365793\n",
      "\n",
      "score 407.54\n",
      "score 405.12\n",
      "score 404.64\n",
      "\n",
      "actor loss -0.004149964499473572\n",
      "\n",
      "critic loss 4.094662664079666\n",
      "\n",
      "score 404.64\n",
      "score 402.52\n",
      "\n",
      "actor loss -0.0024926229387521744\n",
      "\n",
      "critic loss 12.657796206247806\n",
      "\n",
      "score 402.12\n",
      "score 399.14\n",
      "score 399.14\n",
      "\n",
      "actor loss -0.0033957817859947685\n",
      "\n",
      "critic loss 39.624717702198026\n",
      "\n",
      "score 399.14\n",
      "score 398.9\n",
      "\n",
      "actor loss 0.013377762355282907\n",
      "\n",
      "critic loss 36.10531641425491\n",
      "\n",
      "score 396.86\n",
      "score 396.86\n",
      "\n",
      "actor loss -0.0013567769169807434\n",
      "\n",
      "critic loss 43.27210956423282\n",
      "\n",
      "score 393.38\n",
      "score 393.38\n",
      "\n",
      "actor loss -0.0001457103252410889\n",
      "\n",
      "critic loss 34.82159305125475\n",
      "\n",
      "score 393.38\n",
      "score 392.56\n",
      "score 390.16\n",
      "\n",
      "actor loss -0.003801255224645138\n",
      "\n",
      "critic loss 28.110539652204512\n",
      "\n",
      "score 390.16\n",
      "score 390.16\n",
      "\n",
      "actor loss -0.0014453505277633666\n",
      "\n",
      "critic loss 63.82614386516066\n",
      "\n",
      "score 389.74\n",
      "score 389.74\n",
      "\n",
      "actor loss 0.007143353840708734\n",
      "\n",
      "critic loss 29.794781128263473\n",
      "\n",
      "score 386.0\n",
      "score 382.3\n",
      "score 377.76\n",
      "\n",
      "actor loss -0.0004191533580422403\n",
      "\n",
      "critic loss 1.4161800384283068\n",
      "\n",
      "score 373.6\n",
      "score 368.92\n",
      "score 366.54\n",
      "\n",
      "actor loss 9.631621837615976e-06\n",
      "\n",
      "critic loss 1.7388052632331845\n",
      "\n",
      "score 362.7\n",
      "score 362.76\n",
      "score 358.26\n",
      "score 353.78\n",
      "\n",
      "actor loss -0.0036283834934234616\n",
      "\n",
      "critic loss 1.4943390774607659\n",
      "\n",
      "score 356.36\n",
      "score 359.66\n",
      "\n",
      "actor loss -0.005993525886535645\n",
      "\n",
      "critic loss 4.037284537923336\n",
      "\n",
      "score 367.66\n",
      "score 372.88\n",
      "\n",
      "actor loss -0.0038536385283339768\n",
      "\n",
      "critic loss 13.642950763082505\n",
      "\n",
      "score 373.04\n",
      "score 377.6\n",
      "\n",
      "actor loss -0.003827744644880295\n",
      "\n",
      "critic loss 43.774172233486176\n",
      "\n",
      "score 378.34\n",
      "score 378.66\n",
      "score 378.8\n",
      "\n",
      "actor loss -0.0007858784645795822\n",
      "\n",
      "critic loss 29.136382690358165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_batch() # train as PPO, train every epesode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840f3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:50:49.315061Z",
     "start_time": "2022-07-28T13:50:49.315061Z"
    }
   },
   "outputs": [],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb5ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:50:49.316046Z",
     "start_time": "2022-07-28T13:50:49.316046Z"
    }
   },
   "outputs": [],
   "source": [
    "type(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69088a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
