{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f5a87b",
   "metadata": {},
   "source": [
    "### Quick theory\n",
    "Just like the Actor-Critic method, we have two networks:\n",
    "\n",
    "* Actor - It proposes an action given a state.\n",
    "* Critic - It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n",
    "\n",
    "DDPG uses two more techniques not present in the original DQN:\n",
    "\n",
    "1. Uses two Target networks.\n",
    "        Why? Because it add stability to training. In short, we are learning from estimated targets and Target networks are updated slowly, hence keeping our estimated targets stable.Conceptually, this is like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\", as opposed to saying \"I'm going to re-learn how to play this entire game after every move\". See this StackOverflow answer.\n",
    "\n",
    "2. Uses Experience Replay.\n",
    "\n",
    "### Losses:\n",
    "\n",
    "**Critic loss** - Mean Squared Error of y - Q(s, a) where y is the expected return as seen by the Target network, and Q(s, a) is action value predicted by the Critic network. y is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n",
    "\n",
    "**Actor loss** - This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "Hence we update the Actor network so that it produces actions that get the maximum predicted value as seen by the Critic, for a given state.\n",
    "\n",
    "### Initialization:\n",
    "The initialization for last layer of the Actor must be between -0.003 and 0.003 as this prevents us from getting 1 or -1 output values in the initial stages, which would squash our gradients to zero, as we use the tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T11:44:12.459170Z",
     "start_time": "2022-07-26T11:44:12.439169Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T11:44:16.739412Z",
     "start_time": "2022-07-26T11:44:12.461170Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3264a114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T11:44:16.755409Z",
     "start_time": "2022-07-26T11:44:16.743416Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To implement better exploration by the Actor network, we use noisy perturbations, \n",
    "specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. \n",
    "It samples noise from a correlated normal distribution.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c167df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T11:44:16.770409Z",
     "start_time": "2022-07-26T11:44:16.757410Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64, num_actions=1,):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845cdf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T11:05:13.582733Z",
     "start_time": "2022-07-26T11:05:13.569734Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "289c0dd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T12:06:10.086956Z",
     "start_time": "2022-07-26T12:06:10.054334Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate\n",
    "\n",
    "from src.utils.noise import OUActionNoise \n",
    "\n",
    "class Buffer:\n",
    "    def __init__(self,n_actions, n_states, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, n_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, n_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, n_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "\n",
    "class DdpgAgent(Agent):\n",
    "    def __init__(self,\n",
    "                 environment,\n",
    "                 gamma = 0.99,\n",
    "                 tau= 0.005,\n",
    "                 std_dev = 0.2,\n",
    "                 critic_lr = 0.002,\n",
    "                 actor_lr = 0.001,\n",
    "                 buffer_size=50000,\n",
    "                 batch_size=64,\n",
    "                 critic_optimizer = tf.keras.optimizers.Adam,\n",
    "                 actor_optimizer = tf.keras.optimizers.Adam,\n",
    "        ):\n",
    "        super(DdpgAgent,self).__init__(environment,loss_keys=[\"actor_loss\",\"critic_loss\"],args=locals())\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "\n",
    "        self.noise = OUActionNoise(mean=np.zeros(self.n_actions), std_deviation=float(std_dev) * np.ones(self.n_actions))\n",
    "        \n",
    "        self.critic_optimizer = critic_optimizer(critic_lr)\n",
    "        self.actor_optimizer = actor_optimizer(actor_lr)\n",
    "\n",
    "        # Discount factor for future rewards\n",
    "        self.gamma = gamma\n",
    "        # Used to update target networks\n",
    "        self.tau = tau\n",
    "\n",
    "        if self.action_space_mode != \"continuous\":\n",
    "            raise Exception('DDPG only accepts continuous action spaces')\n",
    "\n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        \n",
    "        self._add_models_to_config([self.actor,self.target_actor,self.critic,self.target_critic])\n",
    "        self._init_tensorboard()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = Buffer(self.n_actions, self.n_inputs, self.buffer_size, self.batch_size)\n",
    "            \n",
    "    def __init_networks(self):\n",
    "        \n",
    "        def create_actor():\n",
    "            # Initialize weights between -3e-3 and 3-e3\n",
    "            last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "            inputs = Input(shape=self.env.observation_space.shape)\n",
    "            out = Flatten()(inputs)\n",
    "            out = Dense(256, activation=\"relu\")(out)\n",
    "            out = Dense(256, activation=\"relu\")(out)\n",
    "            outputs = Dense(self.n_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "            # Our upper bound is 2.0 for Pendulum.\n",
    "            outputs = outputs * self.action_upper_bounds\n",
    "            return tf.keras.Model(inputs, outputs)\n",
    "        \n",
    "        def create_critic():\n",
    "            # State as input\n",
    "            state_input = Input(shape=self.env.observation_space.shape)\n",
    "            state_out = Flatten()(state_input)\n",
    "            state_out = Dense(16, activation=\"relu\")(state_out)\n",
    "            state_out = Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "            # Action as input\n",
    "            action_input = Input(shape=(self.n_actions))\n",
    "            action_out = Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "            # Both are passed through seperate layer before concatenating\n",
    "            concat = Concatenate()([state_out, action_out])\n",
    "\n",
    "            out = Dense(256, activation=\"relu\")(concat)\n",
    "            out = Dense(256, activation=\"relu\")(out)\n",
    "            outputs = Dense(1)(out)\n",
    "\n",
    "            # Outputs single value for give state-action\n",
    "            return tf.keras.Model([state_input, action_input], outputs)\n",
    "        \n",
    "        self.actor = create_actor()\n",
    "        self.target_actor = create_actor()\n",
    "        \n",
    "        self.critic = create_critic()\n",
    "        self.target_critic = create_critic()\n",
    "        \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        \n",
    "        self.models = [self.actor,self.target_actor,self.critic,self.target_critic]\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        sampled_actions = tf.squeeze(self.actor(state))\n",
    "        noise = self.noise()\n",
    "        # Adding noise to action\n",
    "        sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "        # We make sure action is within bounds\n",
    "        legal_action = np.clip(sampled_actions, self.action_lower_bounds, self.action_upper_bounds)\n",
    "\n",
    "        return [np.squeeze(legal_action)]\n",
    "    \n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    # @tf.function\n",
    "    def update_target(self,target_weights, weights, tau):\n",
    "        for (a, b) in zip(target_weights, weights):\n",
    "            a.assign(b * tau + a * (1 - tau))\n",
    "    # @tf.function      \n",
    "    def update(self,state_batch, action_batch, reward_batch, next_state_batch):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(state_batch, training=True)\n",
    "            critic_value = self.critic([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor.trainable_variables)\n",
    "        )\n",
    "        \n",
    "        self.learning_log.step_loss({\n",
    "            \"actor_loss\":tf.get_static_value(actor_loss),\n",
    "            \"critic_loss\":tf.get_static_value(critic_loss)\n",
    "        })\n",
    "\n",
    "        # log losses\n",
    "        self.write_tensorboard_scaler('actor_loss',tf.get_static_value(actor_loss),self.learning_log.learning_steps)\n",
    "        self.write_tensorboard_scaler('critic_loss',tf.get_static_value(critic_loss),self.learning_log.learning_steps)\n",
    "            \n",
    "    def replay(self):\n",
    "  \n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.buffer.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "    def test(self, episodes=10, render=True):\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            try:\n",
    "                state = self.env.reset()\n",
    "            except:\n",
    "                self._Agent__init_environment()\n",
    "                state = self.env.reset()\n",
    "\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                    \n",
    "                action = self.choose_action(state)\n",
    "\n",
    "                # Step\n",
    "                state,reward,done, info = self.env.step(action)\n",
    "\n",
    "                # Get next state\n",
    "                score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.close()\n",
    "\n",
    "            print(\"Test episode: {}, score: {:.2f}\".format(episode,score)) \n",
    "    \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_each_n_episodes=100, log_level=1):\n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    "\n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            prev_state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                \n",
    "                action = self.choose_action(prev_state)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                self.buffer.record((prev_state, action, reward, state))\n",
    "                self.replay()\n",
    "                self.update_target(self.target_actor.variables, self.actor.variables, self.tau)\n",
    "                self.update_target(self.target_critic.variables, self.critic.variables, self.tau)\n",
    "                prev_state=state\n",
    "                score += reward\n",
    "                timestep+=1\n",
    "                \n",
    "\n",
    "            self.running_reward.step(score)\n",
    "            # Log details\n",
    "            episode += 1\n",
    "            self.learning_log.episode(\n",
    "                log_each_n_episodes,\n",
    "                score,\n",
    "                self.running_reward.reward, \n",
    "                log_level=log_level\n",
    "            )\n",
    "            \n",
    "            # log scores\n",
    "            self.write_tensorboard_scaler('score',score,self.learning_log.episodes)\n",
    "            \n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                break\n",
    "\n",
    "\n",
    "        if plot_results:\n",
    "            self.plot_learning_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a70afbda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T12:15:17.392451Z",
     "start_time": "2022-07-26T12:06:10.755766Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| Pendulum-v1\n",
      "| \n",
      "| Action space: Continuous with low state-space\n",
      "| Environment beated threshold: -200\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "Argument 'graph_data' is not tf.Graph or tf.compat.v1.GraphDef. Received graph_data=<keras.engine.functional.Functional object at 0x000001D7D889EE60> of type Functional.\n",
      "Argument 'graph_data' is not tf.Graph or tf.compat.v1.GraphDef. Received graph_data=<keras.engine.functional.Functional object at 0x000001D7D885AC20> of type Functional.\n",
      "Argument 'graph_data' is not tf.Graph or tf.compat.v1.GraphDef. Received graph_data=<keras.engine.functional.Functional object at 0x000001D7E15B51E0> of type Functional.\n",
      "Argument 'graph_data' is not tf.Graph or tf.compat.v1.GraphDef. Received graph_data=<keras.engine.functional.Functional object at 0x000001D7DCE8A980> of type Functional.\n",
      "Episode * 10 * Moving Avg Reward is ==> -1416.135 * Last Reward was ==> -1054.773\n",
      "Episode * 20 * Moving Avg Reward is ==> -940.785 * Last Reward was ==> -127.223\n",
      "Episode * 30 * Moving Avg Reward is ==> -699.271 * Last Reward was ==> -128.177\n",
      "Episode * 40 * Moving Avg Reward is ==> -618.546 * Last Reward was ==> -386.099\n",
      "Episode * 50 * Moving Avg Reward is ==> -525.319 * Last Reward was ==> -133.335\n",
      "Episode * 60 * Moving Avg Reward is ==> -267.750 * Last Reward was ==> -235.305\n",
      "Episode * 70 * Moving Avg Reward is ==> -241.755 * Last Reward was ==> -757.438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpendulum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environment\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m DdpgAgent(environment,buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m,)\n\u001b[1;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_each_n_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36mDdpgAgent.learn\u001b[1;34m(self, timesteps, plot_results, reset, success_threshold, log_each_n_episodes, log_level)\u001b[0m\n\u001b[0;32m    247\u001b[0m state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mrecord((prev_state, action, reward, state))\n\u001b[1;32m--> 249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_actor\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_critic\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36mDdpgAgent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    199\u001b[0m reward_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(reward_batch, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    200\u001b[0m next_state_batch \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mnext_state_buffer[batch_indices])\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36mDdpgAgent.update\u001b[1;34m(self, state_batch, action_batch, reward_batch, next_state_batch)\u001b[0m\n\u001b[0;32m    173\u001b[0m     actor_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mreduce_mean(critic_value)\n\u001b[0;32m    175\u001b[0m actor_grad \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(actor_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactor_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_log\u001b[38;5;241m.\u001b[39mstep_loss({\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:tf\u001b[38;5;241m.\u001b[39mget_static_value(actor_loss),\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcritic_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m:tf\u001b[38;5;241m.\u001b[39mget_static_value(critic_loss)\n\u001b[0;32m    183\u001b[0m })\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# log losses\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:665\u001b[0m, in \u001b[0;36mOptimizerV2.apply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m experimental_aggregate_gradients \u001b[38;5;129;01mand\u001b[39;00m strategy \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(strategy,\n\u001b[0;32m    656\u001b[0m                (tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mParameterServerStrategy,\n\u001b[0;32m    657\u001b[0m                 tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mParameterServerStrategy,\n\u001b[0;32m    658\u001b[0m                 tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mCentralStorageStrategy,\n\u001b[0;32m    659\u001b[0m                 tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mCentralStorageStrategy))):\n\u001b[0;32m    660\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    661\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`experimental_aggregate_gradients=False is not supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    662\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameterServerStrategy and CentralStorageStrategy. Used: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    663\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrategy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 665\u001b[0m apply_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m experimental_aggregate_gradients:\n\u001b[0;32m    667\u001b[0m   grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_unaggregated_gradients(grads_and_vars)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:944\u001b[0m, in \u001b[0;36mOptimizerV2._prepare\u001b[1;34m(self, var_list)\u001b[0m\n\u001b[0;32m    942\u001b[0m   apply_state[(var_device, var_dtype)] \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    943\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(var_device):\n\u001b[1;32m--> 944\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m apply_state\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:141\u001b[0m, in \u001b[0;36mAdam._prepare_local\u001b[1;34m(self, var_device, var_dtype, apply_state)\u001b[0m\n\u001b[0;32m    131\u001b[0m beta_2_power \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mpow(beta_2_t, local_step)\n\u001b[0;32m    132\u001b[0m lr \u001b[38;5;241m=\u001b[39m (apply_state[(var_device, var_dtype)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_t\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    133\u001b[0m       (tf\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_2_power) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_1_power)))\n\u001b[0;32m    134\u001b[0m apply_state[(var_device, var_dtype)]\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    136\u001b[0m         lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    137\u001b[0m         epsilon\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon, var_dtype),\n\u001b[0;32m    139\u001b[0m         beta_1_t\u001b[38;5;241m=\u001b[39mbeta_1_t,\n\u001b[0;32m    140\u001b[0m         beta_1_power\u001b[38;5;241m=\u001b[39mbeta_1_power,\n\u001b[1;32m--> 141\u001b[0m         one_minus_beta_1_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta_1_t\u001b[49m,\n\u001b[0;32m    142\u001b[0m         beta_2_t\u001b[38;5;241m=\u001b[39mbeta_2_t,\n\u001b[0;32m    143\u001b[0m         beta_2_power\u001b[38;5;241m=\u001b[39mbeta_2_power,\n\u001b[0;32m    144\u001b[0m         one_minus_beta_2_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta_2_t))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1441\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.r_binary_op_wrapper\u001b[1;34m(y, x)\u001b[0m\n\u001b[0;32m   1437\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28;01mNone\u001b[39;00m, op_name, [x, y]) \u001b[38;5;28;01mas\u001b[39;00m name:\n\u001b[0;32m   1438\u001b[0m   \u001b[38;5;66;03m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[0;32m   1439\u001b[0m   \u001b[38;5;66;03m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m   y, x \u001b[38;5;241m=\u001b[39m maybe_promote_tensors(y, x, force_same_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1441\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\math_ops.py:548\u001b[0m, in \u001b[0;36msubtract\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.subtract\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtract\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[0;32m    546\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubtract\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 548\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:11125\u001b[0m, in \u001b[0;36msub\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  11123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m  11124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 11125\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  11126\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSub\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m  11128\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.pendulum import environment\n",
    "agent = DdpgAgent(environment)\n",
    "agent.learn(log_each_n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dfc7bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T12:15:17.394417Z",
     "start_time": "2022-07-26T12:15:17.394417Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.tensorboard_writer_log_directory\n",
    "#'storage/environments/Pendulum/DdpgAgent/e996ee6c856f7c9cad03964bbe0fa65e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd71537f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T12:15:17.395421Z",
     "start_time": "2022-07-26T12:15:17.395421Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir storage/environments/Pendulum/DdpgAgent/04f201fdc7aa03ddc3ef05f0c1477ebc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abd6c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
