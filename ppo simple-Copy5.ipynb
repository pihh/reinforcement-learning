{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602e667f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:18.072304Z",
     "start_time": "2022-08-07T09:24:18.043314Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca80c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:18.168249Z",
     "start_time": "2022-08-07T09:24:18.074243Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0862e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:45:48.741797Z",
     "start_time": "2022-08-02T18:45:48.550797Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb52acf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:22.021700Z",
     "start_time": "2022-08-07T09:24:18.170240Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import random\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "\n",
    "# Helpers\n",
    "\n",
    "# gaussian_likelihood - ver se consigo uma função global\n",
    "# Keras loss\n",
    "# def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "#     log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "#     pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "#     return K.sum(pre_sum, axis=1)\n",
    "#\n",
    "# # Agent\n",
    "# def gaussian_likelihood(self, action, pred, log_std):\n",
    "#     # https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py\n",
    "#     pre_sum = -0.5 * (((action-pred)/(np.exp(log_std)+1e-8))**2 + 2*log_std + np.log(2*np.pi))\n",
    "#     return np.sum(pre_sum, axis=1)\n",
    "def gaussian_likelihood(log_std, lib=\"keras\"): # for keras custom loss\n",
    "    _exp = K.exp\n",
    "    _log = K.log\n",
    "    _sum = K.sum\n",
    "    if lib == \"numpy\":\n",
    "        _exp = np.exp\n",
    "        _log = np.log\n",
    "        _sum = np.sum\n",
    "    \n",
    "    def fn(actions,pred):\n",
    "        pre_sum = -0.5 * (((actions-pred)/(_exp(log_std)+1e-8))**2 + 2*log_std + _log(2*np.pi))\n",
    "        return _sum(pre_sum, axis=1)\n",
    "    \n",
    "    return fn\n",
    "\n",
    "# Continuous\n",
    "class PpoActorContinuous:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer,loss_clipping = 0.2):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.loss_clipping = loss_clipping\n",
    "        self.log_std = -0.5 * np.ones(self.action_space , dtype=np.float32)\n",
    "\n",
    "        self.gaussian_likelihood = gaussian_likelihood(self.log_std, lib=\"keras\")\n",
    "        \n",
    "        X_input = Input(input_shape)\n",
    "        \n",
    "\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"tanh\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(learning_rate=lr))\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        advantages, actions, logp_old_ph, = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space]\n",
    "\n",
    "        logp = self.gaussian_likelihood(actions, y_pred)\n",
    "\n",
    "        ratio = K.exp(logp - logp_old_ph)\n",
    "\n",
    "        p1 = ratio * advantages\n",
    "        p2 = tf.where(advantages > 0, (1.0 + self.loss_clipping)*advantages, (1.0 - self.loss_clipping)*advantages) # minimum advantage\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "#     def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "        \n",
    "        \n",
    "#         #log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "#         pre_sum = -0.5 * (((actions-pred)/(K.exp(self.log_std)+1e-8))**2 + 2*self.log_std + K.log(2*np.pi))\n",
    "#         return K.sum(pre_sum, axis=1)\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "# Discrete\n",
    "class PpoActorDiscrete:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer,loss_clipping=0.2,loss_entropy=0.001):\n",
    "\n",
    "        self.action_space = action_space\n",
    "        self.loss_clipping = loss_clipping\n",
    "        self.loss_entropy = loss_entropy\n",
    "\n",
    "        X_input = Input(input_shape)\n",
    "\n",
    "\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(learning_rate=lr))\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        #advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        advantages,  actions, prediction_picks = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "\n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = K.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "\n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - self.loss_clipping, max_value=1 + self.loss_clipping) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = self.loss_entropy * K.mean(entropy)\n",
    "\n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "\n",
    "# PPO Critic for discrete or continuous only differs in the initializer\n",
    "class PpoCritic:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer,loss_function_version=1, loss_clipping=0.2,kernel_initializer=False,continuous_action_space=False):\n",
    "\n",
    "        self.loss_clipping = loss_clipping\n",
    "        \n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        if kernel_initializer == False:\n",
    "            if continuous_action_space == False:\n",
    "                kernel_initializer = 'he_uniform'\n",
    "            else:\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.01)\n",
    "                \n",
    "        if loss_function_version == 1:\n",
    "            loss_function = self.ppo_loss\n",
    "        else:\n",
    "            loss_function = self.ppo_loss_2(old_values)\n",
    "\n",
    "        V = Dense(512, activation=\"relu\", kernel_initializer=kernel_initializer)(X_input)\n",
    "        V = Dense(256, activation=\"relu\", kernel_initializer=kernel_initializer)(V)\n",
    "        V = Dense(64, activation=\"relu\", kernel_initializer=kernel_initializer)(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[loss_function], optimizer=optimizer(learning_rate=lr))\n",
    "        \n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "        return value_loss\n",
    "\n",
    "    def ppo_loss_2(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "\n",
    "            clipped_value_loss = values + K.clip(y_pred - values, -self.loss_clipping, self.loss_clipping)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "\n",
    "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "class PpoBuffer:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.states=[] \n",
    "        self.next_states=[] \n",
    "        self.actions=[] \n",
    "        self.rewards=[] \n",
    "        self.predictions=[] \n",
    "        self.dones=[] \n",
    "        \n",
    "# PPO PPOAgent\n",
    "class PpoAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name, training_batch=4000, epochs=80, episodes=1000,lr=0.00025,shuffle=False,target_kl = 0.01, continuous_action_space=False):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        self.target_kl = 0.01\n",
    "        \n",
    "\n",
    "        if continuous_action_space:\n",
    "            self.action_size = self.env.action_space.shape[0]\n",
    "        else:\n",
    "            self.action_size = self.env.action_space.n\n",
    "            \n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = episodes # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs # training epochs\n",
    "        self.shuffle = shuffle\n",
    "        self.Training_batch = training_batch\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "        self.replay_count = 0\n",
    "        self.continuous_action_space=continuous_action_space\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        if continuous_action_space:\n",
    "            self.Actor= PpoActorContinuous(self.state_size, self.action_size, lr=self.lr, optimizer = self.optimizer,loss_clipping = 0.2)\n",
    "        else:\n",
    "            self.Actor= PpoActorDiscrete(self.state_size, self.action_size, lr=self.lr, optimizer = self.optimizer,loss_clipping=0.2,loss_entropy=0.001)\n",
    "\n",
    "        self.Critic = PpoCritic(self.state_size, self.action_size, lr=self.lr, optimizer = self.optimizer,loss_clipping=0.2,kernel_initializer=False,continuous_action_space=continuous_action_space)\n",
    "\n",
    "        # do not change bellow\n",
    "        self.log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "        self.std = np.exp(self.log_std)\n",
    "        \n",
    "        # Bind gaussian likelihood\n",
    "        self.gaussian_likelihood = gaussian_likelihood(self.log_std, lib=\"numpy\")\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.continuous_action_space:\n",
    "            # Use the network to predict the next action to take, using the model\n",
    "            prediction = self.Actor.predict(state)\n",
    "\n",
    "            low, high = -1.0, 1.0 # -1 and 1 are boundaries of tanh\n",
    "            action = prediction + np.random.uniform(low, high, size=prediction.shape) * self.std\n",
    "            action = np.clip(action, low, high)\n",
    "\n",
    "            logp_t = self.gaussian_likelihood(action, prediction)\n",
    "\n",
    "            return action[0], action , logp_t[0]\n",
    "        else:\n",
    "            prediction = self.Actor.predict(state)[0]\n",
    "            action = np.random.choice(self.action_size, p=prediction)\n",
    "            action_onehot = np.zeros([self.action_size])\n",
    "            action_onehot[action] = 1\n",
    "            return action, action_onehot, prediction\n",
    "\n",
    "    def reshape_state(self,state):\n",
    "        return np.reshape(state, [1, self.state_size[0]])\n",
    "    \n",
    "    def run_batch(self):\n",
    "        state = self.env.reset()\n",
    "        state = self.reshape_state(state)\n",
    "        done, score = False, 0\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            buffer = PpoBuffer()\n",
    "\n",
    "            for t in range(self.Training_batch):\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_data, prediction = self.act(state)\n",
    "\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action) \n",
    "                next_state = self.reshape_state(next_state)\n",
    "                # Memorize (state, next_states, action, reward, done, logp_ts) for training\n",
    "                buffer.states.append(state)\n",
    "                buffer.next_states.append(next_state)\n",
    "  \n",
    "                buffer.actions.append(action_data)\n",
    "                buffer.rewards.append(reward)\n",
    "                buffer.dones.append(done)\n",
    "  \n",
    "                buffer.predictions.append(prediction)\n",
    "\n",
    "                # Update current state shape\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average = self.checkpoint(score, self.episode)\n",
    "                    #if str(self.episode)[-2:] == \"00\":\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, ''))\n",
    "                    state, done, score = self.env.reset(), False, 0\n",
    "                    state = self.reshape_state(state) #np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "            \n",
    "            print()\n",
    "            print(action_data)\n",
    "            print()\n",
    "            self.replay(buffer)\n",
    "\n",
    "  \n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def replay(self, buffer):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(buffer.states)\n",
    "        next_states = np.vstack(buffer.next_states)\n",
    "        actions = np.vstack(buffer.actions)\n",
    "        predictions = np.vstack(buffer.predictions)\n",
    "        rewards = buffer.rewards\n",
    "        dones = buffer.dones\n",
    "        \n",
    "        # Get Critic network predictions\n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "\n",
    "        # stack everything to numpy array pack all advantages, predictions and actions to y_true and when they are received in custom loss function we unpack it\n",
    "\n",
    "        y_true = np.hstack([advantages, actions, predictions])\n",
    "\n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "\n",
    "        if self.continuous_action_space:\n",
    "            # calculate loss parameters (should be done in loss, but couldn't find working way how to do that with disabled eager execution)\n",
    "            pred = self.Actor.predict(states)\n",
    "            #log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "            #logp = self.gaussian_likelihood(actions, pred, log_std)\n",
    "            logp = self.gaussian_likelihood(actions, pred)\n",
    "            approx_kl = np.mean(predictions - logp)\n",
    "            approx_ent = np.mean(-logp)\n",
    "            print()\n",
    "            print('approx_kl',approx_kl)\n",
    "            print('approx_ent',approx_ent)\n",
    "            print()\n",
    "        self.replay_count += 1\n",
    "        \n",
    "        buffer.reset()\n",
    "\n",
    "    ### Equal fns\n",
    "#     def gaussian_likelihood(self, action, pred):\n",
    "#         # for continuous only\n",
    "#         # https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py\n",
    "#         pre_sum = -0.5 * (((action-pred)/(np.exp(self.log_std)+1e-8))**2 + 2*self.log_std + np.log(2*np.pi))\n",
    "#         return np.sum(pre_sum, axis=1)\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.90, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def load(self):\n",
    "#         self.Actor.Actor.load_weights(self.Actor_name)\n",
    "#         self.Critic.Critic.load_weights(self.Critic_name)\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "#         self.Actor.Actor.save_weights(self.Actor_name)\n",
    "#         self.Critic.Critic.save_weights(self.Critic_name)\n",
    "        pass\n",
    "\n",
    "    def checkpoint(self, score, episode):\n",
    "        self.scores_.append(score)\n",
    "        self.episodes_.append(episode)\n",
    "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n",
    "        saving = False\n",
    "        # saving best models\n",
    "        if self.average_[-1] >= self.max_average:\n",
    "            self.max_average = self.average_[-1]\n",
    "            self.save()\n",
    "            # decreaate learning rate every saved model\n",
    "            self.lr *= 0.95\n",
    "            K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
    "            K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
    "            saving = True\n",
    "            print()\n",
    "            print('New record')\n",
    "            print()\n",
    "\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            # Do some logging\n",
    "            pass\n",
    "\n",
    "        return self.average_[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7e1c27a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:43.874202Z",
     "start_time": "2022-08-07T09:24:36.273128Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (22065, 8)\n",
      "Shape of DataFrame:  (754, 8)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 250 into shape (1,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#discrete_agent = PpoAgent('CartPole-v1', training_batch=4000,epochs=80,lr=3e-4,episodes=500, continuous_action_space=False)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m discrete_agent \u001b[38;5;241m=\u001b[39m PpoAgent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStockTradingEnvironment-v0\u001b[39m\u001b[38;5;124m'\u001b[39m, training_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4000\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m,lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m,episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, continuous_action_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mdiscrete_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mPpoAgent.run_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    273\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 274\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m     done, score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;66;03m# Instantiate or reset games memory\u001b[39;00m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mPpoAgent.reshape_state\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape_state\u001b[39m(\u001b[38;5;28mself\u001b[39m,state):\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\numpy\\core\\fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[1;34m(a, newshape, order)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreshape\u001b[39m(a, newshape, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;124;03m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m           [5, 6]])\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreshape\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\numpy\\core\\fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bound(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 250 into shape (1,10)"
     ]
    }
   ],
   "source": [
    "import src.environments.continuous.stock_trading  \n",
    "\n",
    "def environment():\n",
    "    env = gym.make('StockTradingEnvironment-v0',\n",
    "                   use_technical_indicators= [\n",
    "    \"macd\",\n",
    "    \"boll_ub\",\n",
    "    \"boll_lb\",\n",
    "    \"rsi_30\",\n",
    "    \"cci_30\",\n",
    "    \"dx_30\",\n",
    "    \"close_30_sma\",\n",
    "    \"close_60_sma\",\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return env\n",
    "\n",
    "#discrete_agent = PpoAgent('CartPole-v1', training_batch=4000,epochs=80,lr=3e-4,episodes=500, continuous_action_space=False)\n",
    "discrete_agent = PpoAgent('StockTradingEnvironment-v0', training_batch=4000,epochs=80,lr=3e-4,episodes=500, continuous_action_space=False)\n",
    "discrete_agent.run_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3b35d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:22.037701Z",
     "start_time": "2022-08-07T09:24:22.037701Z"
    }
   },
   "outputs": [],
   "source": [
    "continuous_agent= PpoAgent('LunarLanderContinuous-v2',training_batch=4000, epochs=80,episodes=500, continuous_action_space=True)\n",
    "#continuous_agent = PpoAgent('InvertedPendulumBulletEnv-v0',training_batch=4000, epochs=80,episodes=500, continuous_action_space=True)\n",
    "continuous_agent.run_batch()\n",
    "\n",
    "# action logp_t\n",
    "# [[-0.54015928  0.29224633]] [-1.34975245]\n",
    "# [-0.54015928  0.29224633] , [[-0.54015928  0.29224633]], -1.34975245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065735dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:22.038701Z",
     "start_time": "2022-08-07T09:24:22.038701Z"
    }
   },
   "outputs": [],
   "source": [
    "e= gym.make('LunarLanderContinuous-v2')\n",
    "e.action_space.sample(),e.action_space.sample().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2626c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:22.040701Z",
     "start_time": "2022-08-07T09:24:22.040701Z"
    }
   },
   "outputs": [],
   "source": [
    "e.reset()\n",
    "e.step(np.array([ 0.0255991 , -0.17774109]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea41580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:22.041702Z",
     "start_time": "2022-08-07T09:24:22.041702Z"
    }
   },
   "outputs": [],
   "source": [
    "e.step([ 0.0255991 , -0.17774109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffff31f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:22.042701Z",
     "start_time": "2022-08-07T09:24:22.042701Z"
    }
   },
   "outputs": [],
   "source": [
    "e.continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01dc5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-07T09:24:22.043701Z",
     "start_time": "2022-08-07T09:24:22.043701Z"
    }
   },
   "outputs": [],
   "source": [
    "e.spec.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60423528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
