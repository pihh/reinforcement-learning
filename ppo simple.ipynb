{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410d1a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:00.484581Z",
     "start_time": "2022-08-09T06:42:00.443584Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747276a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:00.500582Z",
     "start_time": "2022-08-09T06:42:00.488581Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gfootball.env as football_env\n",
    "# import numpy as np\n",
    "\n",
    "# import gym\n",
    "# import tensorflow as tf\n",
    "# from keras.callbacks import TensorBoard\n",
    "# from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "# from keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras import backend as K\n",
    "# from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "# disable_eager_execution()\n",
    "# clipping_val = 0.2\n",
    "# critic_discount = 0.5\n",
    "# entropy_beta = 0.001\n",
    "# gamma = 0.99\n",
    "# lmbda = 0.95\n",
    "\n",
    "\n",
    "# def get_advantages(values, masks, rewards):\n",
    "#     returns = []\n",
    "#     gae = 0\n",
    "#     for i in reversed(range(len(rewards))):\n",
    "#         delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "#         gae = delta + gamma * lmbda * masks[i] * gae\n",
    "#         returns.insert(0, gae + values[i])\n",
    "\n",
    "#     adv = np.array(returns) - values[:-1]\n",
    "#     return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "\n",
    "\n",
    "# def ppo_loss_print(oldpolicy_probs, advantages, rewards, values):\n",
    "#     def loss(y_true, y_pred):\n",
    "#         y_true = tf.Print(y_true, [y_true], 'y_true: ')\n",
    "#         y_pred = tf.Print(y_pred, [y_pred], 'y_pred: ')\n",
    "#         newpolicy_probs = y_pred\n",
    "#         # newpolicy_probs = y_true * y_pred\n",
    "#         newpolicy_probs = tf.Print(newpolicy_probs, [newpolicy_probs], 'new policy probs: ')\n",
    "\n",
    "#         ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))\n",
    "#         ratio = tf.Print(ratio, [ratio], 'ratio: ')\n",
    "#         p1 = ratio * advantages\n",
    "#         p2 = K.clip(ratio, min_value=1 - clipping_val, max_value=1 + clipping_val) * advantages\n",
    "#         actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "#         actor_loss = tf.Print(actor_loss, [actor_loss], 'actor_loss: ')\n",
    "#         critic_loss = K.mean(K.square(rewards - values))\n",
    "#         critic_loss = tf.Print(critic_loss, [critic_loss], 'critic_loss: ')\n",
    "#         term_a = critic_discount * critic_loss\n",
    "#         term_a = tf.Print(term_a, [term_a], 'term_a: ')\n",
    "#         term_b_2 = K.log(newpolicy_probs + 1e-10)\n",
    "#         term_b_2 = tf.Print(term_b_2, [term_b_2], 'term_b_2: ')\n",
    "#         term_b = entropy_beta * K.mean(-(newpolicy_probs * term_b_2))\n",
    "#         term_b = tf.Print(term_b, [term_b], 'term_b: ')\n",
    "#         total_loss = term_a + actor_loss - term_b\n",
    "#         total_loss = tf.Print(total_loss, [total_loss], 'total_loss: ')\n",
    "#         return total_loss\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def ppo_loss(oldpolicy_probs, advantages, rewards, values):\n",
    "#     def loss(y_true, y_pred):\n",
    "#         newpolicy_probs = y_pred\n",
    "#         ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))\n",
    "#         p1 = ratio * advantages\n",
    "#         p2 = K.clip(ratio, min_value=1 - clipping_val, max_value=1 + clipping_val) * advantages\n",
    "#         actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "#         critic_loss = K.mean(K.square(rewards - values))\n",
    "#         total_loss = critic_discount * critic_loss + actor_loss - entropy_beta * K.mean(\n",
    "#             -(newpolicy_probs * K.log(newpolicy_probs + 1e-10)))\n",
    "#         return total_loss\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def get_model_actor_image(input_dims, output_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "#     oldpolicy_probs = Input(shape=(1, output_dims,))\n",
    "#     advantages = Input(shape=(1, 1,))\n",
    "#     rewards = Input(shape=(1, 1,))\n",
    "#     values = Input(shape=(1, 1,))\n",
    "\n",
    "#     feature_extractor = MobileNetV2(include_top=False, weights='imagenet')\n",
    "\n",
    "#     for layer in feature_extractor.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Flatten(name='flatten')(feature_extractor(state_input))\n",
    "#     x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "#     out_actions = Dense(n_actions, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input, oldpolicy_probs, advantages, rewards, values],\n",
    "#                   outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-4), loss=[ppo_loss(\n",
    "#         oldpolicy_probs=oldpolicy_probs,\n",
    "#         advantages=advantages,\n",
    "#         rewards=rewards,\n",
    "#         values=values)])\n",
    "#     model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def get_model_actor_simple(input_dims, output_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "#     oldpolicy_probs = Input(shape=(1, output_dims,))\n",
    "#     advantages = Input(shape=(1, 1,))\n",
    "#     rewards = Input(shape=(1, 1,))\n",
    "#     values = Input(shape=(1, 1,))\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Dense(128, activation='relu', name='fc1')(state_input)\n",
    "#     x = Dense(128, activation='relu', name='fc2')(x)\n",
    "#     out_actions = Dense(n_actions, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input, oldpolicy_probs, advantages, rewards, values],\n",
    "#                   outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-2), loss=[ppo_loss(\n",
    "#         oldpolicy_probs=oldpolicy_probs,\n",
    "#         advantages=advantages,\n",
    "#         rewards=rewards,\n",
    "#         values=values)])\n",
    "#     # model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def get_model_critic_image(input_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "\n",
    "#     feature_extractor = MobileNetV2(include_top=False, weights='imagenet')\n",
    "\n",
    "#     for layer in feature_extractor.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Flatten(name='flatten')(feature_extractor(state_input))\n",
    "#     x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "#     out_actions = Dense(1, activation=Nonw)(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input], outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-2), loss='mse')\n",
    "#     model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def get_model_critic_simple(input_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Dense(128, activation='relu', name='fc1')(state_input)\n",
    "#     x = Dense(128, activation='relu', name='fc2')(x)\n",
    "#     out_actions = Dense(1, activation='tanh')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input], outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-2), loss='mse')\n",
    "#     # model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def test_reward():\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "   \n",
    "#     limit = 0\n",
    "#     while not done:\n",
    "#         state_input = K.expand_dims(state, 0)\n",
    "#         action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "#         action = np.argmax(action_probs)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "#         #print('test reward',reward)\n",
    "#         limit += 1\n",
    "#         if limit > 20:\n",
    "#             break\n",
    "#     print('testing...', total_reward)\n",
    "#     return total_reward\n",
    "\n",
    "\n",
    "# def one_hot_encoding(probs):\n",
    "#     one_hot = np.zeros_like(probs)\n",
    "#     one_hot[:, np.argmax(probs, axis=1)] = 1\n",
    "#     return one_hot\n",
    "\n",
    "\n",
    "# image_based = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "    \n",
    "# state = env.reset()\n",
    "# state_dims = env.observation_space.shape\n",
    "# n_actions = env.action_space.n\n",
    "\n",
    "# dummy_n = np.zeros((1, 1, n_actions))\n",
    "# dummy_1 = np.zeros((1, 1, 1))\n",
    "\n",
    "# tensor_board = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# if image_based:\n",
    "#     model_actor = get_model_actor_image(input_dims=state_dims, output_dims=n_actions)\n",
    "#     model_critic = get_model_critic_image(input_dims=state_dims)\n",
    "# else:\n",
    "#     model_actor = get_model_actor_simple(input_dims=state_dims, output_dims=n_actions)\n",
    "#     model_critic = get_model_critic_simple(input_dims=state_dims)\n",
    "\n",
    "# ppo_steps = 128\n",
    "# target_reached = False\n",
    "# best_reward = 0\n",
    "# iters = 0\n",
    "# max_iters = 50\n",
    "\n",
    "# while not target_reached and iters < max_iters:\n",
    "\n",
    "#     states = []\n",
    "#     actions = []\n",
    "#     values = []\n",
    "#     masks = []\n",
    "#     rewards = []\n",
    "#     actions_probs = []\n",
    "#     actions_onehot = []\n",
    "#     state_input = None\n",
    "\n",
    "#     for itr in range(ppo_steps):\n",
    "#         state_input = K.expand_dims(state, 0)\n",
    "#         action_dist = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "#         q_value = model_critic.predict([state_input], steps=1)\n",
    "#         action = np.random.choice(n_actions, p=action_dist[0, :])\n",
    "#         action_onehot = np.zeros(n_actions)\n",
    "#         action_onehot[action] = 1\n",
    "        \n",
    "        \n",
    "\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         #print('itr: ' + str(itr) + ', action=' + str(action) + ', reward=' + str(reward) + ', q val=' + str(q_value))\n",
    "#         mask = not done\n",
    "\n",
    "#         states.append(state)\n",
    "#         actions.append(action)\n",
    "#         actions_onehot.append(action_onehot)\n",
    "#         values.append(q_value)\n",
    "#         masks.append(mask)\n",
    "#         rewards.append(reward)\n",
    "#         actions_probs.append(action_dist)\n",
    "\n",
    "#         state = observation\n",
    "#         if done:\n",
    "#             print(action_dist)\n",
    "#             env.reset()\n",
    "\n",
    "#     q_value = model_critic.predict(state_input, steps=1)\n",
    "#     values.append(q_value)\n",
    "#     returns, advantages = get_advantages(values, masks, rewards)\n",
    "\n",
    "\n",
    "#     X_actor = [states, actions_probs, advantages, np.reshape(rewards, newshape=(-1, 1, 1)), values[:-1]]\n",
    "#     y_actor = [np.reshape(actions_onehot, newshape=(-1, n_actions))]\n",
    "\n",
    "#     X_critic = [states]\n",
    "#     y_critic = [np.reshape(returns, newshape=(-1, 1))]\n",
    " \n",
    "#     print()\n",
    "#     print('actor loss')\n",
    "#     actor_loss = model_actor.fit(X_actor,y_actor, verbose=True, shuffle=True, epochs=8,callbacks=[tensor_board])\n",
    "#     print()\n",
    "#     print('critic loss')\n",
    "#     critic_loss = model_critic.fit(X_critic, y_critic, shuffle=True, epochs=8,verbose=True, callbacks=[tensor_board])\n",
    "\n",
    "#     avg_reward = np.mean([test_reward() for _ in range(5)])\n",
    "#     print('iteration', iters, 'total test reward=' + str(avg_reward))\n",
    "#     if avg_reward > best_reward:\n",
    "#         print('best reward=' + str(avg_reward))\n",
    "#         model_actor.save('model_actor_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "#         model_critic.save('model_critic_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "#         best_reward = avg_reward\n",
    "#     if best_reward > 450 or iters > max_iters:\n",
    "#         target_reached = True\n",
    "#     iters += 1\n",
    "#     env.reset()\n",
    "\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52843701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:00.516581Z",
     "start_time": "2022-08-09T06:42:00.503582Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "# gym.make('CartPole-v1').action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4195588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:00.532584Z",
     "start_time": "2022-08-09T06:42:00.519584Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import gfootball.env as football_env\n",
    "\n",
    "# env = football_env.create_environment(env_name='academy_empty_goal', render=False)\n",
    "\n",
    "# state = env.reset()\n",
    "# total_reward = 0\n",
    "# while True:\n",
    "#     observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#     total_reward += reward\n",
    "#     if reward != 0:\n",
    "#         print('reward',reward)\n",
    "#     if done:\n",
    "#         env.reset()\n",
    "#         print('done')\n",
    "# #         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "650ebf8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:00.548580Z",
     "start_time": "2022-08-09T06:42:00.536588Z"
    }
   },
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ef5aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:21.592171Z",
     "start_time": "2022-08-09T06:42:21.574068Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()\n",
    "\n",
    "from src.environments.continuous.pendulum import environment\n",
    "#from src.environments.discrete.cartpole import environment\n",
    "from src.agents.ppo import PpoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd56e30d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:22.513467Z",
     "start_time": "2022-08-09T06:42:22.278469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    | ---------------------------------\n",
      "    | Pendulum-v1\n",
      "    | \n",
      "    | Action space: Continuous with low state-space\n",
      "    | Environment beated threshold: -200\n",
      "    | ----------------------------------------------------------   \n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "agent = PpoAgent(environment,batch_size=10000, epochs=100,n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b13d1de9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:43:30.134205Z",
     "start_time": "2022-08-09T06:42:23.591195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 50 * Moving Avg Reward is ==> -1261.82512 * Last Reward was ==> -970.31206\n",
      "\n",
      "* Will replay\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Development\\GIT\\tmp\\reinforcement-learning\\src\\agents\\ppo\\__init__.py:472\u001b[0m, in \u001b[0;36mPpoAgent.learn\u001b[1;34m(self, timesteps, log_level, log_every, success_strict, success_threshold, success_threshold_lookback, plot_results, reset)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_every \u001b[38;5;241m=\u001b[39m log_every\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mplot_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuccess_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuccess_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuccess_threshold_lookback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuccess_threshold_lookback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuccess_strict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuccess_strict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_multiprocesses(timesteps\u001b[38;5;241m=\u001b[39mtimesteps, \n\u001b[0;32m    484\u001b[0m         plot_results\u001b[38;5;241m=\u001b[39mplot_results, \n\u001b[0;32m    485\u001b[0m         reset\u001b[38;5;241m=\u001b[39mreset, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         success_strict\u001b[38;5;241m=\u001b[39msuccess_strict, \n\u001b[0;32m    491\u001b[0m         n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_workers)\n",
      "File \u001b[1;32m~\\Development\\GIT\\tmp\\reinforcement-learning\\src\\agents\\ppo\\__init__.py:262\u001b[0m, in \u001b[0;36mPpoAgent._run_batch\u001b[1;34m(self, timesteps, log_level, log_every, success_strict, success_threshold, success_threshold_lookback, plot_results, reset)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m* Will replay\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 262\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m* Will resume\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32m~\\Development\\GIT\\tmp\\reinforcement-learning\\src\\agents\\ppo\\__init__.py:157\u001b[0m, in \u001b[0;36mPpoAgent._replay\u001b[1;34m(self, buffer)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# training Actor and Critic networkers\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m a_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    158\u001b[0m c_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit([states, values], target, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_tensorboard_scaler(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,a_loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    776\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:641\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    638\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    639\u001b[0m   val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:377\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_index, batch_logs)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    379\u001b[0m   batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\backend.py:4275\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4270\u001b[0m     symbol_vals \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol_vals \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4271\u001b[0m     feed_symbols \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_symbols \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4272\u001b[0m     session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session):\n\u001b[0;32m   4273\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4275\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches):])\n\u001b[0;32m   4278\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4280\u001b[0m     fetched[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4281\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1480\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1479\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1480\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1484\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286e6cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T06:42:18.493770Z",
     "start_time": "2022-08-09T06:42:18.493770Z"
    }
   },
   "outputs": [],
   "source": [
    "#agent.actor.model.optimizer.learning_rate.value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1241d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
