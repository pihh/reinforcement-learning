{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:42:27.182513Z",
     "start_time": "2022-07-30T08:42:27.162428Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:42:31.416397Z",
     "start_time": "2022-07-30T08:42:27.184513Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from threading import Thread, Lock\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "# disable_eager_execution()\n",
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fa49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57a5ff99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:42:31.447079Z",
     "start_time": "2022-07-30T08:42:31.418305Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.utils.networks import CommonLayer\n",
    "\n",
    "class ActorNetwork():\n",
    "    def __init__(self,\n",
    "                 observation_shape,\n",
    "                 action_space_mode,\n",
    "                 action_bound,\n",
    "                 policy,\n",
    "                 n_actions, \n",
    "                 optimizer=Adam,\n",
    "                 learning_rate=0.01,\n",
    "                 std_bound = [1e-2, 1.0],\n",
    "    ):\n",
    "        \n",
    "        self.observation_shape = observation_shape\n",
    "        self.policy = policy\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space_mode = action_space_mode\n",
    "        self.std_bound=std_bound\n",
    "        self.action_bound = action_bound\n",
    "        \n",
    "        optimizer = optimizer(learning_rate)\n",
    "        \n",
    "        \n",
    "        X_input = Input(shape=self.observation_shape) \n",
    "        X = CommonLayer(X_input,self.policy)\n",
    "        \n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            action = Dense(self.n_actions, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "            self.model = Model(inputs = X_input, outputs = action)\n",
    "            self.model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "        else:\n",
    "            mu = Dense(self.n_actions, activation=\"tanh\", kernel_initializer='he_uniform')(X)\n",
    "            mu = Lambda(lambda x: x * self.action_bound)(mu)\n",
    "            sigma = Dense(self.n_actions, activation=\"softplus\", kernel_initializer='he_uniform')(X)\n",
    "            \n",
    "            self.model = Model(inputs = X_input, outputs = Concatenate()([mu,sigma]))\n",
    "            self.model.compile(loss=self.continuous_actor_loss, optimizer=optimizer)\n",
    "    \n",
    "    def log_pdf(self,mu, sigma, action):\n",
    "        std = tf.clip_by_value(sigma, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n",
    "            var * 2 * np.pi\n",
    "        )\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "    \n",
    "    def continuous_actor_loss(self, y_true, y_pred):\n",
    "        actions, advantages = y_true[:, :self.n_actions], y_true[:, self.n_actions:]\n",
    "        mu,sigma = y_pred[:,:1], y_pred[:,1:]\n",
    "        log_policy_pdf = self.log_pdf(mu,sigma,actions)\n",
    "        loss_policy = log_policy_pdf * advantages\n",
    "        \n",
    "        return tf.reduce_sum(-loss_policy)\n",
    "    \n",
    "    def act(self,state):\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            prediction = self.model.predict(state)[0]\n",
    "            action = np.random.choice(self.n_actions, p=prediction)\n",
    "            action_onehot = np.zeros([self.n_actions])\n",
    "            action_onehot[action] = 1\n",
    "        else:\n",
    "            prediction = self.model.predict(state)[0]\n",
    "            mu = prediction[0]\n",
    "            sigma = prediction[1]\n",
    "            action = np.random.normal(mu, sigma,self.n_actions)\n",
    "            action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "            action_onehot = action\n",
    "        return action, action_onehot, prediction\n",
    "    \n",
    "\n",
    "class CriticNetwork():\n",
    "    def __init__(self,\n",
    "                 observation_shape,\n",
    "                 action_space_mode,\n",
    "                 policy,\n",
    "                 n_actions, \n",
    "                 optimizer=Adam,\n",
    "                 learning_rate=0.01,\n",
    "                 std_bound = [1e-2, 1.0],\n",
    "    ):\n",
    "\n",
    "        self.observation_shape = observation_shape\n",
    "        self.policy = policy\n",
    "        self.n_actions = n_actions\n",
    "        self.action_space_mode = action_space_mode\n",
    "        self.std_bound=std_bound\n",
    "        \n",
    "        optimizer = optimizer(learning_rate)\n",
    "        \n",
    "        X_input = Input(shape=self.observation_shape) \n",
    "        X = CommonLayer(X_input,self.policy)\n",
    "        \n",
    "        value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        \n",
    "        self.model = Model(inputs = X_input, outputs = value)\n",
    "        self.model.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6fb7cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:42:31.463075Z",
     "start_time": "2022-07-30T08:42:31.449080Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.size = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.size = 0\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def remember(self, state, action_onehot, reward ):\n",
    "        self.size +=1\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "#     def sample(self, batch_size=64):\n",
    "#         max_mem = min(self.buffer_counter, self.buffer_size)\n",
    "\n",
    "#         batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "#         states = self.state_memory[batch]\n",
    "#         states_ = self.new_state_memory[batch]\n",
    "#         actions = self.action_memory[batch]\n",
    "#         rewards = self.reward_memory[batch]\n",
    "#         dones = self.done_memory[batch]\n",
    "\n",
    "#         return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7248723d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:42:31.479080Z",
     "start_time": "2022-07-30T08:42:31.464067Z"
    }
   },
   "outputs": [],
   "source": [
    "# GLOBAL_EPISODE_NUM = 0\n",
    "\n",
    "# class A3CWorker(Thread):\n",
    "#     def __init__(self, \n",
    "#                 worker_id,\n",
    "#                 env, \n",
    "#                 global_actor, \n",
    "#                 global_critic, \n",
    "#                 action_space_mode,\n",
    "#                 observation_shape,\n",
    "#                 policy,\n",
    "#                 n_actions,\n",
    "#                 actor_optimizer=RMSprop,\n",
    "#                 critic_optimizer=RMSprop,\n",
    "#                 actor_learning_rate=0.001,\n",
    "#                 critic_learning_rate=0.001,\n",
    "#                 std_bound = [1e-2, 1.0],\n",
    "#                 action_bound=False,\n",
    "#                 batch_size=64):\n",
    "#         Thread.__init__(self)\n",
    "        \n",
    "#         self.worker_id = worker_id\n",
    "#         self.env = env\n",
    "#         self.action_space_mode = action_space_mode\n",
    "#         self.observation_shape = observation_shape\n",
    "#         self.policy = policy\n",
    "#         self.n_actions = n_actions\n",
    "#         self.actor_optimizer = actor_optimizer\n",
    "#         self.critic_optimizer = critic_optimizer\n",
    "#         self.actor_learning_rate= actor_learning_rate\n",
    "#         self.critic_learning_rate=critic_learning_rate\n",
    "#         self.std_bound = std_bound\n",
    "#         self.action_bound = action_bound\n",
    "\n",
    "#         self.global_actor = global_actor\n",
    "#         self.global_critic = global_critic\n",
    "#         print('Before init networks')\n",
    "#         self.__init_networks()\n",
    "#         self.__init_buffers()\n",
    "\n",
    "#         print('After init networks')\n",
    "#         print(global_actor.model)\n",
    "#         print(self.actor.model)\n",
    "        \n",
    "# #         print('Before make predict')\n",
    "# #         self.actor.model._make_predict_function()\n",
    "# #         self.critic.model._make_predict_function()\n",
    "# #         print('After make predict')\n",
    "        \n",
    "#         print('Before set weights')\n",
    "#         self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "#         self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "#         print('After set weights')\n",
    "        \n",
    "\n",
    "        \n",
    "#     def __init_networks(self):\n",
    "#         self.actor = ActorNetwork(\n",
    "#             observation_shape=self.observation_shape,\n",
    "#             action_space_mode=self.action_space_mode,\n",
    "#             policy=self.policy,\n",
    "#             n_actions=self.n_actions, \n",
    "#             optimizer=self.actor_optimizer,\n",
    "#             learning_rate=self.actor_learning_rate,\n",
    "#             std_bound = self.std_bound,\n",
    "#             action_bound = self.action_bound\n",
    "#         )\n",
    "        \n",
    "#         self.critic = CriticNetwork(\n",
    "#             observation_shape=self.observation_shape,\n",
    "#             action_space_mode=self.action_space_mode,\n",
    "#             policy=self.policy,\n",
    "#             n_actions=self.n_actions, \n",
    "#             optimizer=self.critic_optimizer,\n",
    "#             learning_rate=self.critic_learning_rate,\n",
    "#             std_bound = self.std_bound\n",
    "#         )\n",
    "    \n",
    "#     def __init_buffers(self):\n",
    "#         self.buffer = ReplayBuffer()\n",
    "        \n",
    "    \n",
    "#     def act(self,state):\n",
    "#         action, action_onehot, prediction = self.actor.act(state)\n",
    "#         return action, action_onehot, prediction\n",
    "    \n",
    "#     def discount_rewards(self, reward):\n",
    "#         # Compute the gamma-discounted rewards over an episode\n",
    "#         gamma = 0.99    # discount rate\n",
    "#         running_add = 0\n",
    "#         discounted_r = np.zeros_like(reward)\n",
    "#         for i in reversed(range(0,len(reward))):\n",
    "#             running_add = running_add * self.gamma + reward[i]\n",
    "#             discounted_r[i] = running_add\n",
    "\n",
    "#         discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "#         discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        \n",
    "#         return discounted_r\n",
    "    \n",
    "#     def replay(self):\n",
    "#         print(self.worker_id, 'replay')\n",
    "\n",
    "#         if self.buffer.size > 1:\n",
    "#             # reshape memory to appropriate shape for training\n",
    "#             states = np.vstack(self.buffer.states)\n",
    "#             actions = np.vstack(self.buffer.actions)\n",
    "\n",
    "#             # Compute discounted rewards\n",
    "#             discounted_r = self.discount_rewards(self.buffer.rewards)\n",
    "\n",
    "#             # Get Critic network predictions\n",
    "#             values = self.global_critic.model.predict(states)[:, 0]\n",
    "#             # Compute advantages\n",
    "#             advantages = discounted_r - values\n",
    "#             # training Actor and Critic networks\n",
    "\n",
    "\n",
    "#             if self.action_space_mode == \"discrete\":\n",
    "#                 self.global_actor.model.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "#             else:\n",
    "#                 self.global_actor.model.fit(states,np.concatenate([actions,np.reshape(advantages,newshape=(len(advantages),1))],axis=1), epochs=1,verbose=0)\n",
    "\n",
    "#             self.global_critic.model.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "            \n",
    "#             # Reset weights\n",
    "#             self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "#             self.critic.model.set_weights(\n",
    "#                 self.global_critic.model.get_weights()\n",
    "#             )\n",
    "#             # reset training memory\n",
    "#             self.buffer.reset()\n",
    "        \n",
    "\n",
    "#     def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50,max_episodes=10000):\n",
    "#         global GLOBAL_EPISODE_NUM\n",
    "#         timestep=0\n",
    "#         while max_episodes >= GLOBAL_EPISODE_NUM:\n",
    "#             state = self.env.reset()\n",
    "#             score = 0\n",
    "#             done = False\n",
    "\n",
    "\n",
    "#             while not done:\n",
    "#                 # self.env.render()\n",
    "#                 #state = np.expand_dims(state, axis=0)\n",
    "#                 action, action_onehot, prediction = self.act(state)\n",
    "#                 # Retrieve new state, reward, and whether the state is terminal\n",
    "#                 next_state, reward, done, _ = self.env.step(action)\n",
    "#                 # Memorize (state, action, reward) for training\n",
    "#                 self.buffer.remember(np.expand_dims(state, axis=0), action_onehot, reward)\n",
    "#                 # Update current state\n",
    "#                 state = next_state\n",
    "#                 score += reward\n",
    "#                 timestep +=1\n",
    "                \n",
    "#                 if self.buffer.size >= self.batch_size:\n",
    "#                     self.replay()\n",
    "                \n",
    "\n",
    "#             # Episode ended\n",
    "#             self.running_reward.step(score)\n",
    "#             GLOBAL_EPISODE_NUM += 1 \n",
    "            \n",
    "#             self.learning_log.episode(\n",
    "#                 log_each_n_episodes,\n",
    "#                 score,\n",
    "#                 self.running_reward.reward, \n",
    "#                 log_level=log_level\n",
    "#             )\n",
    "#             # If done stop\n",
    "#             if self.did_finnish_learning(success_threshold,episode):\n",
    "#                 break\n",
    "                \n",
    "#             # Else learn more\n",
    "#             self.replay()\n",
    "        \n",
    "#         # End of trainig\n",
    "#         self.env.close()\n",
    "        \n",
    "#         if plot_results:\n",
    "#             self.plot_learning_results()\n",
    "\n",
    "# #             print(f\"Episode#{GLOBAL_EPISODE_NUM}, Worker#{self.worker_id}, Reward:{episode_reward}\")\n",
    "# #             tf.summary.scalar(\"episode_reward\", episode_reward, step=GLOBAL_EPISODE_NUM)\n",
    "# #             GLOBAL_EPISODE_NUM += 1\n",
    "\n",
    "#     def run(self):\n",
    "#         self.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456dff8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:42:59.667171Z",
     "start_time": "2022-07-30T08:42:59.643202Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.agent import Agent\n",
    "from src.utils.networks import CommonLayer\n",
    "    \n",
    "\n",
    "class A3CAgent(Agent):\n",
    "    def __init__(self,\n",
    "        environment,\n",
    "        gamma = 0.99,\n",
    "        policy=\"mlp\",\n",
    "        actor_optimizer=RMSprop,\n",
    "        critic_optimizer=RMSprop,\n",
    "        actor_learning_rate=0.001,\n",
    "        critic_learning_rate=0.001,\n",
    "        std_bound = [1e-2, 1.0],\n",
    "        batch_size=64,\n",
    "        n_workers=cpu_count()\n",
    "    ):\n",
    "        \n",
    "        super(A3CAgent, self).__init__(environment,args=locals())\n",
    "        \n",
    "        \n",
    "        # Args\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.std_bound = std_bound\n",
    "        self.batch_size = batch_size\n",
    "        self.policy = policy \n",
    "        self.actor_optimizer=actor_optimizer\n",
    "        self.critic_optimizer=critic_optimizer\n",
    "        self.actor_learning_rate=actor_learning_rate\n",
    "        self.critic_learning_rate=critic_learning_rate\n",
    "        self.n_workers = n_workers\n",
    "        self.lock = Lock()\n",
    "\n",
    "        # Bootstrap\n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        self._add_models_to_config([self.actor.model,self.critic.model])\n",
    "    \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = ReplayBuffer()\n",
    "    \n",
    "    def __init_networks(self):\n",
    "        self.actor = ActorNetwork(\n",
    "            observation_shape=self.observation_shape,\n",
    "            action_space_mode=self.action_space_mode,\n",
    "            policy=self.policy,\n",
    "            n_actions=self.n_actions, \n",
    "            optimizer=self.actor_optimizer,\n",
    "            learning_rate=self.actor_learning_rate,\n",
    "            std_bound = self.std_bound,\n",
    "            action_bound = self.action_bound\n",
    "        )\n",
    "        \n",
    "        self.critic = CriticNetwork(\n",
    "            observation_shape=self.observation_shape,\n",
    "            action_space_mode=self.action_space_mode,\n",
    "            policy=self.policy,\n",
    "            n_actions=self.n_actions, \n",
    "            optimizer=self.critic_optimizer,\n",
    "            learning_rate=self.critic_learning_rate,\n",
    "            std_bound = self.std_bound\n",
    "        )\n",
    "        \n",
    "#         print('Before make predict')\n",
    "#         self.global_actor.model._make_predict_function()\n",
    "#         self.global_critic.model._make_predict_function()\n",
    "#         print('After make predict')\n",
    "    \n",
    "    def act(self,state):\n",
    "        action, action_onehot, prediction = self.actor.act(state)\n",
    "        return action, action_onehot, prediction\n",
    "    \n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * self.gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        \n",
    "        return discounted_r\n",
    "    \n",
    "    def replay(self):\n",
    "\n",
    "        if self.buffer.size > 1:\n",
    "            # Lock\n",
    "            self.lock.acquire()\n",
    "            \n",
    "            # reshape memory to appropriate shape for training\n",
    "            states = np.vstack(self.buffer.states)\n",
    "            actions = np.vstack(self.buffer.actions)\n",
    "\n",
    "            # Compute discounted rewards\n",
    "            discounted_r = self.discount_rewards(self.buffer.rewards)\n",
    "\n",
    "            # Get Critic network predictions\n",
    "            values = self.critic.model.predict(states)[:, 0]\n",
    "            # Compute advantages\n",
    "            advantages = discounted_r - values\n",
    "            # training Actor and Critic networks\n",
    "\n",
    "\n",
    "            if self.action_space_mode == \"discrete\":\n",
    "                self.actor.model.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "            else:\n",
    "                self.actor.model.fit(states,np.concatenate([actions,np.reshape(advantages,newshape=(len(advantages),1))],axis=1), epochs=1,verbose=0)\n",
    "\n",
    "            self.critic.model.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "            \n",
    "            # Reset weights\n",
    "\n",
    "            # reset training memory\n",
    "            self.buffer.reset()\n",
    "            \n",
    "            # Unlock\n",
    "            self.lock.release()\n",
    "    \n",
    "    def train_threading(self,agent,env,thread,log_each_n_episodes,log_level):\n",
    "        # Refactor\n",
    "        while self.episode < self.episodes:  \n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                #state = np.expand_dims(state, axis=0)\n",
    "                action, action_onehot, prediction = agent.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                self.buffer.remember(np.expand_dims(state, axis=0), action_onehot, reward)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                self.timestep +=1\n",
    "                \n",
    "                if self.buffer.size >= self.batch_size:\n",
    "                    self.replay()\n",
    "            \n",
    "            # Episode ended\n",
    "            self.running_reward.step(score)\n",
    "            self.episode += 1\n",
    "            \n",
    "            print(thread)\n",
    "            self.learning_log.episode(\n",
    "                log_each_n_episodes,\n",
    "                score,\n",
    "                self.running_reward.reward, \n",
    "                log_level=log_level,\n",
    "                worker=thread\n",
    "            )\n",
    "            # Refactor\n",
    "            # If done stop\n",
    "#             if self.did_finnish_learning(success_threshold,episode):\n",
    "#                 break\n",
    "                \n",
    "            # Else learn more\n",
    "            self.replay()\n",
    "        \n",
    "        # End of trainig\n",
    "        self.env.close()\n",
    "\n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50,max_episodes=10000):\n",
    "        # Refactor\n",
    "        self.episodes = 1000\n",
    "        self.episode  = 0\n",
    "        self.timestep = 0\n",
    "        \n",
    "        self.env.close()\n",
    "        # Instantiate one environment per thread\n",
    "        envs = [self.environment(describe=False) for i in range(self.n_workers)]\n",
    "\n",
    "        # Create threads\n",
    "        threads = [\n",
    "            Thread(\n",
    "                target=self.train_threading,\n",
    "                daemon=True,\n",
    "                args=(\n",
    "                    self,\n",
    "                    envs[i],\n",
    "                    i,\n",
    "                    log_each_n_episodes,\n",
    "                    log_level\n",
    "                )\n",
    "            ) for i in range(self.n_workers)\n",
    "        ]\n",
    "\n",
    "        # Start them all\n",
    "        for t in threads:\n",
    "            time.sleep(2)\n",
    "            t.start()\n",
    "            \n",
    "            \n",
    "        if plot_results:\n",
    "            self.plot_learning_results()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22213459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:43:33.093626Z",
     "start_time": "2022-07-30T08:43:30.716042Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    | ---------------------------------\n",
      "    | CartPole-v1\n",
      "    | Action space: Discrete with high state-space\n",
      "    | Environment beated threshold: 200\n",
      "    | Dev notes:\n",
      "    |   * Agents that track State/Action combinations like \n",
      "    |     Q learning will fail due to high state space\n",
      "    | ----------------------------------------------------------   \n",
      "\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 2 * Moving Avg Reward is ==> 22.500 * Last Reward was ==> 22.000\n",
      "Episode * 3 * Moving Avg Reward is ==> 20.667 * Last Reward was ==> 17.000\n",
      "Episode * 4 * Moving Avg Reward is ==> 21.750 * Last Reward was ==> 25.000\n",
      "Episode * 5 * Moving Avg Reward is ==> 22.400 * Last Reward was ==> 25.000\n",
      "Episode * 6 * Moving Avg Reward is ==> 21.667 * Last Reward was ==> 18.000\n",
      "Episode * 7 * Moving Avg Reward is ==> 25.143 * Last Reward was ==> 46.000\n",
      "Episode * 8 * Moving Avg Reward is ==> 24.500 * Last Reward was ==> 20.000\n",
      "Episode * 9 * Moving Avg Reward is ==> 23.444 * Last Reward was ==> 15.000\n",
      "Episode * 10 * Moving Avg Reward is ==> 22.200 * Last Reward was ==> 11.000\n",
      "Episode * 11 * Moving Avg Reward is ==> 22.818 * Last Reward was ==> 29.000\n",
      "Episode * 50 * Moving Avg Reward is ==> 26.200 * Last Reward was ==> 77.000\n",
      "Episode * 12 * Moving Avg Reward is ==> 26.750 * Last Reward was ==> 70.000\n",
      "Episode * 13 * Moving Avg Reward is ==> 26.923 * Last Reward was ==> 29.000\n",
      "Episode * 14 * Moving Avg Reward is ==> 27.500 * Last Reward was ==> 35.000\n",
      "Episode * 15 * Moving Avg Reward is ==> 27.600 * Last Reward was ==> 29.000\n",
      "Episode * 16 * Moving Avg Reward is ==> 30.250 * Last Reward was ==> 70.000\n",
      "Episode * 17 * Moving Avg Reward is ==> 30.412 * Last Reward was ==> 33.000\n",
      "Episode * 18 * Moving Avg Reward is ==> 30.222 * Last Reward was ==> 27.000\n",
      "Episode * 19 * Moving Avg Reward is ==> 30.474 * Last Reward was ==> 35.000\n",
      "Episode * 20 * Moving Avg Reward is ==> 31.850 * Last Reward was ==> 58.000\n",
      "Episode * 21 * Moving Avg Reward is ==> 32.190 * Last Reward was ==> 39.000\n",
      "Episode * 22 * Moving Avg Reward is ==> 31.636 * Last Reward was ==> 20.000\n"
     ]
    }
   ],
   "source": [
    "from src.environments.discrete.cartpole import environment\n",
    "agent = A3CAgent(environment, n_workers=1)\n",
    "agent.learn(log_each_n_episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d09947d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T08:42:34.244761Z",
     "start_time": "2022-07-30T08:42:34.231836Z"
    }
   },
   "outputs": [],
   "source": [
    "#from src.environments.continuous.inverted_pendulum import environment\n",
    "#agent = A2CAgent(environment)\n",
    "#agent.learn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47974b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T09:31:41.377934Z",
     "start_time": "2022-07-30T09:31:41.357022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<iterator at 0x2654b364580>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 240 * Moving Avg Reward is ==> 150.080 * Last Reward was ==> 119.000\n"
     ]
    }
   ],
   "source": [
    "agent.actor.prediiter(agent.env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619b59c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
