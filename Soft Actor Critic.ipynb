{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f3e48f",
   "metadata": {},
   "source": [
    "### Soft Actor Critic\n",
    "\n",
    "#### Introduction:\n",
    "* Maximizes long term rewards and entropy\n",
    "* Similar to Q learning ( epsilon greedy - some % of the time selects random action )\n",
    "* Entropy modeled by reward scaling ( inv. relationship )\n",
    "* Networks:\n",
    "    * 1 actor network\n",
    "    * 1 value network\n",
    "    * 2 critic networks (like ddqn/td3)\n",
    "* Uses a target value function (soft update)\n",
    "* Has replay buffer\n",
    "\n",
    "#### Sampling:\n",
    "* actor outputs mu and sigma and we use a normal dist from them\n",
    "\n",
    "#### Network updates:\n",
    "* Actor:\n",
    "    * sample states from buffer, compute new actions\n",
    "    * get the minimum of two critics\n",
    "    * log is computed according to the previous slide\n",
    "* Value:\n",
    "    * use value fn (current params) for states\n",
    "    * samples states and computes new actions\n",
    "    * uses minimum value of two critics\n",
    "    * log is computed according to the prev slide\n",
    "* Target:\n",
    "    * Uses a small tau ( 0.005 for ex )\n",
    "    * Slowly moving avg of online and target network\n",
    "* Critic:\n",
    "    * target value fort new states\n",
    "    * sample states and actions\n",
    "    * reward is scaled here\n",
    "\n",
    "#### Limitations:\n",
    "* Only works in continous environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:23:26.628165Z",
     "start_time": "2022-07-29T11:23:26.605012Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:23:30.928750Z",
     "start_time": "2022-07-29T11:23:26.629661Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "from src.utils.logger import LearningLogger\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00af9100",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:23:30.944750Z",
     "start_time": "2022-07-29T11:23:30.929751Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, input_shape, n_actions):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer_counter = 0\n",
    "        self.state_memory = np.zeros((self.buffer_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.buffer_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.buffer_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.buffer_size)\n",
    "        self.done_memory = np.zeros(self.buffer_size, dtype=np.bool)\n",
    "\n",
    "    def remember(self, state, action, reward, state_, done):\n",
    "        index = self.buffer_counter % self.buffer_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.done_memory[index] = done\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        max_mem = min(self.buffer_counter, self.buffer_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3842083c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:23:30.959751Z",
     "start_time": "2022-07-29T11:23:30.946751Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "@tf.function\n",
    "def trace_call(model, state):\n",
    "    return model(state)\n",
    "\n",
    "def MultiLayerPerceptron(policy=\"mlp\"):\n",
    "    layers = []\n",
    "    if type(policy) == str:\n",
    "        if policy == \"mlp\":\n",
    "            layers.append(Dense(256, activation='relu', name=\"mlp_dense_layer_0\"))\n",
    "            layers.append(Dense(256, activation='relu', name=\"mlp_dense_layer_1\"))\n",
    "    else:\n",
    "        for i,layer in enumerate(policy):\n",
    "            layer._name = 'mlp_custom_layer_{}'.format(i)\n",
    "            layers.append(layer)\n",
    "            \n",
    "    return layers\n",
    "        \n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                policy=\"mlp\",\n",
    "                n_actions=2,\n",
    "                name='critic'\n",
    "        ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.model_name = name\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        self.q = Dense(1, activation=None)\n",
    "        self._name = name\n",
    "        \n",
    "    def call(self, state, action):\n",
    "        X = tf.concat([state, action], axis=1)\n",
    "\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "            \n",
    "        q = self.q(X)\n",
    "        return q\n",
    "\n",
    "class ValueNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 policy=\"mlp\",\n",
    "                 name='value',  \n",
    "        ):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "\n",
    "        self.model_name = name\n",
    "\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        self.v = Dense(1, activation=None)\n",
    "        self._name = name\n",
    "        \n",
    "    def call(self, state):\n",
    "        X = state\n",
    "\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "\n",
    "        v = self.v(X)\n",
    "\n",
    "        return v\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, \n",
    "            policy=\"mlp\",\n",
    "            n_actions=2,\n",
    "            max_action=1, \n",
    "            name='actor', \n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.model_name = name\n",
    "        self.max_action = max_action\n",
    "        self.noise = 1e-6\n",
    "\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        \n",
    "        self.mu = Dense(n_actions, activation=None)\n",
    "        self.sigma = Dense(n_actions, activation=None)\n",
    "        \n",
    "        self._name = name\n",
    "\n",
    "    def call(self, state):\n",
    "        X = state\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "\n",
    "        mu = self.mu(X)\n",
    "        sigma = self.sigma(X)\n",
    "        sigma = tf.clip_by_value(sigma, self.noise, 1)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_normal(self, state, reparameterize=True):\n",
    "        mu, sigma = self.call(state)\n",
    "        probabilities = tfp.distributions.Normal(mu, sigma)\n",
    "\n",
    "        if reparameterize:\n",
    "            actions = probabilities.sample() # + something else if you want to implement\n",
    "        else:\n",
    "            actions = probabilities.sample()\n",
    "\n",
    "        action = tf.math.tanh(actions)*self.max_action\n",
    "        log_probs = probabilities.log_prob(actions)\n",
    "        log_probs -= tf.math.log(1-tf.math.pow(action,2)+self.noise)\n",
    "        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)\n",
    "\n",
    "        return action, log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c377dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:23:30.990752Z",
     "start_time": "2022-07-29T11:23:30.960751Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.agent import Agent\n",
    "\n",
    "\n",
    "class SoftActorCriticAgent(Agent):\n",
    "    def __init__(self, \n",
    "            environment,\n",
    "            alpha=0.0003, \n",
    "            beta=0.0003, \n",
    "            gamma=0.99, \n",
    "            tau=0.005,\n",
    "            buffer_size=1000000, \n",
    "            policy=\"mlp\", \n",
    "            batch_size=256, \n",
    "            reward_scale=2, \n",
    "            loss_function = keras.losses.MSE, #keras.losses.Huber()\n",
    "    ):\n",
    "        super(SoftActorCriticAgent, self).__init__(environment,loss_keys=[\"actor\",\"value\",\"critic_1\",\"critic_2\"],args=locals())\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.policy = policy\n",
    "        self.reward_scale = reward_scale\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        #self._add_models_to_config([self.actor,self.critic_1,self.critic_2,self.value,self.target_value])\n",
    "        #self._init_tensorboard()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.observation_shape, self.n_actions)\n",
    "            \n",
    "    def __init_networks(self):\n",
    "        self.actor = ActorNetwork(n_actions=self.n_actions,policy=self.policy, max_action=self.env.action_space.high)\n",
    "        self.critic_1 = CriticNetwork(n_actions=self.n_actions,policy=self.policy, name='critic_1')\n",
    "        self.critic_2 = CriticNetwork(n_actions=self.n_actions,policy=self.policy, name='critic_2')\n",
    "        self.value = ValueNetwork(name='value',policy=self.policy)\n",
    "        self.target_value = ValueNetwork(name='target_value',policy=self.policy)\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=self.alpha))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.critic_2.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.value.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.target_value.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "        \n",
    "        self.models = [self.actor,self.critic_1,self.critic_2,self.value,self.target_value]\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.buffer.remember(state, action, reward, new_state, done)      \n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_value.weights\n",
    "        for i, weight in enumerate(self.value.weights):\n",
    "            weights.append(weight * tau + targets[i]*(1-tau))\n",
    "\n",
    "        self.target_value.set_weights(weights)\n",
    "        \n",
    "    def replay(self):\n",
    "        if self.buffer.buffer_counter < self.batch_size:\n",
    "            return\n",
    "    \n",
    "        state,action, reward, state_, done = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(state_, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "        \n",
    "        # Value network update\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = tf.squeeze(self.value(states),1)\n",
    "            value_= tf.squeeze(self.target_value(states_),1)\n",
    "            #self.trace_call(self.actor.sample_normal,states,'actor_graph')\n",
    "            current_policy_actions, log_probs = self.actor.sample_normal(states)\n",
    "            log_probs = tf.squeeze(log_probs,1)\n",
    "            \n",
    "            q1_new_policy = self.critic_1(states,current_policy_actions)\n",
    "            q2_new_policy = self.critic_2(states,current_policy_actions)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy,q2_new_policy))\n",
    "            \n",
    "            value_target = critic_value - log_probs\n",
    "            value_loss = 0.5 *self.loss_function(value,value_target)\n",
    "            \n",
    "            \n",
    "        value_network_gradient = tape.gradient(value_loss,self.value.trainable_variables)\n",
    "        self.value.optimizer.apply_gradients(zip(value_network_gradient, self.value.trainable_variables))\n",
    "        \n",
    "        # Actor network update\n",
    "        with tf.GradientTape() as tape:\n",
    "            # in the original paper, they reparameterize here. \n",
    "            new_policy_actions, log_probs = self.actor.sample_normal(states,reparameterize=True)\n",
    "            \n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            q1_new_policy = self.critic_1(states, new_policy_actions)\n",
    "            q2_new_policy = self.critic_2(states, new_policy_actions)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy, q2_new_policy), 1)\n",
    "        \n",
    "            actor_loss = log_probs - critic_value\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_network_gradient, self.actor.trainable_variables))\n",
    "\n",
    "        # Critic network update\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            \n",
    "            q_hat = self.reward_scale*reward + self.gamma*value_*(1-done)\n",
    "            q1_old_policy = tf.squeeze(self.critic_1(state, action), 1)\n",
    "            q2_old_policy = tf.squeeze(self.critic_2(state, action), 1)\n",
    "            critic_1_loss = 0.5 * self.loss_function(q1_old_policy, q_hat)\n",
    "            critic_2_loss = 0.5 * self.loss_function(q2_old_policy, q_hat)\n",
    "    \n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss,self.critic_1.trainable_variables)\n",
    "        critic_2_network_gradient = tape.gradient(critic_2_loss,self.critic_2.trainable_variables)\n",
    "\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(critic_2_network_gradient, self.critic_2.trainable_variables))\n",
    "\n",
    "        # log losses\n",
    "        self.learning_log.step_loss({\n",
    "            \"actor\":actor_loss.numpy(),\n",
    "            \"value\":value_loss.numpy(),\n",
    "            \"critic_1\":critic_1_loss.numpy(),\n",
    "            \"critic_2\":critic_2_loss.numpy()\n",
    "        })\n",
    "        \n",
    "        self.update_network_parameters()\n",
    "\n",
    "        # log evolution on tensorboard\n",
    "\n",
    "        self.write_tensorboard_scaler('actor_loss',tf.get_static_value(actor_loss),self.learning_log.learning_steps)\n",
    "        self.write_tensorboard_scaler('value_loss',tf.get_static_value(value_loss),self.learning_log.learning_steps)\n",
    "        self.write_tensorboard_scaler('critic_1_loss',tf.get_static_value(critic_1_loss),self.learning_log.learning_steps)\n",
    "        self.write_tensorboard_scaler('critic_2_loss',tf.get_static_value(critic_2_loss),self.learning_log.learning_steps)\n",
    "    \n",
    "    def trace_call(self,model,state,name):\n",
    "\n",
    "        try:\n",
    "            tf.summary.trace_on(graph=True, profiler=True)\n",
    "        except:\n",
    "            pass\n",
    "        out = trace_call(model, state)\n",
    "\n",
    "        with self.tensorboard_writer.as_default():\n",
    "            tf.summary.trace_export(\n",
    "                name=name, step=0, profiler_outdir=self.tensorboard_writer_log_directory\n",
    "            )\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def test(self, episodes=10, render=True, init_environment=False):\n",
    "        for episode in range(episodes):\n",
    "            try:\n",
    "                state = self.env.reset()\n",
    "            except:\n",
    "                self._Agent__init_environment()\n",
    "                state = self.env.reset()\n",
    "                \n",
    "            done = False\n",
    "            score = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                # Sample action, probs and critic\n",
    "                action = self.choose_action(state)\n",
    "\n",
    "                # Step\n",
    "                state,reward,done, info = self.env.step(action)\n",
    "\n",
    "                # Get next state\n",
    "                score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.close()\n",
    "\n",
    "            self.learning_log.episode_test_log(score,episode)\n",
    "\n",
    "            \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50,):\n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    " \n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        \n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                state_, reward, done, info = self.env.step(action)\n",
    "                score += reward\n",
    "                self.remember(state, action, reward, state_, done)\n",
    "                self.replay()\n",
    "                state = state_\n",
    "            \n",
    "            self.running_reward.step(score)\n",
    "             # Log details\n",
    "            episode += 1\n",
    "            \n",
    "            self.learning_log.episode(\n",
    "                log_each_n_episodes,\n",
    "                score,\n",
    "                self.running_reward.reward, \n",
    "                log_level=log_level\n",
    "            )\n",
    "            \n",
    "            # log scores\n",
    "            self.write_tensorboard_scaler('score',score,self.learning_log.episodes)\n",
    "           \n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                break\n",
    "\n",
    "        if plot_results:\n",
    "            self.plot_learning_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f340af",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-29T11:23:26.608Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 10 * Moving Avg Reward is ==> 31.900 * Last Reward was ==> 21.000\n",
      "Episode * 20 * Moving Avg Reward is ==> 27.550 * Last Reward was ==> 14.000\n",
      "Episode * 30 * Moving Avg Reward is ==> 25.467 * Last Reward was ==> 25.000\n",
      "Episode * 40 * Moving Avg Reward is ==> 26.750 * Last Reward was ==> 25.000\n",
      "Episode * 50 * Moving Avg Reward is ==> 28.260 * Last Reward was ==> 35.000\n",
      "Episode * 60 * Moving Avg Reward is ==> 30.800 * Last Reward was ==> 32.000\n",
      "Episode * 70 * Moving Avg Reward is ==> 46.760 * Last Reward was ==> 375.000\n",
      "Episode * 80 * Moving Avg Reward is ==> 76.900 * Last Reward was ==> 198.000\n",
      "Episode * 90 * Moving Avg Reward is ==> 108.440 * Last Reward was ==> 220.000\n"
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "\n",
    "agent= SoftActorCriticAgent(environment)\n",
    "agent.learn(log_each_n_episodes=10, success_threshold=950 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114cfcf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-29T11:23:26.609Z"
    }
   },
   "outputs": [],
   "source": [
    "#agent.tensorboard_writer_log_directory\n",
    "#'storage/environments/Pendulum/DdpgAgent/e996ee6c856f7c9cad03964bbe0fa65e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31041e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-29T11:23:26.610Z"
    }
   },
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir storage/environments/InvertedPendulumBulletEnv/logs/88e695ba9c9146f81631e0b2fc3c9926__20220727-175543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233dddc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
