{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f5a87b",
   "metadata": {},
   "source": [
    "### Quick theory\n",
    "Just like the Actor-Critic method, we have two networks:\n",
    "\n",
    "* Actor - It proposes an action given a state.\n",
    "* Critic - It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n",
    "\n",
    "DDPG uses two more techniques not present in the original DQN:\n",
    "\n",
    "1. Uses two Target networks.\n",
    "        Why? Because it add stability to training. In short, we are learning from estimated targets and Target networks are updated slowly, hence keeping our estimated targets stable.Conceptually, this is like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\", as opposed to saying \"I'm going to re-learn how to play this entire game after every move\". See this StackOverflow answer.\n",
    "\n",
    "2. Uses Experience Replay.\n",
    "\n",
    "### Losses:\n",
    "\n",
    "**Critic loss** - Mean Squared Error of y - Q(s, a) where y is the expected return as seen by the Target network, and Q(s, a) is action value predicted by the Critic network. y is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n",
    "\n",
    "**Actor loss** - This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "Hence we update the Actor network so that it produces actions that get the maximum predicted value as seen by the Critic, for a given state.\n",
    "\n",
    "### Initialization:\n",
    "The initialization for last layer of the Actor must be between -0.003 and 0.003 as this prevents us from getting 1 or -1 output values in the initial stages, which would squash our gradients to zero, as we use the tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:46.673827Z",
     "start_time": "2022-07-22T11:01:46.669617Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:46.850736Z",
     "start_time": "2022-07-22T11:01:46.845708Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a26d83a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:47.020377Z",
     "start_time": "2022-07-22T11:01:47.013376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  [2.]\n",
      "Min Value of Action ->  [-2.]\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high#[0]\n",
    "lower_bound = env.action_space.low#[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3264a114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:47.165415Z",
     "start_time": "2022-07-22T11:01:47.158387Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To implement better exploration by the Actor network, we use noisy perturbations, \n",
    "specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. \n",
    "It samples noise from a correlated normal distribution.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c167df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:47.305457Z",
     "start_time": "2022-07-22T11:01:47.299457Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48727e8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:03:26.596582Z",
     "start_time": "2022-07-22T11:03:26.572988Z"
    }
   },
   "outputs": [],
   "source": [
    "class DdpgAgent(Agent):\n",
    "    def __init__(self,\n",
    "                 environment,\n",
    "                 gamma = 0.99,\n",
    "                 tau= 0.005,\n",
    "                 std_dev = 0.2,\n",
    "                 critic_lr = 0.002,\n",
    "                 actor_lr = 0.001,\n",
    "                 buffer_size=50000,\n",
    "                 batch_size=64,\n",
    "                 critic_optimizer = tf.keras.optimizers.Adam,\n",
    "                 actor_optimizer = tf.keras.optimizers.Adam,\n",
    "        ):\n",
    "        super(DdpgAgent,self).__init__(environment)\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "\n",
    "        self.noise = OUActionNoise(mean=np.zeros(self.n_actions), std_deviation=float(std_dev) * np.ones(self.n_actions))\n",
    "        \n",
    "        self.critic_optimizer = critic_optimizer(critic_lr)\n",
    "        self.actor_optimizer = actor_optimizer(actor_lr)\n",
    "\n",
    "        # Discount factor for future rewards\n",
    "        self.gamma = gamma\n",
    "        # Used to update target networks\n",
    "        self.tau = tau\n",
    "\n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = Buffer(self.buffer_size, self.batch_size)\n",
    "            \n",
    "    def __init_networks(self):\n",
    "        \n",
    "        def create_actor():\n",
    "            # Initialize weights between -3e-3 and 3-e3\n",
    "            last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "            inputs = layers.Input(shape=self.env.observation_space.shape)\n",
    "            out = layers.Flatten()(inputs)\n",
    "            out = layers.Dense(256, activation=\"relu\")(out)\n",
    "            out = layers.Dense(256, activation=\"relu\")(out)\n",
    "            outputs = layers.Dense(self.n_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "            # Our upper bound is 2.0 for Pendulum.\n",
    "            outputs = outputs * self.action_upper_bounds\n",
    "            return tf.keras.Model(inputs, outputs)\n",
    "        \n",
    "        def create_critic():\n",
    "            # State as input\n",
    "            state_input = layers.Input(shape=self.env.observation_space.shape)\n",
    "            state_out = layers.Flatten()(state_input)\n",
    "            state_out = layers.Dense(16, activation=\"relu\")(state_out)\n",
    "            state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "            # Action as input\n",
    "            action_input = layers.Input(shape=(self.n_actions))\n",
    "            action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "            # Both are passed through seperate layer before concatenating\n",
    "            concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "            out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "            out = layers.Dense(256, activation=\"relu\")(out)\n",
    "            outputs = layers.Dense(1)(out)\n",
    "\n",
    "            # Outputs single value for give state-action\n",
    "            return tf.keras.Model([state_input, action_input], outputs)\n",
    "        \n",
    "        self.actor = create_actor()\n",
    "        self.target_actor = create_actor()\n",
    "        \n",
    "        self.critic = create_critic()\n",
    "        self.target_critic = create_critic()\n",
    "        \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        sampled_actions = tf.squeeze(self.actor(state))\n",
    "        noise = self.noise()\n",
    "        # Adding noise to action\n",
    "        sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "        # We make sure action is within bounds\n",
    "        legal_action = np.clip(sampled_actions, self.action_lower_bounds, self.action_upper_bounds)\n",
    "\n",
    "        return [np.squeeze(legal_action)]\n",
    "    \n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target(self,target_weights, weights, tau):\n",
    "        for (a, b) in zip(target_weights, weights):\n",
    "            a.assign(b * tau + a * (1 - tau))\n",
    "            \n",
    "    @tf.function\n",
    "    def update(self,\n",
    "        state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(state_batch, training=True)\n",
    "            critic_value = self.critic([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor.trainable_variables)\n",
    "        )\n",
    "            \n",
    "    def replay(self):\n",
    "\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
    "        \n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.buffer.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "    def test(self, episodes=10, render=True):\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                    \n",
    "                action = self.choose_action(state)\n",
    "\n",
    "                # Step\n",
    "                state,reward,done, info = self.env.step(action)\n",
    "\n",
    "                # Get next state\n",
    "                score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.close()\n",
    "\n",
    "            print(\"Test episode: {}, score: {:.2f}\".format(episode,score)) \n",
    "    \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, log_each_n_episodes=100, success_threshold=False):\n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    "\n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            prev_state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                \n",
    "                action = self.choose_action(prev_state)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                self.buffer.record((prev_state, action, reward, state))\n",
    "                self.replay()\n",
    "                self.update_target(self.target_actor.variables, self.actor.variables, self.tau)\n",
    "                self.update_target(self.target_critic.variables, self.critic.variables, self.tau)\n",
    "                \n",
    "                score += reward\n",
    "                timestep+=1\n",
    "                prev_state=state\n",
    "                \n",
    " \n",
    "            self.running_reward.step(score)\n",
    "            # Log details\n",
    "            episode += 1\n",
    "            if episode % log_each_n_episodes == 0 and episode > 0:\n",
    "                print('episode {}, running reward: {:.2f}, last reward: {:.2f}'.format(episode,self.running_reward.reward, score))\n",
    "\n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                break\n",
    "                        \n",
    "                prev_state = state\n",
    "\n",
    "        if plot_results:\n",
    "            self.plot_learning_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc4915",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-22T11:03:27.634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| Pendulum-v1\n",
      "| \n",
      "| Action space: Continuous with low state-space\n",
      "| Environment beated threshold: -200\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "episode 1, running reward: -65.81, last reward: -1316.26\n",
      "episode 2, running reward: -141.50, last reward: -1579.49\n",
      "episode 3, running reward: -218.18, last reward: -1675.20\n",
      "episode 4, running reward: -277.80, last reward: -1410.63\n",
      "episode 5, running reward: -327.33, last reward: -1268.31\n",
      "episode 6, running reward: -377.54, last reward: -1331.58\n",
      "episode 7, running reward: -428.02, last reward: -1387.09\n",
      "episode 8, running reward: -471.11, last reward: -1289.82\n",
      "episode 9, running reward: -526.14, last reward: -1571.71\n",
      "episode 10, running reward: -562.35, last reward: -1250.40\n",
      "episode 11, running reward: -579.30, last reward: -901.38\n",
      "episode 12, running reward: -583.09, last reward: -655.09\n",
      "episode 13, running reward: -592.88, last reward: -778.85\n",
      "episode 14, running reward: -599.21, last reward: -719.53\n",
      "episode 15, running reward: -576.15, last reward: -137.87\n",
      "episode 16, running reward: -554.25, last reward: -138.31\n",
      "episode 17, running reward: -589.66, last reward: -1262.32\n",
      "episode 18, running reward: -566.56, last reward: -127.70\n",
      "episode 19, running reward: -538.30, last reward: -1.44\n",
      "episode 20, running reward: -530.51, last reward: -382.45\n",
      "episode 21, running reward: -576.52, last reward: -1450.68\n",
      "episode 22, running reward: -563.18, last reward: -309.70\n",
      "episode 23, running reward: -552.95, last reward: -358.62\n",
      "episode 24, running reward: -531.59, last reward: -125.75\n",
      "episode 25, running reward: -517.33, last reward: -246.29\n",
      "episode 26, running reward: -503.88, last reward: -248.32\n",
      "episode 27, running reward: -484.80, last reward: -122.32\n",
      "episode 28, running reward: -472.86, last reward: -245.94\n",
      "episode 29, running reward: -455.63, last reward: -128.43\n",
      "episode 30, running reward: -458.81, last reward: -519.10\n",
      "episode 31, running reward: -459.70, last reward: -476.63\n",
      "episode 32, running reward: -461.43, last reward: -494.34\n",
      "episode 33, running reward: -462.32, last reward: -479.16\n",
      "episode 34, running reward: -445.70, last reward: -129.92\n",
      "episode 35, running reward: -429.90, last reward: -129.72\n",
      "episode 36, running reward: -420.99, last reward: -251.66\n",
      "episode 37, running reward: -406.60, last reward: -133.35\n",
      "episode 38, running reward: -411.23, last reward: -499.17\n",
      "episode 39, running reward: -409.54, last reward: -377.46\n",
      "episode 40, running reward: -395.80, last reward: -134.68\n",
      "episode 41, running reward: -388.20, last reward: -243.77\n",
      "episode 42, running reward: -375.49, last reward: -134.11\n",
      "episode 43, running reward: -374.53, last reward: -356.30\n",
      "episode 44, running reward: -362.24, last reward: -128.64\n",
      "episode 45, running reward: -350.44, last reward: -126.25\n",
      "episode 46, running reward: -339.70, last reward: -135.71\n",
      "episode 47, running reward: -335.03, last reward: -246.15\n",
      "episode 48, running reward: -330.00, last reward: -234.53\n",
      "episode 49, running reward: -320.10, last reward: -131.93\n",
      "episode 50, running reward: -310.51, last reward: -128.33\n",
      "episode 51, running reward: -301.57, last reward: -131.75\n",
      "episode 52, running reward: -301.11, last reward: -292.40\n",
      "episode 53, running reward: -292.69, last reward: -132.66\n",
      "episode 54, running reward: -289.83, last reward: -235.44\n",
      "episode 55, running reward: -281.90, last reward: -131.35\n",
      "episode 56, running reward: -274.54, last reward: -134.55\n",
      "episode 57, running reward: -273.16, last reward: -246.95\n",
      "episode 58, running reward: -266.19, last reward: -133.83\n",
      "episode 59, running reward: -269.44, last reward: -331.21\n",
      "episode 60, running reward: -272.83, last reward: -337.18\n",
      "episode 61, running reward: -265.76, last reward: -131.38\n",
      "episode 62, running reward: -258.68, last reward: -124.19\n",
      "episode 63, running reward: -257.78, last reward: -240.65\n",
      "episode 64, running reward: -245.37, last reward: -9.70\n",
      "episode 65, running reward: -248.01, last reward: -298.09\n",
      "episode 66, running reward: -241.97, last reward: -127.16\n",
      "episode 67, running reward: -230.37, last reward: -10.03\n",
      "episode 68, running reward: -219.24, last reward: -7.79\n",
      "episode 69, running reward: -214.78, last reward: -130.12\n",
      "episode 70, running reward: -210.56, last reward: -130.34\n",
      "episode 71, running reward: -206.47, last reward: -128.64\n",
      "episode 72, running reward: -213.64, last reward: -349.95\n",
      "episode 73, running reward: -209.02, last reward: -121.18\n",
      "episode 74, running reward: -215.22, last reward: -333.16\n",
      "episode 75, running reward: -218.79, last reward: -286.62\n",
      "episode 76, running reward: -219.94, last reward: -241.63\n",
      "episode 77, running reward: -215.37, last reward: -128.65\n",
      "episode 78, running reward: -211.02, last reward: -128.32\n",
      "episode 79, running reward: -212.68, last reward: -244.17\n",
      "episode 80, running reward: -208.41, last reward: -127.37\n",
      "episode 81, running reward: -216.07, last reward: -361.63\n",
      "episode 82, running reward: -211.68, last reward: -128.22\n",
      "episode 83, running reward: -207.55, last reward: -129.05\n",
      "episode 84, running reward: -203.14, last reward: -119.45\n",
      "episode 85, running reward: -209.35, last reward: -327.27\n",
      "episode 86, running reward: -205.39, last reward: -130.09\n",
      "episode 87, running reward: -201.19, last reward: -121.39\n",
      "episode 88, running reward: -197.30, last reward: -123.52\n",
      "episode 89, running reward: -193.80, last reward: -127.15\n",
      "episode 90, running reward: -196.25, last reward: -242.81\n",
      "episode 91, running reward: -198.39, last reward: -239.18\n",
      "episode 92, running reward: -194.53, last reward: -121.19\n",
      "episode 93, running reward: -190.95, last reward: -122.95\n",
      "episode 94, running reward: -193.14, last reward: -234.71\n",
      "episode 95, running reward: -183.63, last reward: -2.90\n",
      "episode 96, running reward: -180.49, last reward: -120.78\n",
      "episode 97, running reward: -177.80, last reward: -126.80\n",
      "episode 98, running reward: -175.02, last reward: -122.19\n",
      "episode 99, running reward: -166.38, last reward: -2.10\n",
      "episode 100, running reward: -181.75, last reward: -473.95\n",
      "episode 101, running reward: -172.94, last reward: -5.37\n",
      "episode 102, running reward: -176.63, last reward: -246.85\n",
      "episode 103, running reward: -180.09, last reward: -245.81\n",
      "episode 104, running reward: -177.54, last reward: -129.07\n",
      "episode 105, running reward: -174.66, last reward: -119.93\n",
      "episode 106, running reward: -172.00, last reward: -121.53\n",
      "episode 107, running reward: -169.65, last reward: -124.93\n",
      "episode 108, running reward: -167.53, last reward: -127.35\n",
      "episode 109, running reward: -165.12, last reward: -119.30\n",
      "episode 110, running reward: -175.30, last reward: -368.59\n",
      "episode 111, running reward: -178.50, last reward: -239.45\n",
      "episode 112, running reward: -181.51, last reward: -238.57\n",
      "episode 113, running reward: -178.75, last reward: -126.40\n",
      "episode 114, running reward: -170.02, last reward: -4.07\n",
      "episode 115, running reward: -168.06, last reward: -130.85\n",
      "episode 116, running reward: -166.11, last reward: -129.05\n",
      "episode 117, running reward: -176.89, last reward: -381.73\n",
      "episode 118, running reward: -174.24, last reward: -123.94\n",
      "episode 119, running reward: -171.78, last reward: -124.91\n",
      "episode 120, running reward: -174.98, last reward: -235.80\n",
      "episode 121, running reward: -172.39, last reward: -123.25\n",
      "episode 122, running reward: -182.07, last reward: -366.03\n",
      "episode 123, running reward: -179.40, last reward: -128.56\n",
      "episode 124, running reward: -176.54, last reward: -122.37\n",
      "episode 125, running reward: -180.64, last reward: -258.38\n",
      "episode 126, running reward: -189.77, last reward: -363.20\n",
      "episode 127, running reward: -186.62, last reward: -126.85\n",
      "episode 128, running reward: -189.67, last reward: -247.71\n",
      "episode 129, running reward: -186.14, last reward: -118.92\n",
      "episode 130, running reward: -176.90, last reward: -1.38\n",
      "episode 131, running reward: -174.20, last reward: -122.84\n",
      "episode 132, running reward: -165.59, last reward: -2.13\n",
      "episode 133, running reward: -163.46, last reward: -122.91\n",
      "episode 134, running reward: -161.09, last reward: -116.09\n",
      "episode 135, running reward: -169.24, last reward: -324.07\n",
      "episode 136, running reward: -172.46, last reward: -233.74\n",
      "episode 137, running reward: -169.90, last reward: -121.28\n",
      "episode 138, running reward: -179.11, last reward: -354.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 139, running reward: -176.22, last reward: -121.34\n",
      "episode 140, running reward: -180.17, last reward: -255.04\n",
      "episode 141, running reward: -183.56, last reward: -248.15\n",
      "episode 142, running reward: -180.71, last reward: -126.52\n",
      "episode 143, running reward: -171.84, last reward: -3.33\n",
      "episode 144, running reward: -175.47, last reward: -244.29\n",
      "episode 145, running reward: -173.04, last reward: -126.90\n",
      "episode 146, running reward: -170.83, last reward: -128.97\n",
      "episode 147, running reward: -168.54, last reward: -125.01\n",
      "episode 148, running reward: -160.22, last reward: -2.15\n",
      "episode 149, running reward: -170.68, last reward: -369.38\n",
      "episode 150, running reward: -180.41, last reward: -365.21\n",
      "episode 151, running reward: -177.57, last reward: -123.58\n",
      "episode 152, running reward: -168.82, last reward: -2.57\n",
      "episode 153, running reward: -172.64, last reward: -245.33\n",
      "episode 154, running reward: -169.99, last reward: -119.63\n",
      "episode 155, running reward: -173.06, last reward: -231.36\n",
      "episode 156, running reward: -171.12, last reward: -134.22\n",
      "episode 157, running reward: -168.79, last reward: -124.49\n",
      "episode 158, running reward: -166.46, last reward: -122.20\n",
      "episode 159, running reward: -163.88, last reward: -114.83\n",
      "episode 160, running reward: -167.72, last reward: -240.83\n",
      "episode 161, running reward: -165.59, last reward: -125.10\n",
      "episode 162, running reward: -163.47, last reward: -123.14\n",
      "episode 163, running reward: -161.39, last reward: -121.96\n",
      "episode 164, running reward: -159.82, last reward: -129.89\n",
      "episode 165, running reward: -163.58, last reward: -234.97\n",
      "episode 166, running reward: -167.75, last reward: -247.11\n",
      "episode 167, running reward: -165.46, last reward: -121.97\n",
      "episode 168, running reward: -175.26, last reward: -361.44\n",
      "episode 169, running reward: -172.64, last reward: -122.84\n",
      "episode 170, running reward: -182.46, last reward: -368.99\n",
      "episode 171, running reward: -186.79, last reward: -269.12\n",
      "episode 172, running reward: -183.69, last reward: -124.82\n",
      "episode 173, running reward: -180.71, last reward: -123.95\n",
      "episode 174, running reward: -178.12, last reward: -128.96\n",
      "episode 175, running reward: -169.38, last reward: -3.41\n",
      "episode 176, running reward: -172.16, last reward: -224.94\n",
      "episode 177, running reward: -169.78, last reward: -124.62\n",
      "episode 178, running reward: -167.92, last reward: -132.48\n",
      "episode 179, running reward: -176.91, last reward: -347.76\n",
      "episode 180, running reward: -185.61, last reward: -350.86\n",
      "episode 181, running reward: -182.36, last reward: -120.70\n",
      "episode 182, running reward: -179.64, last reward: -127.97\n",
      "episode 183, running reward: -181.90, last reward: -224.81\n",
      "episode 184, running reward: -173.07, last reward: -5.29\n",
      "episode 185, running reward: -170.45, last reward: -120.62\n",
      "episode 186, running reward: -168.15, last reward: -124.45\n",
      "episode 187, running reward: -185.37, last reward: -512.65\n",
      "episode 188, running reward: -182.91, last reward: -136.17\n",
      "episode 189, running reward: -180.75, last reward: -139.63\n",
      "episode 190, running reward: -183.08, last reward: -227.28\n",
      "episode 191, running reward: -191.70, last reward: -355.52\n",
      "episode 192, running reward: -194.84, last reward: -254.50\n",
      "episode 193, running reward: -215.94, last reward: -616.81\n"
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "agent = DdpgAgent(environment)\n",
    "agent.learn(log_each_n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ae790a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:03:12.758110Z",
     "start_time": "2022-07-22T11:03:12.758110Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdbac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
