{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T10:18:57.570866Z",
     "start_time": "2022-07-29T10:18:57.547499Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T10:19:01.840435Z",
     "start_time": "2022-07-29T10:18:57.572814Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from multiprocessing import cpu_count\n",
    "from threading import Thread\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fa49b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5ff99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6fb7cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T10:19:01.856046Z",
     "start_time": "2022-07-29T10:19:01.841427Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.size = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.size = 0\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def remember(self, state, action_onehot, reward ):\n",
    "        self.size +=1\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "#     def sample(self, batch_size=64):\n",
    "#         max_mem = min(self.buffer_counter, self.buffer_size)\n",
    "\n",
    "#         batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "#         states = self.state_memory[batch]\n",
    "#         states_ = self.new_state_memory[batch]\n",
    "#         actions = self.action_memory[batch]\n",
    "#         rewards = self.reward_memory[batch]\n",
    "#         dones = self.done_memory[batch]\n",
    "\n",
    "#         return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7248723d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T10:19:57.501054Z",
     "start_time": "2022-07-29T10:19:57.465657Z"
    }
   },
   "outputs": [],
   "source": [
    "GLOBAL_EPISODE_NUM = 0\n",
    "\n",
    "class A3CWorker(Thread):\n",
    "    def __init__(self, \n",
    "                worker_id,\n",
    "                env, \n",
    "                global_actor, \n",
    "                global_critic, \n",
    "                action_space_mode,\n",
    "                observation_shape,\n",
    "                policy,\n",
    "                n_actions,\n",
    "                actor_optimizer,\n",
    "                critic_optimizer,\n",
    "                std_bound,\n",
    "                max_episodes = 10000):\n",
    "        Thread.__init__(self)\n",
    "        \n",
    "        self.worker_id = worker_id\n",
    "        self.env = env\n",
    "        self.action_space_mode = action_space_mode\n",
    "        self.observation_shape = observation_shape\n",
    "        self.policy = policy\n",
    "        self.n_actions = n_actions\n",
    "        self.actor_optimizer = actor_optimizer\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.std_bound = std_bound\n",
    "        self.max_episodes = 10000\n",
    "\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "        self.__init_networks()\n",
    "\n",
    "        #self.actor.set_weights(self.global_actor.get_weights())\n",
    "        #self.critic.set_weights(self.global_critic.get_weights())\n",
    "\n",
    "    def __init_networks(self):\n",
    "        X_input = Input(shape=self.observation_shape) \n",
    "        X = CommonLayer(X_input,self.policy,rename=False)\n",
    "        \n",
    "        action = Dense(self.n_actions, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "        value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        \n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            action = Dense(self.n_actions, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "            self.actor = Model(inputs = X_input, outputs = action)\n",
    "            self.actor.compile(loss='categorical_crossentropy', optimizer=self.actor_optimizer)\n",
    "        else:\n",
    "            mu = Dense(self.n_actions, activation=\"tanh\", kernel_initializer='he_uniform')(X)\n",
    "            mu = Lambda(lambda x: x * self.action_bound)(mu)\n",
    "            sigma = Dense(self.n_actions, activation=\"softplus\", kernel_initializer='he_uniform')(X)\n",
    "            \n",
    "            self.actor = Model(inputs = X_input, outputs = Concatenate()([mu,sigma]))\n",
    "            self.actor.compile(loss=self.continuous_actor_loss, optimizer=self.actor_optimizer)\n",
    "        \n",
    "        self.critic = Model(inputs = X_input, outputs = value)\n",
    "        self.critic.compile(loss='mse', optimizer=self.critic_optimizer)\n",
    "    \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = ReplayBuffer()\n",
    "        \n",
    "    def log_pdf(self,mu, sigma, action):\n",
    "        std = tf.clip_by_value(sigma, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n",
    "            var * 2 * np.pi\n",
    "        )\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "    \n",
    "    def continuous_actor_loss(self, y_true, y_pred):\n",
    "        actions, advantages = y_true[:, :1], y_true[:, 1:]\n",
    "        mu,sigma = y_pred[:,:1], y_pred[:,1:]\n",
    "        log_policy_pdf = self.log_pdf(mu,sigma,actions)\n",
    "        loss_policy = log_policy_pdf * advantages\n",
    "        \n",
    "        return tf.reduce_sum(-loss_policy)\n",
    "\n",
    "    def act(self,state):\n",
    "  \n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            prediction = self.global_actor.predict(state)[0]\n",
    "            action = np.random.choice(self.n_actions, p=prediction)\n",
    "            action_onehot = np.zeros([self.n_actions])\n",
    "            action_onehot[action] = 1\n",
    "        else:\n",
    "            prediction = self.global_actor.predict(state)[0]\n",
    "            mu = prediction[0]\n",
    "            sigma = prediction[1]\n",
    "            action = np.random.normal(mu, sigma,self.n_actions)\n",
    "            action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "            action_onehot = action\n",
    "        return action, action_onehot, prediction\n",
    "    \n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * self.gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        \n",
    "        return discounted_r\n",
    "    \n",
    "    def replay(self):\n",
    "\n",
    "        if self.buffer.size > 1:\n",
    "            # reshape memory to appropriate shape for training\n",
    "            states = np.vstack(self.buffer.states)\n",
    "            actions = np.vstack(self.buffer.actions)\n",
    "\n",
    "            # Compute discounted rewards\n",
    "            discounted_r = self.discount_rewards(self.buffer.rewards)\n",
    "\n",
    "            # Get Critic network predictions\n",
    "            values = self.global_critic.predict(states)[:, 0]\n",
    "            # Compute advantages\n",
    "            advantages = discounted_r - values\n",
    "            # training Actor and Critic networks\n",
    "\n",
    "\n",
    "            if self.action_space_mode == \"discrete\":\n",
    "                self.global_actor.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "            else:\n",
    "                self.global_actor.fit(states,np.concatenate([actions,np.reshape(advantages,newshape=(len(advantages),1))],axis=1), epochs=1,verbose=0)\n",
    "\n",
    "            self.global_critic.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "            \n",
    "            # Reset weights\n",
    "            self.actor.set_weights(self.global_actor.get_weights())\n",
    "            self.critic.set_weights(\n",
    "                self.global_critic.get_weights()\n",
    "            )\n",
    "            # reset training memory\n",
    "            self.buffer.reset()\n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "        global GLOBAL_EPISODE_NUM\n",
    "        while self.max_episodes >= GLOBAL_EPISODE_NUM:\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while not done:\n",
    "                # self.env.render()\n",
    "                state = np.expand_dims(state, axis=0)\n",
    "                action = self.act(state)\n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append(reward)\n",
    "\n",
    "                if len(state_batch) >= args.update_interval or done:\n",
    "                    states = np.array([state.squeeze() for state in state_batch])\n",
    "                    actions = np.array([action.squeeze() for action in action_batch])\n",
    "                    rewards = np.array([reward.squeeze() for reward in reward_batch])\n",
    "                    next_v_value = self.critic.model.predict(next_state)\n",
    "                    td_targets = self.n_step_td_target(rewards, next_v_value, done)\n",
    "                    advantages = td_targets - self.critic.model.predict(states)\n",
    "\n",
    "                    actor_loss = self.global_actor.train(states, actions, advantages)\n",
    "                    critic_loss = self.global_critic.train(states, td_targets)\n",
    "\n",
    "                    self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "                    self.critic.model.set_weights(\n",
    "                        self.global_critic.model.get_weights()\n",
    "                    )\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "\n",
    "                episode_reward += reward[0][0]\n",
    "                state = next_state[0]\n",
    "\n",
    "            print(f\"Episode#{GLOBAL_EPISODE_NUM}, Worker#{self.worker_id}, Reward:{episode_reward}\")\n",
    "            tf.summary.scalar(\"episode_reward\", episode_reward, step=GLOBAL_EPISODE_NUM)\n",
    "            GLOBAL_EPISODE_NUM += 1\n",
    "\n",
    "    def run(self):\n",
    "        self.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "456dff8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T10:19:57.841031Z",
     "start_time": "2022-07-29T10:19:57.815994Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.agent import Agent\n",
    "from src.utils.networks import CommonLayer\n",
    "    \n",
    "\n",
    "class A3CAgent(Agent):\n",
    "    def __init__(self,\n",
    "        environment,\n",
    "        gamma = 0.99,\n",
    "        policy=\"mlp\",\n",
    "        actor_optimizer=RMSprop(0.0001),\n",
    "        critic_optimizer=RMSprop(0.0001),\n",
    "        std_bound = [1e-2, 1.0],\n",
    "        batch_size=64,\n",
    "        n_workers=cpu_count()\n",
    "    ):\n",
    "        \n",
    "        super(A3CAgent, self).__init__(environment,args=locals())\n",
    "        \n",
    "        \n",
    "        # Args\n",
    "        self.environment = environment\n",
    "        self.gamma = gamma\n",
    "        self.std_bound = std_bound\n",
    "        self.batch_size = batch_size\n",
    "        self.policy = policy \n",
    "        self.actor_optimizer=actor_optimizer\n",
    "        self.critic_optimizer=critic_optimizer\n",
    "        self.n_workers = n_workers\n",
    "\n",
    "        # Bootstrap\n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        self._add_models_to_config([self.global_actor,self.global_critic])\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        X_input = Input(shape=self.observation_shape) \n",
    "        X = CommonLayer(X_input,self.policy)\n",
    "        \n",
    "        action = Dense(self.n_actions, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "        value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        \n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            action = Dense(self.n_actions, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "            self.global_actor = Model(inputs = X_input, outputs = action)\n",
    "            self.global_actor.compile(loss='categorical_crossentropy', optimizer=self.actor_optimizer)\n",
    "        else:\n",
    "            mu = Dense(self.n_actions, activation=\"tanh\", kernel_initializer='he_uniform')(X)\n",
    "            mu = Lambda(lambda x: x * self.action_bound)(mu)\n",
    "            sigma = Dense(self.n_actions, activation=\"softplus\", kernel_initializer='he_uniform')(X)\n",
    "            \n",
    "            self.global_actor = Model(inputs = X_input, outputs = Concatenate()([mu,sigma]))\n",
    "            self.global_actor.compile(loss=self.continuous_actor_loss, optimizer=self.actor_optimizer)\n",
    "        \n",
    "        self.global_critic = Model(inputs = X_input, outputs = value)\n",
    "        self.global_critic.compile(loss='mse', optimizer=self.critic_optimizer)\n",
    "    \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = ReplayBuffer()\n",
    "        \n",
    "    def log_pdf(self,mu, sigma, action):\n",
    "        std = tf.clip_by_value(sigma, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n",
    "            var * 2 * np.pi\n",
    "        )\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "    \n",
    "    def continuous_actor_loss(self, y_true, y_pred):\n",
    "        actions, advantages = y_true[:, :1], y_true[:, 1:]\n",
    "        mu,sigma = y_pred[:,:1], y_pred[:,1:]\n",
    "        log_policy_pdf = self.log_pdf(mu,sigma,actions)\n",
    "        loss_policy = log_policy_pdf * advantages\n",
    "        \n",
    "        return tf.reduce_sum(-loss_policy)\n",
    "\n",
    "    \n",
    "\n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50,max_episodes=10000):\n",
    "        workers = []\n",
    "\n",
    "        for i in range(self.n_workers):\n",
    "            env = self.environment()\n",
    "            workers.append(\n",
    "                A3CWorker(\n",
    "                    i,\n",
    "                    self.env, \n",
    "                    self.global_actor, \n",
    "                    self.global_critic, \n",
    "                    self.action_space_mode,\n",
    "                    self.observation_shape,\n",
    "                    self.policy,\n",
    "                    self.n_actions,\n",
    "                    self.actor_optimizer,\n",
    "                    self.critic_optimizer,\n",
    "                    self.std_bound,\n",
    "                    max_episodes\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22213459",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T10:19:58.467058Z",
     "start_time": "2022-07-29T10:19:58.320164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\filip\\AppData\\Local\\Temp\\ipykernel_25144\\3697331783.py\", line 195, in run\n",
      "  File \"C:\\Users\\filip\\AppData\\Local\\Temp\\ipykernel_25144\\3697331783.py\", line 154, in learn\n",
      "  File \"C:\\Users\\filip\\AppData\\Local\\Temp\\ipykernel_25144\\3697331783.py\", line 81, in act\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py\", line 970, in predict\n",
      "    return func.predict(\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\", line 700, in predict\n",
      "    return predict_loop(\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\", line 178, in model_iteration\n",
      "    f = _make_execution_function(model, mode)\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py\", line 548, in _make_execution_function\n",
      "    return model._make_execution_function(mode)\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py\", line 2091, in _make_execution_function\n",
      "    self._make_predict_function()\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py\", line 2076, in _make_predict_function\n",
      "    self.predict_function = backend.function(\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\backend.py\", line 4336, in function\n",
      "    return GraphExecutionFunction(\n",
      "  File \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\backend.py\", line 4129, in __init__\n",
      "    with tf.control_dependencies([self.outputs[0]]):\n",
      "  File \"C:\\Users\\filip\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 5616, in control_dependencies\n",
      "    return get_default_graph().control_dependencies(control_inputs)\n",
      "  File \"C:\\Users\\filip\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 5070, in control_dependencies\n",
      "    c = self.as_graph_element(c)\n",
      "  File \"C:\\Users\\filip\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3974, in as_graph_element\n",
      "    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n",
      "  File \"C:\\Users\\filip\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 4053, in _as_graph_element_locked\n",
      "    raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\n",
      "ValueError: Tensor Tensor(\"dense_13/Softmax:0\", shape=(None, 2), dtype=float32) is not an element of this graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| CartPole-v1\n",
      "| Action space: Discrete with high state-space\n",
      "| Environment beated threshold: 200\n",
      "| Dev notes:\n",
      "|   * Agents that track State/Action combinations like \n",
      "|     Q learning will fail due to high state space\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "\n",
      "| ---------------------------------\n",
      "| CartPole-v1\n",
      "| Action space: Discrete with high state-space\n",
      "| Environment beated threshold: 200\n",
      "| Dev notes:\n",
      "|   * Agents that track State/Action combinations like \n",
      "|     Q learning will fail due to high state space\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "<keras.engine.functional.Functional object at 0x000002169B45FF10>\n"
     ]
    }
   ],
   "source": [
    "from src.environments.discrete.cartpole import environment\n",
    "agent = A3CAgent(environment, n_workers=1)\n",
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d09947d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T10:19:03.125156Z",
     "start_time": "2022-07-29T10:19:02.745559Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A2CAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minverted_pendulum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environment\n\u001b[1;32m----> 2\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mA2CAgent\u001b[49m(environment)\n\u001b[0;32m      3\u001b[0m agent\u001b[38;5;241m.\u001b[39mlearn()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A2CAgent' is not defined"
     ]
    }
   ],
   "source": [
    "#from src.environments.continuous.inverted_pendulum import environment\n",
    "#agent = A2CAgent(environment)\n",
    "#agent.learn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47974b27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
