{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6487dee",
   "metadata": {},
   "source": [
    "### Monte Carlo\n",
    "First vs. Every Visit MC\n",
    "* Tracking rewards received after visiting states\n",
    "* Rewards received after first visit → First visit MC\n",
    "* Rewards received after every visit → Every visit MC\n",
    "\n",
    "### Blackjack Overview\n",
    "* Player vs. Dealer; first to 21 wins, > 21 is a loss (bust)\n",
    "* Ace worth 1 or 11, other face cards worth 10\n",
    "* Ace that doesn’t cause a bust is called usable\n",
    "* One dealer card is showing\n",
    "* Inifinte deck with replacement → no counting\n",
    "* State space is a 3 tuple:\n",
    "    * Player sum (4-21), \n",
    "    * dealer showing card (ace – 10), \n",
    "    * boolean for a usable ace\n",
    "* Reward +1 for winning, 0 for draw, -1 for loss\n",
    "* Policy: draw new card (hit) if player total < 20, else stick\n",
    "\n",
    "### Algorithm Overview\n",
    "* Initialize the policy to be evaluated\n",
    "* Initialize the value function arbitrarily\n",
    "* Initialize list of returns for all states in the state space\n",
    "* Repeat for large number of episodes:\n",
    "    * Generate episode using policy\n",
    "    * For each state s in the agent’s memory:\n",
    "        * Calculate the return that followed first visit to s\n",
    "        * Append return G to list of returns\n",
    "        * Calculate the average of the returns for state s\n",
    "* 500,000 games → print value of state (21, 2, True)\n",
    "* Split into agent class and main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22353e6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T08:38:15.786099Z",
     "start_time": "2022-07-28T08:38:14.417742Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()\n",
    "\n",
    "import gym\n",
    "import enum\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rich.table import Table\n",
    "from rich.console import Console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc086cb6",
   "metadata": {},
   "source": [
    "### BlackJack setup\n",
    "* Game and card setup\n",
    "* Dealer turn logic\n",
    "* Dealer hand value evaluation logic\n",
    "* Player hand value evaluation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e07e330",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T08:38:15.801099Z",
     "start_time": "2022-07-28T08:38:15.788100Z"
    }
   },
   "outputs": [],
   "source": [
    "RANKS = {\n",
    "    \"two\" : 2,\n",
    "    \"three\" : 3,\n",
    "    \"four\" : 4,\n",
    "    \"five\" : 5,\n",
    "    \"six\" : 6,\n",
    "    \"seven\" : 7,\n",
    "    \"eight\" : 8,\n",
    "    \"nine\" : 9,\n",
    "    \"ten\" : 10,\n",
    "    \"jack\" : 10,\n",
    "    \"queen\" : 10,\n",
    "    \"king\" : 10,\n",
    "    \"ace\" : (1, 11)\n",
    "}\n",
    "    \n",
    "class Suit(enum.Enum):\n",
    "    spades = \"spades\"\n",
    "    clubs = \"clubs\"\n",
    "    diamonds = \"diamonds\"\n",
    "    hearts = \"hearts\"\n",
    "    \n",
    "class Card:\n",
    "    def __init__(self, suit, rank, value):\n",
    "        self.suit = suit\n",
    "        self.rank = rank\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.rank + \" of \" + self.suit.value\n",
    "\n",
    "class Deck:\n",
    "    def __init__(self, num=1):\n",
    "        self.cards = []\n",
    "        for i in range(num):\n",
    "            for suit in Suit:\n",
    "                for rank, value in RANKS.items():\n",
    "                    self.cards.append(Card(suit, rank, value))\n",
    "                \n",
    "    def shuffle(self):\n",
    "        random.shuffle(self.cards)\n",
    "        \n",
    "    def deal(self):\n",
    "        return self.cards.pop(0)\n",
    "    \n",
    "    def peek(self):\n",
    "        if len(self.cards) > 0:\n",
    "            return self.cards[0]\n",
    "        \n",
    "    def add_to_bottom(self, card):\n",
    "        self.cards.append(card)\n",
    "        \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        for card in self.cards:\n",
    "            result += str(card) + \"\\n\"\n",
    "        return result\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0bced4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T08:38:15.832098Z",
     "start_time": "2022-07-28T08:38:15.803100Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "INITIAL_BALANCE = 1000\n",
    "NUM_DECKS = 2\n",
    "\n",
    "class BlackjackEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, num_decks = NUM_DECKS, initial_balance=INITIAL_BALANCE):\n",
    "        super(BlackjackEnv, self).__init__()\n",
    "        \n",
    "        # Initialize the blackjack deck.\n",
    "        self.deck = Deck(num_decks)\n",
    "        self.initial_balance = initial_balance\n",
    "        \n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        \n",
    "        self.reward_options = {\"lose\":-100, \"tie\":0, \"win\":100}\n",
    "        \n",
    "        # hit = 0, stand = 1\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        '''\n",
    "        First element of tuple is the range of possible hand values for the player. (3 through 20)\n",
    "        This is the possible range of values that the player will actually have to make a decision for.\n",
    "        Any player hand value 21 or above already has automatic valuations, and needs no input from an\n",
    "        AI Agent. \n",
    "        \n",
    "        However, we also need to add all the hand values that the agent could possibly end up in when\n",
    "        they bust. Maybe the agent can glean some correlations based on what hand value they bust at,\n",
    "        so this should be in the observation space. Also, the layout of OpenAI Gym environment class\n",
    "        makes us have to include the bust-value in the step() function because we need to return that\n",
    "        done is true alongside the final obs, which is the bust-value.\n",
    "        '''\n",
    "        \n",
    "        # Second element of the tuple is the range of possible values for the dealer's upcard. (2 through 11)\n",
    "        self.observation_space = spaces.Tuple((spaces.Discrete(18), spaces.Discrete(10)))\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "    \n",
    "    ### HAND EVALUATION LOGIC  \n",
    "    def dealer_hand_value_evaluation(self,hand):\n",
    "        num_ace = 0\n",
    "        use_one = 0\n",
    "        for card in hand:\n",
    "            if card.rank ==\"ace\":\n",
    "                num_ace +=1\n",
    "                use_one += card.value[0] # Assume 1 for Ace\n",
    "            else:\n",
    "                use_one += card.value\n",
    "                \n",
    "        if num_ace > 0:\n",
    "            # If has 11 instead of 1 for aces, dealer's hand is closer to the [17,21] range\n",
    "            # Will follow Hard 17 rules ( dealer wont hi again is Ace yields a 17 )\n",
    "            # Aces can be changed to 1's\n",
    "            ace_counter = 0\n",
    "            while ace_counter < num_ace:\n",
    "                use_eleven = use_one + 10\n",
    "                if use_eleven > 21:\n",
    "                    return use_one\n",
    "                elif use_eleven >= 17 and use_eleven <= 21:\n",
    "                    return use_eleven\n",
    "                else:\n",
    "                    # The case where even using Ace as eleven is less than 17.\n",
    "                    use_one = use_eleven\n",
    "            \n",
    "                ace_counter += 1\n",
    "            return use_one\n",
    "        else:\n",
    "            return use_one\n",
    "            \n",
    "    def player_hand_value_evaluation(self,hand):\n",
    "        num_ace = 0\n",
    "        # use_one means that every ace that in the hand is counted as one.\n",
    "        use_one = 0\n",
    "        for card in hand:\n",
    "            if card.rank == \"ace\":\n",
    "                num_ace += 1\n",
    "                use_one += card.value[0] # use 1 for Ace\n",
    "            else:\n",
    "                use_one += card.value\n",
    "\n",
    "        if num_ace > 0:\n",
    "            # Define player policy for Aces:\n",
    "            # Make Aces 11 if they get you to the range [18,21]\n",
    "            # Otherwise, use one.\n",
    "            ace_counter = 0\n",
    "            while ace_counter < num_ace:\n",
    "                use_eleven = use_one + 10 \n",
    "\n",
    "                if use_eleven > 21:\n",
    "                    return use_one\n",
    "                elif use_eleven >= 18 and use_eleven <= 21:\n",
    "                    return use_eleven\n",
    "                else:\n",
    "                    # This allows for some Aces to be 11s, and others to be 1.\n",
    "                    use_one = use_eleven\n",
    "\n",
    "                ace_counter += 1\n",
    "\n",
    "            return use_one\n",
    "        else:\n",
    "            return use_one\n",
    "        \n",
    "    ### PLAYER BET POLICY\n",
    "    def discrete_policy(self,hand, deck):\n",
    "        value = self.player_hand_value_evaluation(hand)\n",
    "\n",
    "        # Implement Discrete Policy\n",
    "        # If hand >= 18, stand. Otherwise, hit.\n",
    "        while value < 18:\n",
    "            # hit\n",
    "            hand.append(deck.deal())\n",
    "            value = self.player_hand_value_evaluation(hand)\n",
    "\n",
    "        return value, hand, deck\n",
    "\n",
    "    def stochastic_policy(self,hand, deck):\n",
    "        value = self.player_hand_value_evaluation(hand)\n",
    "\n",
    "        # Implement Stochastic Policy\n",
    "        # If hand >= 18: 80% Stand, 20% Hit\n",
    "        # Else: 80% Hit, 20% Stand\n",
    "\n",
    "        stand = False\n",
    "        while value < 18 and stand == False:\n",
    "            percent = random.randint(1, 10)\n",
    "            if percent <= 8:\n",
    "                # hit\n",
    "                hand.append(deck.deal())\n",
    "                value = self.player_hand_value_evaluation(hand)\n",
    "            else:\n",
    "                # stand\n",
    "                stand = True\n",
    "    \n",
    "        if stand:\n",
    "            return value, hand, deck\n",
    "    \n",
    "        # player_value is now >= 18.\n",
    "        if value < 21:\n",
    "            percent = random.randint(1, 10)\n",
    "            if percent > 8:\n",
    "                # hit\n",
    "                hand.append(deck.deal())\n",
    "                value = self.player_hand_value_evaluation(hand)        \n",
    "\n",
    "            # else: Do nothing, stand.\n",
    "            # else: player_value is 21 or higher, so we must stand in both cases.\n",
    "\n",
    "        return value, hand, deck\n",
    "    \n",
    "    \n",
    "    ### TURN EVALUATION LOGIC\n",
    "    def dealer_turn(self,dealer_hand,deck):\n",
    "        # Calculate dealer hand's value.\n",
    "        dealer_value = self.dealer_hand_value_evaluation(dealer_hand)\n",
    "        \n",
    "        # Define dealer policy (is fixed to official rules)\n",
    "\n",
    "        # The dealer keeps hitting until their total is 17 or more\n",
    "        while dealer_value < 17:\n",
    "            # hit\n",
    "            dealer_hand.append(deck.deal())\n",
    "            dealer_value = self.dealer_hand_value_evaluation(dealer_hand)\n",
    "\n",
    "        return dealer_value, dealer_hand, deck\n",
    "        \n",
    "    def _take_action(self, action):\n",
    "        if action == 0: # hit\n",
    "            self.player_hand.append(self.deck.deal())\n",
    "            \n",
    "        # re-calculate the value of the player's hand after any changes to the hand.\n",
    "        self.player_value = self.player_hand_value_evaluation(self.player_hand)\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        \n",
    "        # End the episode/game is the player stands or has a hand value >= 21.\n",
    "        self.done = action == 1 or self.player_value >= 21\n",
    "        \n",
    "        # rewards are 0 when the player hits and is still below 21, and they\n",
    "        # keep playing.\n",
    "        rewards = 0\n",
    "        \n",
    "        if self.done:\n",
    "            # CALCULATE REWARDS\n",
    "            if self.player_value > 21: # above 21, player loses automatically.\n",
    "                rewards = self.reward_options[\"lose\"]\n",
    "            elif self.player_value == 21: # blackjack! Player wins automatically.\n",
    "                rewards = self.reward_options[\"win\"]\n",
    "            else:\n",
    "                ## Begin dealer turn phase.\n",
    "\n",
    "                dealer_value, self.dealer_hand, self.deck = self.dealer_turn(self.dealer_hand, self.deck)\n",
    "\n",
    "                ## End of dealer turn phase\n",
    "\n",
    "                #------------------------------------------------------------#\n",
    "\n",
    "                ## Final Compare\n",
    "\n",
    "                if dealer_value > 21: # dealer above 21, player wins automatically\n",
    "                    rewards = self.reward_options[\"win\"]\n",
    "                elif dealer_value == 21: # dealer has blackjack, player loses automatically\n",
    "                    rewards = self.reward_options[\"lose\"]\n",
    "                else: # dealer and player have values less than 21.\n",
    "                    if self.player_value > dealer_value: # player closer to 21, player wins.\n",
    "                        rewards = self.reward_options[\"win\"]\n",
    "                    elif self.player_value < dealer_value: # dealer closer to 21, dealer wins.\n",
    "                        rewards = self.reward_options[\"lose\"]\n",
    "                    else:\n",
    "                        rewards = self.reward_options[\"tie\"]\n",
    "        \n",
    "        self.balance += rewards\n",
    "        \n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 3 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "        \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = self.dealer_hand_value_evaluation([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs, rewards, self.done, {}\n",
    "    \n",
    "    def reset(self): # resets game to an initial state\n",
    "        # Add the player and dealer cards back into the deck.\n",
    "        self.deck.cards += self.player_hand + self.dealer_hand\n",
    "\n",
    "        # Shuffle before beginning. Only shuffle once before the start of each game.\n",
    "        self.deck.shuffle()\n",
    "         \n",
    "        self.balance = self.initial_balance\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        # returns the start state for the agent\n",
    "        # deal 2 cards to the agent and the dealer\n",
    "        self.player_hand = [self.deck.deal(), self.deck.deal()]\n",
    "        self.dealer_hand = [self.deck.deal(), self.deck.deal()]\n",
    "        self.dealer_upcard = self.dealer_hand[0]\n",
    "        \n",
    "        # calculate the value of the agent's hand\n",
    "        self.player_value = self.player_hand_value_evaluation(self.player_hand)\n",
    "        \n",
    "        # Subtract by 1 to fit into the possible observation range.\n",
    "        # This makes the possible range of 2 through 20 into 1 through 18\n",
    "        player_value_obs = self.player_value - 2\n",
    "            \n",
    "        # get the value of the dealer's upcard, this value is what the agent sees.\n",
    "        # Subtract by 1 to fit the possible observation range of 1 to 10.\n",
    "        upcard_value_obs = self.dealer_hand_value_evaluation([self.dealer_upcard]) - 1\n",
    "        \n",
    "        # the state is represented as a player hand-value + dealer upcard pair.\n",
    "        obs = np.array([player_value_obs, upcard_value_obs])\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self, mode='human', close=False):\n",
    "        # convert the player hand into a format that is\n",
    "        # easy to read and understand.\n",
    "        hand_list = []\n",
    "        for card in self.player_hand:\n",
    "            hand_list.append(card.rank)\n",
    "            \n",
    "        # re-calculate the value of the dealer upcard.\n",
    "        upcard_value = self.dealer_hand_value_evaluation([self.dealer_upcard])\n",
    "        \n",
    "        print(f'Balance: {self.balance}')\n",
    "        print(f'Player Hand: {hand_list}')\n",
    "        print(f'Player Value: {self.player_value}')\n",
    "        print(f'Dealer Upcard: {upcard_value}')\n",
    "        print(f'Done: {self.done}')\n",
    "        \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb8e1a18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T08:47:39.674599Z",
     "start_time": "2022-07-28T08:47:39.633599Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlackJackAgent:\n",
    "    def __init__(self, environment, alpha = 0.001,epsilon=1,decay = 0.9999,epsilon_min = 0.9,gamma = 0.8):\n",
    "        self.env = environment \n",
    "        # The learning rate. Very small to avoid making quick, large changes in our policy.\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # The rate by which epsilon will decay over time.\n",
    "        # Since the probability we take the option with the highest Q-value is 1-epsilon + probability,\n",
    "        # this decay will make sure we are the taking the better option more often in the longrun.\n",
    "        # This allows the algorithm to explore in the early stages, and exploit in the later stages.\n",
    "        self.decay = decay\n",
    "\n",
    "        # The lowest value that epsilon can go to.\n",
    "        # Although the decay seems slow, it actually grows exponentially, and this is magnified when\n",
    "        # running thousands of episodes.\n",
    "        self.epsilon_min = epsilon_min\n",
    "\n",
    "        # may have to be tweaked later.\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Create tables\n",
    "        self.__init_buffers()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.Q = np.zeros([self.env.observation_space[0].n * self.env.observation_space[1].n, self.env.action_space.n], dtype=np.float16)\n",
    "\n",
    "        # This map contains the probability distributions for each action (hit or stand) given a state.\n",
    "        # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
    "        # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
    "        self.prob = np.zeros([self.env.observation_space[0].n * self.env.observation_space[1].n, self.env.action_space.n], dtype=np.float16) + 0.5\n",
    "    \n",
    "    \n",
    "    ### GAME LOGIC\n",
    "    def run_mc(self,num_episodes):\n",
    "        '''\n",
    "        observation_space[0] is the 18 possible player values. (3 through 20)\n",
    "        observation_space[1] is the 10 possible dealer upcards. (2 through 11)\n",
    "\n",
    "        Combining these together yields all possible states.\n",
    "\n",
    "        Multiplying this with hit/stand yields all possible state/action pairs.\n",
    "\n",
    "        This is the Q map.\n",
    "        '''\n",
    "        self.Q = np.zeros([self.env.observation_space[0].n * self.env.observation_space[1].n, self.env.action_space.n], dtype=np.float16)\n",
    "\n",
    "\n",
    "        # This map contains the probability distributions for each action (hit or stand) given a state.\n",
    "        # The state (combo of player hand value and dealer upcard value) index in this array yields a 2-element array\n",
    "        # The 0th index of this 2-element array refers to the probability of \"hit\", and the 1st index is the probability of \"stand\"\n",
    "        self.prob = np.zeros([self.env.observation_space[0].n * self.env.observation_space[1].n, self.env.action_space.n], dtype=np.float16) + 0.5\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.play_game()\n",
    "\n",
    "            self.epsilon = max(self.epsilon * self.decay, self.epsilon_min)\n",
    "\n",
    "            Q = self.update_Q(episode)\n",
    "            prob = self.update_prob(episode)\n",
    "\n",
    "        return Q, prob\n",
    "    \n",
    "    ### Q LEARNING\n",
    "    def get_Q_state_index(self,state):\n",
    "        # the player value is already subtracted by 1 in the env when it returns the state.\n",
    "        # subtract by 1 again to fit with the array indexing that starts at 0\n",
    "        initial_player_value = state[0] - 1\n",
    "        # the upcard value is already subtracted by 1 in the env when it returns the state.\n",
    "        # dealer_upcard will be subtracted by 1 to fit with the array indexing that starts at 0\n",
    "        dealer_upcard = state[1] - 1\n",
    "\n",
    "        return (self.env.observation_space[1].n * (initial_player_value)) + (dealer_upcard)\n",
    "        \n",
    "    def update_Q(self,episode):\n",
    "        '''\n",
    "        THIS IS WHERE THE ALGORITHM HINGES ON BEING FIRST VISIT OR EVERY VISIT.\n",
    "        I AM GOING TO USE FIRST-VISIT, AND HERE'S WHY.\n",
    "\n",
    "        If you want first-visit, you need to use the cumulative reward of the entire\n",
    "        episode when updating a Q-value for ALL of the state/action pairs in the\n",
    "        episode, even the first state/action pair. In this algorithm, an episode\n",
    "        is a round of Blackjack. Although the bulk of the reward may come from the\n",
    "        2nd or 3rd decision, deciding to hit on the 1st decision is what enabled\n",
    "        the future situations to even occur, so it is important to include the\n",
    "        entire cumulative reward. We can reduce the impact of the rewards of the\n",
    "        future decisions by lowering gamma, which will lower the G value for our\n",
    "        early state/action pair in which we hit and did not get any immediate rewards.\n",
    "        This will make our agent consider future rewards, and not just look at \n",
    "        each state in isolation despite having hit previously.\n",
    "\n",
    "        If you want Every-Visit MC, do not use the cumulative rewards when updating Q-values,\n",
    "        and just use the immediate reward in this episode for each state/action pair.\n",
    "        '''\n",
    "        step = 0\n",
    "        for state, action, reward in episode:\n",
    "            # calculate the cumulative reward of taking this action in this state.\n",
    "            # Start from the immediate rewards, and use all the rewards from the\n",
    "            # subsequent states. Do not use rewards from previous states.\n",
    "            total_reward = 0\n",
    "            gamma_exp = 0\n",
    "            for curr_step in range(step, len(episode)):\n",
    "                curr_reward = episode[curr_step][2]\n",
    "                total_reward += (self.gamma ** gamma_exp) * curr_reward\n",
    "                gamma_exp += 1\n",
    "\n",
    "            # Update the Q-value\n",
    "            Q_state_index = self.get_Q_state_index(state)\n",
    "            curr_Q_value = self.Q[Q_state_index][action]\n",
    "            self.Q[Q_state_index][action] = curr_Q_value + self.alpha * (total_reward - curr_Q_value)\n",
    "\n",
    "            # update step to start further down the episode next time.\n",
    "            step += 1\n",
    "\n",
    "\n",
    "        return self.Q\n",
    "\n",
    "    def get_prob_of_best_action(self,state):\n",
    "        # Use the mapping function to figure out which index of Q corresponds to \n",
    "        # the player hand value + dealer upcard value that defines each state.\n",
    "        Q_state_index = self.get_Q_state_index(state)\n",
    "\n",
    "        # Use this index in the Q 2-D array to get a 2-element array that yield\n",
    "        # the current Q-values for hitting (index 0) and standing (index 1) in this state.\n",
    "        # Use the np.argmax() function to find the index of the action that yields the\n",
    "        # rewards i.e. the best action we are looking for.\n",
    "        best_action = np.argmax(self.Q[Q_state_index])\n",
    "\n",
    "        # Retrieve the probability of the best action using the \n",
    "        # state/action pair as indices for the `prob` array,\n",
    "        # which stores the probability of taking an action (hit or stand)\n",
    "        # for a given state/action pair.\n",
    "        return self.prob[Q_state_index][best_action]\n",
    "\n",
    "    def update_prob_of_best_action(self,state):\n",
    "\n",
    "        Q_state_index = self.get_Q_state_index(state)\n",
    "\n",
    "        best_action = np.argmax(self.Q[Q_state_index])\n",
    "\n",
    "        # Slightly alter the probability of this best action being taken by using epsilon\n",
    "        # Epsilon starts at 1.0, and slowly decays over time.\n",
    "        # Therefore, as per the equation below, the AI agent will use the probability listed \n",
    "        # for the best action in the `prob` array during the beginning of the algorithm.\n",
    "        # As time goes on, the likelihood that the best action is taken is increased from\n",
    "        # what is listed in the `prob` array.\n",
    "        # This allows for exploration of other moves in the beginning of the algorithm,\n",
    "        # but exploitation later for a greater reward.\n",
    "        #prob[Q_state_index][best_action] = prob[Q_state_index][best_action] + ((1 - epsilon) * (1 - prob[Q_state_index][best_action]))\n",
    "        self.prob[Q_state_index][best_action] = min(1, self.prob[Q_state_index][best_action] + 1 - self.epsilon)\n",
    "\n",
    "        other_action = 1 if best_action == 0 else 0\n",
    "        self.prob[Q_state_index][other_action] = 1 - self.prob[Q_state_index][best_action]\n",
    "\n",
    "        return self.prob\n",
    "    \n",
    "    def update_prob(self,episode):\n",
    "        for state, action, reward in episode:\n",
    "            # Update the probabilities of the actions that can be taken given the current\n",
    "            # state. The goal is that the new update in Q has changed what the best action\n",
    "            # is, and epsilon will be used to create a small increase in the probability\n",
    "            # that the new, better action is chosen.\n",
    "            prob = self.update_prob_of_best_action(state)\n",
    "\n",
    "        return self.prob\n",
    "\n",
    "    def play_game(self):\n",
    "        # Can contain numerous state->action->reward tuples because a round of \n",
    "        # Blackjack is not always resolved in one turn.\n",
    "        # However, there will be no state that has a player hand value that exceeds 20, since only initial\n",
    "        # states BEFORE actions are made are used when storing state->action->reward tuples.\n",
    "        episode = []\n",
    "\n",
    "        state = self.env.reset()\n",
    "\n",
    "        while self.env.done == False:\n",
    "            if state[0] == 19: #Player was dealt Blackjack, player_value already subtracted by 2 to get state[0]\n",
    "                # don't do any episode analysis for this episode. This is a useless episode.\n",
    "                next_state, reward, self.env.done, info = self.env.step(1) # doesn't matter what action is taken.\n",
    "            else:\n",
    "                # Get the index in Q that corresponds to the current state\n",
    "                Q_state_index = self.get_Q_state_index(state)\n",
    "\n",
    "                # Use the index to get the possible actions, and use np.argmax()\n",
    "                # to get the index of the action that has the highest current Q\n",
    "                # value. Index 0 is hit, index 1 is stand.\n",
    "                best_action = np.argmax(self.Q[Q_state_index])\n",
    "\n",
    "                # Go to the prob table to retrieve the probability of this action.\n",
    "                # This uses the same Q_state_index used for finding the state index\n",
    "                # of the Q-array.\n",
    "                prob_of_best_action = self.get_prob_of_best_action(state)\n",
    "\n",
    "                action_to_take = None\n",
    "\n",
    "                if random.uniform(0,1) < prob_of_best_action: # Take the best action\n",
    "                    action_to_take = best_action\n",
    "                else: # Take the other action\n",
    "                    action_to_take = 1 if best_action == 0 else 0\n",
    "\n",
    "                # The agent does the action, and we get the next state, the rewards,\n",
    "                # and whether the game is now done.\n",
    "                next_state, reward, self.env.done, info = self.env.step(action_to_take)\n",
    "\n",
    "                # We now have a state->action->reward sequence we can log\n",
    "                # in `episode`\n",
    "                episode.append((state, action_to_take, reward))\n",
    "\n",
    "                # update the state for the next decision made by the agent.\n",
    "                state = next_state\n",
    "\n",
    "        return episode\n",
    "    \n",
    "    def best_policy(self,Q):\n",
    "        best_policy_binary = []\n",
    "        best_policy_string = []\n",
    "        best_policy_colors = []\n",
    "        for i in range(len(Q)):\n",
    "            best_policy_binary.append(np.argmax(Q[i]))\n",
    "            best_policy_string.append(\"Hit\" if np.argmax(Q[i]) == 0 else \"Stand\")\n",
    "            best_policy_colors.append(\"g\" if np.argmax(Q[i]) == 0 else \"r\")\n",
    "\n",
    "        return best_policy_binary, best_policy_string, best_policy_colors\n",
    "    \n",
    "    def learn(self,episodes=1000000, plot=True):\n",
    "        new_Q, new_prob = self.run_mc(episodes)\n",
    "        self.new_Q = new_Q\n",
    "        self.new_prob = new_prob\n",
    "        self.new_Q_binary, self.new_Q_string, self.new_Q_colors =self.best_policy(self.new_Q)\n",
    "                    \n",
    "        if plot:\n",
    "            df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "            color_df = pd.DataFrame(columns = range(2, 12))\n",
    "\n",
    "            for s in range(3, 21): # possible player values in the range 3 to 20\n",
    "                start = self.env.observation_space[1].n * (s-3)\n",
    "                end = start + 10\n",
    "                df.loc[s]=(self.new_Q_string[start:end])\n",
    "                color_df.loc[s]=(self.new_Q_colors[start:end])\n",
    "\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "            # hide axes\n",
    "            fig.patch.set_visible(False)\n",
    "            ax.set_axis_off()\n",
    "            ax.axis('tight')\n",
    "\n",
    "            ax.table(cellText=df.values, cellColours=color_df.values, cellLoc=\"center\", rowLabels=df.index, colLabels=df.columns, loc='center')\n",
    "            ax.set_title('X = Dealer hand value, Y = player hand value', pad=20)\n",
    "            fig.tight_layout()\n",
    "\n",
    "            plt.show()\n",
    "                    \n",
    "    def test(self,episodes = 100000):\n",
    "        total_rewards = 0\n",
    "\n",
    "        for _ in range(episodes):\n",
    "            state = self.env.reset()\n",
    "\n",
    "            while self.env.done == False:\n",
    "                if state[0] == 19: # Player was dealt Blackjack\n",
    "                    next_state, reward, self.env.done, info = self.env.step(1) # doesn't matter what action is taken.\n",
    "                    # don't do any episode analysis for this episode. This is a useless episode.\n",
    "                    total_rewards += reward\n",
    "                else:\n",
    "                    Q_index = self.get_Q_state_index(state)\n",
    "                    action = self.new_Q_binary[Q_index]\n",
    "\n",
    "                    new_state, reward, done, desc = self.env.step(action)\n",
    "                    state = new_state\n",
    "                    total_rewards += reward\n",
    "\n",
    "        avg_reward = total_rewards / episodes\n",
    "        print(avg_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96b312aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T08:57:41.131446Z",
     "start_time": "2022-07-28T08:47:53.312396Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = BlackjackEnv()\n",
    "agent= BlackJackAgent(environment)\n",
    "agent.learn(episodes = 5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1bdb64c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T08:57:47.151724Z",
     "start_time": "2022-07-28T08:57:41.133449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.361\n"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500813db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
