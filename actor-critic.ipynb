{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87a64db4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:03:03.487937Z",
     "start_time": "2022-07-21T21:03:03.449971Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc5c983",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:03:06.965996Z",
     "start_time": "2022-07-21T21:03:03.850368Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from src.utils.gym_environment import GymEnvironment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cabc1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:18:12.848870Z",
     "start_time": "2022-07-21T18:18:12.817911Z"
    }
   },
   "outputs": [],
   "source": [
    "#sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "#sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "#norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment, \n",
    "                alpha = 0.1,\n",
    "                gamma = 0.99,\n",
    "                eps = np.finfo(np.float32).eps.item(),\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                ):\n",
    "        \n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Environment\n",
    "        env = GymEnvironment(environment)\n",
    "        self.env = env.env\n",
    "        self.n_actions = env.n_actions\n",
    "        self.actions = env.actions\n",
    "        self.observation_shape = env.observation_shape\n",
    "        \n",
    "        self.__init_networks()\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        num_inputs = self.observation_shape[0]\n",
    "        num_actions = self.n_actions\n",
    "        num_hidden = 128\n",
    "\n",
    "        inputs = layers.Input(shape=(num_inputs,))\n",
    "        common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "        action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "        critic = layers.Dense(1)(common)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "    \n",
    "    def choose_action(self,action_probs):\n",
    "        \n",
    "        # Sample action from action probability distribution\n",
    "        action = np.random.choice(self.n_actions, p=np.squeeze(action_probs))\n",
    "        action_log_prob = tf.math.log(action_probs[0, action])\n",
    "\n",
    "        return action, action_log_prob\n",
    "    \n",
    "    def learn(self):\n",
    "        huber_loss = keras.losses.Huber()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        running_reward = 0\n",
    "        episode_count = 0\n",
    "        \n",
    "        while True:  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                for timestep in range(1, 1000):\n",
    "                    # env.render(); Adding this line would show the attempts\n",
    "                    # of the agent in a pop up window.\n",
    "\n",
    "                    state = tf.convert_to_tensor(state)\n",
    "                    state = tf.expand_dims(state, 0)\n",
    "\n",
    "                    # Predict action probabilities and estimated future rewards\n",
    "                    # from environment state\n",
    "                    action_probs, critic_value = self.model(state)\n",
    "                    critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "                    # Sample action from action probability distribution\n",
    "                    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "                    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "                    action, action_log_prob = self.choose_action(action_probs)\n",
    "\n",
    "                    action_probs_history.append(action_log_prob)\n",
    "                    \n",
    "                    #print({\"action\":action, \"action_log_prob\":action_log_prob})\n",
    "                    \n",
    "                    # Apply the sampled action in our environment\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    rewards_history.append(reward)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Update running reward to check condition for solving\n",
    "                running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "                # Calculate expected value from rewards\n",
    "                # - At each timestep what was the total reward received after that timestep\n",
    "                # - Rewards in the past are discounted by multiplying them with gamma\n",
    "                # - These are the labels for our critic\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                for r in rewards_history[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # Normalize\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                # Calculating loss values to update our network\n",
    "                history = zip(action_probs_history, critic_value_history, returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                for log_prob, value, ret in history:\n",
    "                    # At this point in history, the critic estimated that we would get a\n",
    "                    # total reward = `value` in the future. We took an action with log probability\n",
    "                    # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                    # The actor must be updated so that it predicts an action that leads to\n",
    "                    # high rewards (compared to critic's estimate) with high probability.\n",
    "                    diff = ret - value\n",
    "                    actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                    # The critic must be updated so that it predicts a better estimate of\n",
    "                    # the future rewards.\n",
    "                    critic_losses.append(\n",
    "                        huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "                # Backpropagation\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # Clear the loss and reward history\n",
    "                action_probs_history.clear()\n",
    "                critic_value_history.clear()\n",
    "                rewards_history.clear()\n",
    "\n",
    "            # Log details\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                template = \"running reward: {:.2f} at episode {}\"\n",
    "                print(template.format(running_reward, episode_count))\n",
    "\n",
    "            if running_reward > 195:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83842830",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:11:52.114012Z",
     "start_time": "2022-07-21T18:07:45.760504Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.discrete.cartpole import environment\n",
    "agent = Agent(environment)\n",
    "agent.learn()\n",
    "#{'action': 0, 'action_log_prob': <tf.Tensor: shape=(), dtype=float32, numpy=-0.6900733>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259c5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcf6775",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:03:08.586199Z",
     "start_time": "2022-07-21T21:03:08.555576Z"
    }
   },
   "outputs": [],
   "source": [
    "#sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "#sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "#norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment, \n",
    "                alpha = 0.1,\n",
    "                gamma = 0.99,\n",
    "                eps = np.finfo(np.float32).eps.item(),\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                ):\n",
    "        \n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Environment\n",
    "        env = GymEnvironment(environment)\n",
    "        self.env = env.env\n",
    "        self.n_actions = env.n_actions\n",
    "        self.actions = env.actions\n",
    "        self.observation_shape = env.observation_shape\n",
    "        \n",
    "        self.__init_networks()\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        num_inputs = self.observation_shape[0]\n",
    "        num_actions = self.n_actions\n",
    "        num_hidden = 128\n",
    "\n",
    "        inputs = layers.Input(shape=(num_inputs,))\n",
    "        common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "        sigma = layers.Dense(1, activation=\"softplus\", name=\"sigma\")(common)\n",
    "        mu = layers.Dense(1, activation=\"tanh\" , name='mu')(common)\n",
    "        #sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        #norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "        #action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "        critic = layers.Dense(1, activation=None ,name='critic')(common)\n",
    "        \n",
    "        actor = tf.keras.layers.Concatenate(axis=-1)([mu,sigma])\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[actor, critic])\n",
    "\n",
    "    \n",
    "    def choose_action(self,mu,sigma):\n",
    "        \n",
    "        # Sample action from action probability distribution\n",
    "        #action = np.random.choice(self.n_actions, p=np.squeeze(action_probs))\n",
    "        #action_log_prob = tf.math.log(action_probs[0, action])\n",
    "        #mu = tf.math.tanh(mu)\n",
    "        norm_dist = tfp.distributions.Normal(mu, sigma)\n",
    "        action = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_log_prob = -(norm_dist.log_prob(action)+self.eps)\n",
    "        action = tf.clip_by_value(\n",
    "            action, self.env.action_space.low[0], \n",
    "            self.env.action_space.high[0])\n",
    "        \n",
    "        return np.array(action[0],dtype=np.float32), action_log_prob\n",
    "    \n",
    "    def learn(self):\n",
    "        huber_loss = keras.losses.Huber()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        running_reward = 0\n",
    "        episode_count = 0\n",
    "        \n",
    "        while True:  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                for timestep in range(1, 1000):\n",
    "                    # env.render(); Adding this line would show the attempts\n",
    "                    # of the agent in a pop up window.\n",
    "\n",
    "                    state = tf.convert_to_tensor(state)\n",
    "                    state = tf.expand_dims(state, 0)\n",
    "\n",
    "                    # Predict action probabilities and estimated future rewards\n",
    "                    # from environment state\n",
    "                    actor_value, critic_value = self.model(state)\n",
    "                    critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "                    #print('teste',tfp.distributions.Normal(actor_value))\n",
    "                    mu = actor_value[:,0:1]\n",
    "                    sigma = actor_value[:,1:]\n",
    "                    # Sample action from action probability distribution\n",
    "                    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "                    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "                    action, action_log_prob = self.choose_action(mu,sigma)\n",
    "\n",
    "                    action_probs_history.append(action_log_prob)\n",
    "                    #action_probs_history.append(actor_value)\n",
    "                    \n",
    "                    #print({\"action\":action, \"action_log_prob\":action_log_prob})\n",
    "                    \n",
    "                    # Apply the sampled action in our environment\n",
    "                    \n",
    "                    #print(a)\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    rewards_history.append(reward)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Update running reward to check condition for solving\n",
    "                running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "                # Calculate expected value from rewards\n",
    "                # - At each timestep what was the total reward received after that timestep\n",
    "                # - Rewards in the past are discounted by multiplying them with gamma\n",
    "                # - These are the labels for our critic\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                for r in rewards_history[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # Normalize\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                # Calculating loss values to update our network\n",
    "                history = zip(action_probs_history, critic_value_history, returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                for log_prob, value, ret in history:\n",
    "                    # At this point in history, the critic estimated that we would get a\n",
    "                    # total reward = `value` in the future. We took an action with log probability\n",
    "                    # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                    # The actor must be updated so that it predicts an action that leads to\n",
    "                    # high rewards (compared to critic's estimate) with high probability.\n",
    "                    diff = ret - value\n",
    "                    actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                    # The critic must be updated so that it predicts a better estimate of\n",
    "                    # the future rewards.\n",
    "                    critic_losses.append(\n",
    "                        huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "                # Backpropagation\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # Clear the loss and reward history\n",
    "                action_probs_history.clear()\n",
    "                critic_value_history.clear()\n",
    "                rewards_history.clear()\n",
    "\n",
    "            # Log details\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                template = \"running reward: {:.2f} at episode {}\"\n",
    "                print(template.format(running_reward, episode_count))\n",
    "\n",
    "            if running_reward > 0:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4591311d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-21T21:03:09.576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| MountainCarContinuous-v0\n",
      "| Action space:\n",
      "|   * Continuous with low state-space\n",
      "| Dev notes:\n",
      "|   * Switched _max_episode_steps from 200 to 1000 so \n",
      "|     the agent can explore better.\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.mountain_car import environment\n",
    "\n",
    "\n",
    "agent = Agent(environment)\n",
    "agent.learn()\n",
    "\n",
    "#{'action': array([0.6679693], dtype=float32), 'action_log_prob': array([1.0068668], dtype=float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42358f2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:02:51.989368Z",
     "start_time": "2022-07-21T21:02:51.989368Z"
    }
   },
   "outputs": [],
   "source": [
    "state = agent.env.reset()\n",
    "#state = agent.get_state(obs)\n",
    "done = False\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "    agent.env.render()\n",
    "    actor_value, critic_value = agent.model(np.expand_dims(state,axis=0))\n",
    "    mu = actor_value[:,0:1]\n",
    "    sigma = actor_value[:,1:]\n",
    "    # Sample action from action probability distribution\n",
    "    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "    action, action_log_prob = agent.choose_action(mu,sigma)\n",
    "    #action = mu#agent.max_action(state)\n",
    "            \n",
    "    # Step\n",
    "    obs_,reward,done, info = agent.env.step(action)\n",
    "            \n",
    "    # Get next state\n",
    "    score += reward\n",
    "    #state_ = agent.get_state(obs_)\n",
    "    state = obs_\n",
    "    # Set state as next state so the agent keeps \n",
    "    #state = state_\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ca265",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:02:51.990369Z",
     "start_time": "2022-07-21T21:02:51.990369Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "agent.env.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23796f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
