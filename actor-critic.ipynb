{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:03:03.487937Z",
     "start_time": "2022-07-21T21:03:03.449971Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:03:06.965996Z",
     "start_time": "2022-07-21T21:03:03.850368Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from src.utils.gym_environment import GymEnvironment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8743d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:18:12.848870Z",
     "start_time": "2022-07-21T18:18:12.817911Z"
    }
   },
   "outputs": [],
   "source": [
    "#sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "#sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "#norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment, \n",
    "                alpha = 0.1,\n",
    "                gamma = 0.99,\n",
    "                eps = np.finfo(np.float32).eps.item(),\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                ):\n",
    "        \n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Environment\n",
    "        env = GymEnvironment(environment)\n",
    "        self.env = env.env\n",
    "        self.n_actions = env.n_actions\n",
    "        self.actions = env.actions\n",
    "        self.observation_shape = env.observation_shape\n",
    "        \n",
    "        self.__init_networks()\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        num_inputs = self.observation_shape[0]\n",
    "        num_actions = self.n_actions\n",
    "        num_hidden = 128\n",
    "\n",
    "        inputs = layers.Input(shape=(num_inputs,))\n",
    "        common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "        action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "        critic = layers.Dense(1)(common)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "    \n",
    "    def choose_action(self,action_probs):\n",
    "        \n",
    "        # Sample action from action probability distribution\n",
    "        action = np.random.choice(self.n_actions, p=np.squeeze(action_probs))\n",
    "        action_log_prob = tf.math.log(action_probs[0, action])\n",
    "\n",
    "        return action, action_log_prob\n",
    "    \n",
    "    def learn(self):\n",
    "        huber_loss = keras.losses.Huber()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        running_reward = 0\n",
    "        episode_count = 0\n",
    "        \n",
    "        while True:  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                for timestep in range(1, 1000):\n",
    "                    # env.render(); Adding this line would show the attempts\n",
    "                    # of the agent in a pop up window.\n",
    "\n",
    "                    state = tf.convert_to_tensor(state)\n",
    "                    state = tf.expand_dims(state, 0)\n",
    "\n",
    "                    # Predict action probabilities and estimated future rewards\n",
    "                    # from environment state\n",
    "                    action_probs, critic_value = self.model(state)\n",
    "                    critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "                    # Sample action from action probability distribution\n",
    "                    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "                    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "                    action, action_log_prob = self.choose_action(action_probs)\n",
    "\n",
    "                    action_probs_history.append(action_log_prob)\n",
    "                    \n",
    "                    #print({\"action\":action, \"action_log_prob\":action_log_prob})\n",
    "                    \n",
    "                    # Apply the sampled action in our environment\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    rewards_history.append(reward)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Update running reward to check condition for solving\n",
    "                running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "                # Calculate expected value from rewards\n",
    "                # - At each timestep what was the total reward received after that timestep\n",
    "                # - Rewards in the past are discounted by multiplying them with gamma\n",
    "                # - These are the labels for our critic\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                for r in rewards_history[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # Normalize\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                # Calculating loss values to update our network\n",
    "                history = zip(action_probs_history, critic_value_history, returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                for log_prob, value, ret in history:\n",
    "                    # At this point in history, the critic estimated that we would get a\n",
    "                    # total reward = `value` in the future. We took an action with log probability\n",
    "                    # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                    # The actor must be updated so that it predicts an action that leads to\n",
    "                    # high rewards (compared to critic's estimate) with high probability.\n",
    "                    diff = ret - value\n",
    "                    actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                    # The critic must be updated so that it predicts a better estimate of\n",
    "                    # the future rewards.\n",
    "                    critic_losses.append(\n",
    "                        huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "                # Backpropagation\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # Clear the loss and reward history\n",
    "                action_probs_history.clear()\n",
    "                critic_value_history.clear()\n",
    "                rewards_history.clear()\n",
    "\n",
    "            # Log details\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                template = \"running reward: {:.2f} at episode {}\"\n",
    "                print(template.format(running_reward, episode_count))\n",
    "\n",
    "            if running_reward > 195:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62df8d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:11:52.114012Z",
     "start_time": "2022-07-21T18:07:45.760504Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.discrete.cartpole import environment\n",
    "agent = Agent(environment)\n",
    "agent.learn()\n",
    "#{'action': 0, 'action_log_prob': <tf.Tensor: shape=(), dtype=float32, numpy=-0.6900733>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb2662b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06acb01b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:03:08.586199Z",
     "start_time": "2022-07-21T21:03:08.555576Z"
    }
   },
   "outputs": [],
   "source": [
    "#sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "#sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "#norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment, \n",
    "                alpha = 0.1,\n",
    "                gamma = 0.99,\n",
    "                eps = np.finfo(np.float32).eps.item(),\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                ):\n",
    "        \n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Environment\n",
    "        env = GymEnvironment(environment)\n",
    "        self.env = env.env\n",
    "        self.n_actions = env.n_actions\n",
    "        self.actions = env.actions\n",
    "        self.observation_shape = env.observation_shape\n",
    "        \n",
    "        self.__init_networks()\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        num_inputs = self.observation_shape[0]\n",
    "        num_actions = self.n_actions\n",
    "        num_hidden = 128\n",
    "\n",
    "        inputs = layers.Input(shape=(num_inputs,))\n",
    "        common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "        sigma = layers.Dense(1, activation=\"softplus\", name=\"sigma\")(common)\n",
    "        mu = layers.Dense(1, activation=\"tanh\" , name='mu')(common)\n",
    "        #sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        #norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "        #action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "        critic = layers.Dense(1, activation=None ,name='critic')(common)\n",
    "        \n",
    "        actor = tf.keras.layers.Concatenate(axis=-1)([mu,sigma])\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[actor, critic])\n",
    "\n",
    "    \n",
    "    def choose_action(self,mu,sigma):\n",
    "        \n",
    "        # Sample action from action probability distribution\n",
    "        #action = np.random.choice(self.n_actions, p=np.squeeze(action_probs))\n",
    "        #action_log_prob = tf.math.log(action_probs[0, action])\n",
    "        #mu = tf.math.tanh(mu)\n",
    "        norm_dist = tfp.distributions.Normal(mu, sigma)\n",
    "        action = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_log_prob = -(norm_dist.log_prob(action)+self.eps)\n",
    "        action = tf.clip_by_value(\n",
    "            action, self.env.action_space.low[0], \n",
    "            self.env.action_space.high[0])\n",
    "        \n",
    "        return np.array(action[0],dtype=np.float32), action_log_prob\n",
    "    \n",
    "    def learn(self):\n",
    "        huber_loss = keras.losses.Huber()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        running_reward = 0\n",
    "        episode_count = 0\n",
    "        \n",
    "        while True:  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                for timestep in range(1, 1000):\n",
    "                    # env.render(); Adding this line would show the attempts\n",
    "                    # of the agent in a pop up window.\n",
    "\n",
    "                    state = tf.convert_to_tensor(state)\n",
    "                    state = tf.expand_dims(state, 0)\n",
    "\n",
    "                    # Predict action probabilities and estimated future rewards\n",
    "                    # from environment state\n",
    "                    actor_value, critic_value = self.model(state)\n",
    "                    critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "                    #print('teste',tfp.distributions.Normal(actor_value))\n",
    "                    mu = actor_value[:,0:1]\n",
    "                    sigma = actor_value[:,1:]\n",
    "                    # Sample action from action probability distribution\n",
    "                    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "                    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "                    action, action_log_prob = self.choose_action(mu,sigma)\n",
    "\n",
    "                    action_probs_history.append(action_log_prob)\n",
    "                    #action_probs_history.append(actor_value)\n",
    "                    \n",
    "                    #print({\"action\":action, \"action_log_prob\":action_log_prob})\n",
    "                    \n",
    "                    # Apply the sampled action in our environment\n",
    "                    \n",
    "                    #print(a)\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    rewards_history.append(reward)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Update running reward to check condition for solving\n",
    "                running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "                # Calculate expected value from rewards\n",
    "                # - At each timestep what was the total reward received after that timestep\n",
    "                # - Rewards in the past are discounted by multiplying them with gamma\n",
    "                # - These are the labels for our critic\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                for r in rewards_history[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # Normalize\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                # Calculating loss values to update our network\n",
    "                history = zip(action_probs_history, critic_value_history, returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                for log_prob, value, ret in history:\n",
    "                    # At this point in history, the critic estimated that we would get a\n",
    "                    # total reward = `value` in the future. We took an action with log probability\n",
    "                    # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                    # The actor must be updated so that it predicts an action that leads to\n",
    "                    # high rewards (compared to critic's estimate) with high probability.\n",
    "                    diff = ret - value\n",
    "                    actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                    # The critic must be updated so that it predicts a better estimate of\n",
    "                    # the future rewards.\n",
    "                    critic_losses.append(\n",
    "                        huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "                # Backpropagation\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # Clear the loss and reward history\n",
    "                action_probs_history.clear()\n",
    "                critic_value_history.clear()\n",
    "                rewards_history.clear()\n",
    "\n",
    "            # Log details\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                template = \"running reward: {:.2f} at episode {}\"\n",
    "                print(template.format(running_reward, episode_count))\n",
    "\n",
    "            if running_reward > 0:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c3c599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T06:12:54.571703Z",
     "start_time": "2022-07-21T21:03:09.578149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| MountainCarContinuous-v0\n",
      "| Action space:\n",
      "|   * Continuous with low state-space\n",
      "| Dev notes:\n",
      "|   * Switched _max_episode_steps from 200 to 1000 so \n",
      "|     the agent can explore better.\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "running reward: -15.65 at episode 10\n",
      "running reward: -36.80 at episode 20\n",
      "running reward: -57.63 at episode 30\n",
      "running reward: -68.36 at episode 40\n",
      "running reward: -74.80 at episode 50\n",
      "running reward: -58.35 at episode 60\n",
      "running reward: -69.45 at episode 70\n",
      "running reward: -76.94 at episode 80\n",
      "running reward: -65.64 at episode 90\n",
      "running reward: -55.65 at episode 100\n",
      "running reward: -64.43 at episode 110\n",
      "running reward: -67.22 at episode 120\n",
      "running reward: -67.71 at episode 130\n",
      "running reward: -74.32 at episode 140\n",
      "running reward: -84.55 at episode 150\n",
      "running reward: -85.75 at episode 160\n",
      "running reward: -81.82 at episode 170\n",
      "running reward: -84.24 at episode 180\n",
      "running reward: -75.64 at episode 190\n",
      "running reward: -80.02 at episode 200\n",
      "running reward: -83.11 at episode 210\n",
      "running reward: -70.24 at episode 220\n",
      "running reward: -82.13 at episode 230\n",
      "running reward: -85.00 at episode 240\n",
      "running reward: -85.77 at episode 250\n",
      "running reward: -87.93 at episode 260\n",
      "running reward: -78.40 at episode 270\n",
      "running reward: -80.14 at episode 280\n",
      "running reward: -82.97 at episode 290\n",
      "running reward: -89.76 at episode 300\n",
      "running reward: -82.96 at episode 310\n",
      "running reward: -83.56 at episode 320\n",
      "running reward: -81.42 at episode 330\n",
      "running reward: -82.73 at episode 340\n",
      "running reward: -89.61 at episode 350\n",
      "running reward: -77.39 at episode 360\n",
      "running reward: -75.35 at episode 370\n",
      "running reward: -77.57 at episode 380\n",
      "running reward: -82.41 at episode 390\n",
      "running reward: -85.80 at episode 400\n",
      "running reward: -91.46 at episode 410\n",
      "running reward: -87.48 at episode 420\n",
      "running reward: -84.83 at episode 430\n",
      "running reward: -82.62 at episode 440\n",
      "running reward: -63.66 at episode 450\n",
      "running reward: -60.81 at episode 460\n",
      "running reward: -60.20 at episode 470\n",
      "running reward: -55.50 at episode 480\n",
      "running reward: -63.61 at episode 490\n",
      "running reward: -74.25 at episode 500\n",
      "running reward: -73.12 at episode 510\n",
      "running reward: -77.03 at episode 520\n",
      "running reward: -86.21 at episode 530\n",
      "running reward: -80.69 at episode 540\n",
      "running reward: -83.08 at episode 550\n",
      "running reward: -89.83 at episode 560\n",
      "running reward: -93.87 at episode 570\n",
      "running reward: -90.59 at episode 580\n",
      "running reward: -79.15 at episode 590\n",
      "running reward: -87.48 at episode 600\n",
      "running reward: -87.88 at episode 610\n",
      "running reward: -81.13 at episode 620\n",
      "running reward: -75.89 at episode 630\n",
      "running reward: -68.93 at episode 640\n",
      "running reward: -71.50 at episode 650\n",
      "running reward: -77.63 at episode 660\n",
      "running reward: -80.69 at episode 670\n",
      "running reward: -82.28 at episode 680\n",
      "running reward: -72.86 at episode 690\n",
      "running reward: -77.57 at episode 700\n",
      "running reward: -86.53 at episode 710\n",
      "running reward: -84.49 at episode 720\n",
      "running reward: -90.67 at episode 730\n",
      "running reward: -90.46 at episode 740\n",
      "running reward: -94.25 at episode 750\n",
      "running reward: -88.21 at episode 760\n",
      "running reward: -80.75 at episode 770\n",
      "running reward: -74.16 at episode 780\n",
      "running reward: -84.49 at episode 790\n",
      "running reward: -80.08 at episode 800\n",
      "running reward: -67.89 at episode 810\n",
      "running reward: -75.58 at episode 820\n",
      "running reward: -74.03 at episode 830\n",
      "running reward: -84.41 at episode 840\n",
      "running reward: -80.72 at episode 850\n",
      "running reward: -71.87 at episode 860\n",
      "running reward: -67.69 at episode 870\n",
      "running reward: -74.86 at episode 880\n",
      "running reward: -84.90 at episode 890\n",
      "running reward: -90.92 at episode 900\n",
      "running reward: -88.62 at episode 910\n",
      "running reward: -78.14 at episode 920\n",
      "running reward: -86.86 at episode 930\n",
      "running reward: -92.09 at episode 940\n",
      "running reward: -84.84 at episode 950\n",
      "running reward: -85.85 at episode 960\n",
      "running reward: -86.29 at episode 970\n",
      "running reward: -88.04 at episode 980\n",
      "running reward: -81.98 at episode 990\n",
      "running reward: -79.34 at episode 1000\n",
      "running reward: -81.46 at episode 1010\n",
      "running reward: -78.75 at episode 1020\n",
      "running reward: -76.86 at episode 1030\n",
      "running reward: -82.79 at episode 1040\n",
      "running reward: -84.85 at episode 1050\n",
      "running reward: -76.49 at episode 1060\n",
      "running reward: -71.58 at episode 1070\n",
      "running reward: -77.60 at episode 1080\n",
      "running reward: -77.46 at episode 1090\n",
      "running reward: -79.97 at episode 1100\n",
      "running reward: -82.82 at episode 1110\n",
      "running reward: -83.17 at episode 1120\n",
      "running reward: -74.04 at episode 1130\n",
      "running reward: -70.53 at episode 1140\n",
      "running reward: -64.63 at episode 1150\n",
      "running reward: -67.32 at episode 1160\n",
      "running reward: -75.63 at episode 1170\n",
      "running reward: -85.36 at episode 1180\n",
      "running reward: -84.44 at episode 1190\n",
      "running reward: -80.44 at episode 1200\n",
      "running reward: -59.22 at episode 1210\n",
      "running reward: -55.59 at episode 1220\n",
      "running reward: -61.03 at episode 1230\n",
      "running reward: -66.87 at episode 1240\n",
      "running reward: -75.09 at episode 1250\n",
      "running reward: -80.37 at episode 1260\n",
      "running reward: -88.20 at episode 1270\n",
      "running reward: -87.17 at episode 1280\n",
      "running reward: -92.28 at episode 1290\n",
      "running reward: -88.88 at episode 1300\n",
      "running reward: -93.30 at episode 1310\n",
      "running reward: -90.47 at episode 1320\n",
      "running reward: -90.01 at episode 1330\n",
      "running reward: -79.28 at episode 1340\n",
      "running reward: -81.20 at episode 1350\n",
      "running reward: -82.90 at episode 1360\n",
      "running reward: -89.72 at episode 1370\n",
      "running reward: -93.81 at episode 1380\n",
      "running reward: -84.96 at episode 1390\n",
      "running reward: -81.63 at episode 1400\n",
      "running reward: -84.95 at episode 1410\n",
      "running reward: -82.21 at episode 1420\n",
      "running reward: -83.74 at episode 1430\n",
      "running reward: -90.23 at episode 1440\n",
      "running reward: -82.74 at episode 1450\n",
      "running reward: -80.04 at episode 1460\n",
      "running reward: -77.90 at episode 1470\n",
      "running reward: -80.60 at episode 1480\n",
      "running reward: -78.46 at episode 1490\n",
      "running reward: -82.48 at episode 1500\n",
      "running reward: -83.08 at episode 1510\n",
      "running reward: -80.49 at episode 1520\n",
      "running reward: -84.56 at episode 1530\n",
      "running reward: -74.70 at episode 1540\n",
      "running reward: -84.81 at episode 1550\n",
      "running reward: -75.60 at episode 1560\n",
      "running reward: -80.06 at episode 1570\n",
      "running reward: -84.34 at episode 1580\n",
      "running reward: -81.46 at episode 1590\n",
      "running reward: -83.07 at episode 1600\n",
      "running reward: -83.95 at episode 1610\n",
      "running reward: -63.59 at episode 1620\n",
      "running reward: -73.89 at episode 1630\n",
      "running reward: -69.42 at episode 1640\n",
      "running reward: -77.09 at episode 1650\n",
      "running reward: -86.24 at episode 1660\n",
      "running reward: -74.92 at episode 1670\n",
      "running reward: -71.10 at episode 1680\n",
      "running reward: -78.60 at episode 1690\n",
      "running reward: -87.15 at episode 1700\n",
      "running reward: -80.88 at episode 1710\n",
      "running reward: -77.88 at episode 1720\n",
      "running reward: -86.71 at episode 1730\n",
      "running reward: -70.08 at episode 1740\n",
      "running reward: -73.17 at episode 1750\n",
      "running reward: -79.57 at episode 1760\n",
      "running reward: -65.54 at episode 1770\n",
      "running reward: -72.54 at episode 1780\n",
      "running reward: -77.44 at episode 1790\n",
      "running reward: -78.26 at episode 1800\n",
      "running reward: -78.91 at episode 1810\n",
      "running reward: -64.26 at episode 1820\n",
      "running reward: -69.07 at episode 1830\n",
      "running reward: -77.80 at episode 1840\n",
      "running reward: -81.54 at episode 1850\n",
      "running reward: -65.98 at episode 1860\n",
      "running reward: -74.52 at episode 1870\n",
      "running reward: -80.07 at episode 1880\n",
      "running reward: -84.12 at episode 1890\n",
      "running reward: -81.73 at episode 1900\n",
      "running reward: -89.02 at episode 1910\n",
      "running reward: -93.39 at episode 1920\n",
      "running reward: -96.00 at episode 1930\n",
      "running reward: -97.56 at episode 1940\n",
      "running reward: -93.38 at episode 1950\n",
      "running reward: -73.93 at episode 1960\n",
      "running reward: -80.42 at episode 1970\n",
      "running reward: -88.24 at episode 1980\n",
      "running reward: -87.61 at episode 1990\n",
      "running reward: -92.54 at episode 2000\n",
      "running reward: -92.25 at episode 2010\n",
      "running reward: -89.45 at episode 2020\n",
      "running reward: -83.73 at episode 2030\n",
      "running reward: -82.59 at episode 2040\n",
      "running reward: -84.87 at episode 2050\n",
      "running reward: -86.98 at episode 2060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: -92.16 at episode 2070\n",
      "running reward: -90.97 at episode 2080\n",
      "running reward: -94.55 at episode 2090\n",
      "running reward: -96.70 at episode 2100\n",
      "running reward: -92.40 at episode 2110\n",
      "running reward: -88.28 at episode 2120\n",
      "running reward: -89.42 at episode 2130\n",
      "running reward: -81.34 at episode 2140\n",
      "running reward: -68.22 at episode 2150\n",
      "running reward: -76.37 at episode 2160\n",
      "running reward: -81.70 at episode 2170\n",
      "running reward: -89.00 at episode 2180\n",
      "running reward: -83.24 at episode 2190\n",
      "running reward: -74.94 at episode 2200\n",
      "running reward: -84.95 at episode 2210\n",
      "running reward: -72.61 at episode 2220\n",
      "running reward: -78.88 at episode 2230\n",
      "running reward: -76.47 at episode 2240\n",
      "running reward: -79.84 at episode 2250\n",
      "running reward: -76.00 at episode 2260\n",
      "running reward: -73.57 at episode 2270\n",
      "running reward: -68.64 at episode 2280\n",
      "running reward: -81.18 at episode 2290\n",
      "running reward: -85.33 at episode 2300\n",
      "running reward: -82.51 at episode 2310\n",
      "running reward: -85.44 at episode 2320\n",
      "running reward: -81.06 at episode 2330\n",
      "running reward: -88.62 at episode 2340\n",
      "running reward: -87.24 at episode 2350\n",
      "running reward: -92.32 at episode 2360\n",
      "running reward: -71.68 at episode 2370\n",
      "running reward: -83.01 at episode 2380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmountain_car\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environment\n\u001b[0;32m      4\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(environment)\n\u001b[1;32m----> 5\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m    147\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(actor_losses) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(critic_losses)\n\u001b[1;32m--> 148\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Clear the loss and reward history\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\backprop.py:1081\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_gradients \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1079\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(output_gradients)]\n\u001b[1;32m-> 1081\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[0;32m   1090\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\backprop.py:156\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    154\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:409\u001b[0m, in \u001b[0;36m_ReluGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;129m@ops\u001b[39m\u001b[38;5;241m.\u001b[39mRegisterGradient(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ReluGrad\u001b[39m(op, grad):\n\u001b[1;32m--> 409\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:10719\u001b[0m, in \u001b[0;36mrelu_grad\u001b[1;34m(gradients, features, name)\u001b[0m\n\u001b[0;32m  10717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m  10718\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 10719\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10720\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReluGrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  10721\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m  10722\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.mountain_car import environment\n",
    "\n",
    "\n",
    "agent = Agent(environment)\n",
    "agent.learn()\n",
    "\n",
    "#{'action': array([0.6679693], dtype=float32), 'action_log_prob': array([1.0068668], dtype=float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f9259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:02:51.989368Z",
     "start_time": "2022-07-21T21:02:51.989368Z"
    }
   },
   "outputs": [],
   "source": [
    "state = agent.env.reset()\n",
    "#state = agent.get_state(obs)\n",
    "done = False\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "    agent.env.render()\n",
    "    actor_value, critic_value = agent.model(np.expand_dims(state,axis=0))\n",
    "    mu = actor_value[:,0:1]\n",
    "    sigma = actor_value[:,1:]\n",
    "    # Sample action from action probability distribution\n",
    "    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "    action, action_log_prob = agent.choose_action(mu,sigma)\n",
    "    #action = mu#agent.max_action(state)\n",
    "            \n",
    "    # Step\n",
    "    obs_,reward,done, info = agent.env.step(action)\n",
    "            \n",
    "    # Get next state\n",
    "    score += reward\n",
    "    #state_ = agent.get_state(obs_)\n",
    "    state = obs_\n",
    "    # Set state as next state so the agent keeps \n",
    "    #state = state_\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62385c49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:02:51.990369Z",
     "start_time": "2022-07-21T21:02:51.990369Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "agent.env.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec6165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
