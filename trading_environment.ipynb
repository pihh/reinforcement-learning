{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c22e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T16:42:11.145341Z",
     "start_time": "2022-08-08T16:42:10.581378Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()\n",
    "\n",
    "import gym\n",
    "import src.environments.continuous.stock_trading  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3248213",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T15:21:01.332156Z",
     "start_time": "2022-08-08T15:20:59.717151Z"
    }
   },
   "source": [
    "### Train the agent\n",
    "* Run it until he has a running average above the success_threshold\n",
    "* Use a large number of episodes for the running average ( 1000+ ) so if even it falls into a privileged sample, it wont be prone to error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2904b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T15:21:04.196241Z",
     "start_time": "2022-08-08T15:21:01.334159Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab573ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T16:42:11.161311Z",
     "start_time": "2022-08-08T16:42:11.147340Z"
    }
   },
   "outputs": [],
   "source": [
    "def environment():\n",
    "    env = gym.make('StockTradingEnvironment-v0',\n",
    "        use_technical_indicators= [\n",
    "        \"macd\",\n",
    "        \"boll_ub\",\n",
    "        \"boll_lb\",\n",
    "        \"rsi_30\",\n",
    "        \"cci_30\",\n",
    "        \"dx_30\",\n",
    "        \"close_30_sma\",\n",
    "        \"close_60_sma\",\n",
    "    ])\n",
    "    \n",
    "    env.success_threshold =0.25 # 25%\n",
    "    return env\n",
    "\n",
    "# from src.environments.continuous.trading import environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d896e9f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T16:42:16.190620Z",
     "start_time": "2022-08-08T16:42:11.162311Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.actor_critic.a2c import A2CAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f83292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T16:42:17.083589Z",
     "start_time": "2022-08-08T16:42:16.192591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StockTradingEnvironment-v0\n",
      "\n",
      "[Errno 2] Unable to open file (unable to open file: name = 'storage/environments/AaplStockTradingEnvironment-v0/23773bb461926c23ff13bee130dda8e0/models/a2c-actor.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\n",
      "[Errno 2] Unable to open file (unable to open file: name = 'storage/environments/AaplStockTradingEnvironment-v0/23773bb461926c23ff13bee130dda8e0/models/a2c-critic.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
      "\n",
      "StockTradingEnvironment-v0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent=A2CAgent(environment, epochs=1, actor_learning_rate=0.000025,critic_learning_rate=0.000025,policy=\"CNN\")\n",
    "agent.load()\n",
    "environment().success_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bbb3cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T16:42:17.099013Z",
     "start_time": "2022-08-08T16:42:17.084592Z"
    }
   },
   "outputs": [],
   "source": [
    "# agent.hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947b8a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T16:42:10.586Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.env.mode = \"train\"\n",
    "agent.learning_log.episodes = 0\n",
    "agent.learn(\n",
    "    timesteps=-1, \n",
    "    log_every=100,\n",
    "    success_threshold_lookback=1000,\n",
    "    success_strict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef5824",
   "metadata": {},
   "source": [
    "### Test the results\n",
    "* Runs a set of episodes with unseen data\n",
    "* Stores the results in a csv file for later consulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbe8c2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T16:42:10.586Z"
    }
   },
   "outputs": [],
   "source": [
    "success = 0\n",
    "n_tests = 10000\n",
    "\n",
    "scores = []\n",
    "targets = []\n",
    "portfolio_target_ratios = []\n",
    "initial_investments = []\n",
    "\n",
    "for i in trange(n_tests):\n",
    "    state = agent.env.reset(visualize=False,mode=\"test\")\n",
    "    step = 0\n",
    "    score = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    \n",
    "    targets.append((agent.env.episode_target-agent.env.initial_investment)/agent.env.initial_investment)\n",
    "    initial_investments.append(agent.env.initial_investment)\n",
    "\n",
    "    while not done:\n",
    "        agent.env.render()\n",
    "        #state = np.expand_dims(state, axis=0)\n",
    "        action, action_onehot, prediction = agent.act(state)\n",
    "        # Retrieve new state, reward, and whether the state is terminal\n",
    "        next_state, reward, done, info = agent.env.step(action)\n",
    "        #print(action, reward, agent.env.portfolio_value)\n",
    "        # Memorize (state, action, reward) for training\n",
    "        #self.buffer.remember(np.expand_dims(state, axis=0), action_onehot, reward)\n",
    "        # Update current state\n",
    "        if done :\n",
    "            if agent.env.portfolio_value > agent.env.initial_investment:\n",
    "                success +=1\n",
    "\n",
    "        step+=1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    \n",
    "    # Track scores and ratios\n",
    "    scores.append(score)\n",
    "    portfolio_target_ratios.append(info[\"portfolio_value\"]/info[\"episode_target\"] -1)\n",
    "    \n",
    "    \n",
    "test_results_dataframe = pd.DataFrame([[\n",
    "    n_tests,\n",
    "    str(round(np.mean(scores)*100,3))+'%',\n",
    "    str(round(np.mean(targets)*100,3))+'%',\n",
    "    str(round(np.mean(portfolio_target_ratios)*100,3))+'%',\n",
    "    str(round(min(scores)*100,3))+'%',\n",
    "    str(round(max(scores)*100,3))+'%',\n",
    "    str(round((success/n_tests)*100,3)) +'%'\n",
    "]],\n",
    "    columns=[\n",
    "        '# Blind tests',\n",
    "        '% Average portfolio return', \n",
    "        '% Desired portfolio return', \n",
    "        'Portfolio/Target rate',\n",
    "        '% Historical minimum return',\n",
    "        '% Historical maximum return', \n",
    "        '% Episodes concluded with positive outcome'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_results_dataframe.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50379f0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T16:42:10.587Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.results_writer.store_test_results(agent,test_results_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a982e5d3",
   "metadata": {},
   "source": [
    "### Visual test\n",
    "* Runs a set of episodes with unseen data\n",
    "* See the evolution in real time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad483d48",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T16:42:10.588Z"
    }
   },
   "outputs": [],
   "source": [
    "success = 0\n",
    "n_tests = 2\n",
    "\n",
    "scores = []\n",
    "targets = []\n",
    "\n",
    "\n",
    "for i in trange(n_tests):\n",
    "    state = agent.env.reset(visualize=True,mode=\"test\")\n",
    "    step = 0\n",
    "    score = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    targets.append((agent.env.episode_target-agent.env.initial_investment)/agent.env.initial_investment)\n",
    "    initial_portfolio = agent.env.portfolio_value\n",
    "\n",
    "    while not done:\n",
    "        agent.env.render()\n",
    "        #state = np.expand_dims(state, axis=0)\n",
    "        action, action_onehot, prediction = agent.act(state)\n",
    "        # Retrieve new state, reward, and whether the state is terminal\n",
    "        next_state, reward, done, _ = agent.env.step(action)\n",
    "        #print(action, reward, agent.env.portfolio_value)\n",
    "        # Memorize (state, action, reward) for training\n",
    "        #self.buffer.remember(np.expand_dims(state, axis=0), action_onehot, reward)\n",
    "        # Update current state\n",
    "        if done :\n",
    "            if agent.env.portfolio_value > agent.env.initial_investment:\n",
    "                success +=1\n",
    "\n",
    "        step+=1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    \n",
    "    #print(score,initial_portfolio, agent.env.portfolio_value)\n",
    "    scores.append(score)\n",
    "\n",
    "agent.env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
