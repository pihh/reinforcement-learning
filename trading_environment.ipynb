{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c22e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:58:32.088594Z",
     "start_time": "2022-08-08T10:58:31.516068Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()\n",
    "\n",
    "import gym\n",
    "import src.environments.continuous.stock_trading  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from tqdm.notebook import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33323dcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:58:33.572529Z",
     "start_time": "2022-08-08T10:58:32.089567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('StockTradingEnvironment-v0', use_technical_indicators= [\n",
    "    \"macd\",\n",
    "    \"boll_ub\",\n",
    "    \"boll_lb\",\n",
    "    \"rsi_30\",\n",
    "    \"cci_30\",\n",
    "    \"dx_30\",\n",
    "    \"close_30_sma\",\n",
    "    \"close_60_sma\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2904b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:58:36.279351Z",
     "start_time": "2022-08-08T10:58:33.573529Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.actor_critic.a2c import A2CAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab573ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:58:36.295143Z",
     "start_time": "2022-08-08T10:58:36.281352Z"
    }
   },
   "outputs": [],
   "source": [
    "def environment():\n",
    "    env = gym.make('StockTradingEnvironment-v0',\n",
    "                   use_technical_indicators= [\n",
    "        \"macd\",\n",
    "        \"boll_ub\",\n",
    "        \"boll_lb\",\n",
    "        \"rsi_30\",\n",
    "        \"cci_30\",\n",
    "        \"dx_30\",\n",
    "        \"close_30_sma\",\n",
    "        \"close_60_sma\",\n",
    "    ])\n",
    "    \n",
    "    env.success_threshold =0.06 # 5%\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03f83292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:58:37.110744Z",
     "start_time": "2022-08-08T10:58:36.297143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Models successfully loaded *\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent=A2CAgent(environment, epochs=1, actor_learning_rate=0.000025,critic_learning_rate=0.000025,policy=\"CNN\")\n",
    "agent.load()\n",
    "environment().success_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bbb3cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-08T10:58:37.126743Z",
     "start_time": "2022-08-08T10:58:37.112744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ed75666dd5e47547b3de000d534a9270'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947b8a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T10:58:31.520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 100 * Worker * False * Moving Avg Reward is ==> 0.06214 * Last Reward was ==> 0.08256\n",
      "Episode * 200 * Worker * False * Moving Avg Reward is ==> 0.05940 * Last Reward was ==> 0.08547\n",
      "Episode * 300 * Worker * False * Moving Avg Reward is ==> 0.06442 * Last Reward was ==> 0.02403\n"
     ]
    }
   ],
   "source": [
    "agent.env.mode = \"train\"\n",
    "agent.learning_log.episodes = 0\n",
    "agent.learn(\n",
    "    timesteps=-1, \n",
    "    log_every=100,\n",
    "    success_threshold_lookback=1000,\n",
    "    success_strict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cbe8c2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T10:58:31.521Z"
    }
   },
   "outputs": [],
   "source": [
    "success = 0\n",
    "n_tests = 0 #10000\n",
    "\n",
    "scores = []\n",
    "targets = []\n",
    "\n",
    "for i in trange(n_tests):\n",
    "    state = agent.env.reset(visualize=False,mode=\"test\")\n",
    "    step = 0\n",
    "    score = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    targets.append((agent.env.episode_target-agent.env.initial_investment)/agent.env.initial_investment)\n",
    "\n",
    "    while not done:\n",
    "        agent.env.render()\n",
    "        #state = np.expand_dims(state, axis=0)\n",
    "        action, action_onehot, prediction = agent.act(state)\n",
    "        # Retrieve new state, reward, and whether the state is terminal\n",
    "        next_state, reward, done, _ = agent.env.step(action)\n",
    "        #print(action, reward, agent.env.portfolio_value)\n",
    "        # Memorize (state, action, reward) for training\n",
    "        #self.buffer.remember(np.expand_dims(state, axis=0), action_onehot, reward)\n",
    "        # Update current state\n",
    "        if done :\n",
    "            if agent.env.portfolio_value > agent.env.initial_investment:\n",
    "                success +=1\n",
    "\n",
    "        step+=1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    \n",
    "    #print(score,initial_portfolio, agent.env.portfolio_value)\n",
    "    scores.append(score)\n",
    "    \n",
    "test_results_dataframe = pd.DataFrame([[\n",
    "    n_tests,\n",
    "    str(np.mean(scores)*100)+'%',\n",
    "    str(np.mean(targets)*100)+'%',\n",
    "    str(min(scores)*100)+'%',\n",
    "    str(max(scores)*100)+'%',\n",
    "    str((success/n_tests)*100) +'%'\n",
    "]],\n",
    "    columns=[\n",
    "        '# Blind tests',\n",
    "        '% Average portfolio return', \n",
    "        '% Desired portfolio return', \n",
    "        '% Historical minimum return',\n",
    "        '% Historical maximum return', \n",
    "        '% Episode concluded with positive outcome'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print(np.mean(scores))\n",
    "# print(min(scores))\n",
    "# print(max(scores))\n",
    "# print(success)\n",
    "\n",
    "test_results_dataframe.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1a6f7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T10:58:31.522Z"
    }
   },
   "outputs": [],
   "source": [
    "#agent.results_writer.store_test_results(agent,test_results_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0086dc",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T10:58:31.523Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datetime import datetime\n",
    "\n",
    "# test_results_dataframe.to_csv(agent.writer_log_directory+'/results__'+datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0728f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-08T10:58:31.524Z"
    }
   },
   "outputs": [],
   "source": [
    "success = 0\n",
    "n_tests = 2\n",
    "\n",
    "scores = []\n",
    "targets = []\n",
    "\n",
    "\n",
    "for i in trange(n_tests):\n",
    "    state = agent.env.reset(visualize=True,mode=\"test\")\n",
    "    step = 0\n",
    "    score = 0\n",
    "    reward = 0\n",
    "    done = False\n",
    "    targets.append((agent.env.episode_target-agent.env.initial_investment)/agent.env.initial_investment)\n",
    "    initial_portfolio = agent.env.portfolio_value\n",
    "\n",
    "    while not done:\n",
    "        agent.env.render()\n",
    "        #state = np.expand_dims(state, axis=0)\n",
    "        action, action_onehot, prediction = agent.act(state)\n",
    "        # Retrieve new state, reward, and whether the state is terminal\n",
    "        next_state, reward, done, _ = agent.env.step(action)\n",
    "        #print(action, reward, agent.env.portfolio_value)\n",
    "        # Memorize (state, action, reward) for training\n",
    "        #self.buffer.remember(np.expand_dims(state, axis=0), action_onehot, reward)\n",
    "        # Update current state\n",
    "        if done :\n",
    "            if agent.env.portfolio_value > agent.env.initial_investment:\n",
    "                success +=1\n",
    "\n",
    "        step+=1\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    \n",
    "    #print(score,initial_portfolio, agent.env.portfolio_value)\n",
    "    scores.append(score)\n",
    "\n",
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e7dfa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
