{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09074ee0",
   "metadata": {},
   "source": [
    "#### PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T20:11:16.569581Z",
     "start_time": "2022-07-25T20:11:16.540546Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T20:11:21.456259Z",
     "start_time": "2022-07-25T20:11:17.180499Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "from src.utils.logger import LearningLogger\n",
    "\n",
    "import scipy.signal\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ec0c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T20:11:21.471760Z",
     "start_time": "2022-07-25T20:11:21.458260Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def set_global_determinism(seed=SEED):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460fbf93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:13:32.667034Z",
     "start_time": "2022-07-25T16:05:03.295253Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f'GPUs {gpus}')\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass\n",
    "\n",
    "class Environment(Process):\n",
    "    def __init__(self, env_idx, child_conn, env_name, state_size, action_size, visualize=False):\n",
    "        super(Environment, self).__init__()\n",
    "        self.env = gym.make(env_name)\n",
    "        self.is_render = visualize\n",
    "        self.env_idx = env_idx\n",
    "        self.child_conn = child_conn\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def run(self):\n",
    "        super(Environment, self).run()\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        self.child_conn.send(state)\n",
    "        while True:\n",
    "            action = self.child_conn.recv()\n",
    "            if self.is_render and self.env_idx == 0:\n",
    "                self.env.render()\n",
    "\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            self.child_conn.send([state, reward, done, info])\n",
    "\n",
    "\n",
    "class Actor_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        ENTROPY_LOSS = 0.001\n",
    "        \n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = K.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "\n",
    "class Critic_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        V = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "        V = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        V = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[self.critic_PPO2_loss(old_values)], optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def critic_PPO2_loss(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "            LOSS_CLIPPING = 0.2\n",
    "            clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = 10000 # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = 0.00025\n",
    "        self.epochs = 10 # training epochs\n",
    "        self.shuffle=False\n",
    "        self.Training_batch = 1000\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n",
    "        \n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        \n",
    "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n",
    "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\" example:\n",
    "        pred = np.array([0.05, 0.85, 0.1])\n",
    "        action_size = 3\n",
    "        np.random.choice(a, p=pred)\n",
    "        result>>> 1, because it have the highest probability to be taken\n",
    "        \"\"\"\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        return action, action_onehot, prediction\n",
    "\n",
    "    def discount_rewards(self, reward):#gaes is better\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        # We apply the discount and normalize it to avoid big variability of rewards\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.9, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        #discounted_r = self.discount_rewards(rewards)\n",
    "        #advantages = np.vstack(discounted_r - values)\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "        '''\n",
    "        pylab.plot(advantages,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        # pack all advantages, predictions and actions to y_true and when they are received\n",
    "        # in custom PPO loss function we unpack it\n",
    "        y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.replay_count += 1\n",
    " \n",
    "    def load(self):\n",
    "        self.Actor.Actor.load_weights(self.Actor_name)\n",
    "        self.Critic.Critic.load_weights(self.Critic_name)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.Actor.save_weights(self.Actor_name)\n",
    "        self.Critic.Critic.save_weights(self.Critic_name)\n",
    "        \n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores_.append(score)\n",
    "        self.episodes_.append(episode)\n",
    "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes_, self.scores_, 'b')\n",
    "            pylab.plot(self.episodes_, self.average_, 'r')\n",
    "            pylab.title(self.env_name+\" PPO training cycle\", fontsize=18)\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.grid(True)\n",
    "                pylab.savefig(self.env_name+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "        # saving best models\n",
    "        if self.average_[-1] >= self.max_average:\n",
    "            self.max_average = self.average_[-1]\n",
    "            self.save()\n",
    "            SAVING = \"SAVING\"\n",
    "            # decreaate learning rate every saved model\n",
    "            self.lr *= 0.95\n",
    "            K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
    "            K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
    "        else:\n",
    "            SAVING = \"\"\n",
    "\n",
    "        return self.average_[-1], SAVING\n",
    "    \n",
    "    def run(self): # train only when episode is finished\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "                    \n",
    "                    self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "    def run_batch(self): # train every self.Training_batch episodes\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            for t in range(self.Training_batch):\n",
    "                self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "                    \n",
    "            self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()  \n",
    "\n",
    "        \n",
    "    def run_multiprocesses(self, num_worker = 4):\n",
    "        works, parent_conns, child_conns = [], [], []\n",
    "        for idx in range(num_worker):\n",
    "            parent_conn, child_conn = Pipe()\n",
    "            work = Environment(idx, child_conn, self.env_name, self.state_size[0], self.action_size, True)\n",
    "            work.start()\n",
    "            works.append(work)\n",
    "            parent_conns.append(parent_conn)\n",
    "            child_conns.append(child_conn)\n",
    "\n",
    "        states =        [[] for _ in range(num_worker)]\n",
    "        next_states =   [[] for _ in range(num_worker)]\n",
    "        actions =       [[] for _ in range(num_worker)]\n",
    "        rewards =       [[] for _ in range(num_worker)]\n",
    "        dones =         [[] for _ in range(num_worker)]\n",
    "        predictions =   [[] for _ in range(num_worker)]\n",
    "        score =         [0 for _ in range(num_worker)]\n",
    "\n",
    "        state = [0 for _ in range(num_worker)]\n",
    "        for worker_id, parent_conn in enumerate(parent_conns):\n",
    "            state[worker_id] = parent_conn.recv()\n",
    "\n",
    "        while self.episode < self.EPISODES:\n",
    "            predictions_list = self.Actor.predict(np.reshape(state, [num_worker, self.state_size[0]]))\n",
    "            actions_list = [np.random.choice(self.action_size, p=i) for i in predictions_list]\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                parent_conn.send(actions_list[worker_id])\n",
    "                action_onehot = np.zeros([self.action_size])\n",
    "                action_onehot[actions_list[worker_id]] = 1\n",
    "                actions[worker_id].append(action_onehot)\n",
    "                predictions[worker_id].append(predictions_list[worker_id])\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                next_state, reward, done, _ = parent_conn.recv()\n",
    "\n",
    "                states[worker_id].append(state[worker_id])\n",
    "                next_states[worker_id].append(next_state)\n",
    "                rewards[worker_id].append(reward)\n",
    "                dones[worker_id].append(done)\n",
    "                state[worker_id] = next_state\n",
    "                score[worker_id] += reward\n",
    "\n",
    "                if done:\n",
    "                    average, SAVING = self.PlotModel(score[worker_id], self.episode)\n",
    "                    print(\"episode: {}/{}, worker: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, worker_id, score[worker_id], average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/score_per_episode', score[worker_id], self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/learning_rate', self.lr, self.episode)\n",
    "                    score[worker_id] = 0\n",
    "                    if(self.episode < self.EPISODES):\n",
    "                        self.episode += 1\n",
    "                        \n",
    "            for worker_id in range(num_worker):\n",
    "                if len(states[worker_id]) >= self.Training_batch:\n",
    "                    self.replay(states[worker_id], actions[worker_id], rewards[worker_id], predictions[worker_id], dones[worker_id], next_states[worker_id])\n",
    "                    \n",
    "                    states[worker_id] = []\n",
    "                    next_states[worker_id] = []\n",
    "                    actions[worker_id] = []\n",
    "                    rewards[worker_id] = []\n",
    "                    dones[worker_id] = []\n",
    "                    predictions[worker_id] = []\n",
    "\n",
    "        # terminating processes after while loop\n",
    "        works.append(work)\n",
    "        for work in works:\n",
    "            work.terminate()\n",
    "            print('TERMINATED:', work)\n",
    "            work.join()\n",
    "            \n",
    "\n",
    "    def test(self, test_episodes = 100):\n",
    "        self.load()\n",
    "        for e in range(100):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size[0]])\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state)[0])\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, test_episodes, score))\n",
    "                    break\n",
    "        self.env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'LunarLander-v2'\n",
    "    agent = PPOAgent(env_name)\n",
    "    #agent.run() # train as PPO, train every epesode\n",
    "    agent.run_batch() # train as PPO, train every batch, trains better\n",
    "    #agent.run_multiprocesses(num_worker = 8)  # train PPO multiprocessed (fastest)\n",
    "    #agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76af8450",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8de4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da045390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T08:29:35.011332Z",
     "start_time": "2022-07-26T08:29:34.990608Z"
    }
   },
   "outputs": [],
   "source": [
    "env_name = 'LunarLander-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d21a858",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T08:29:38.140390Z",
     "start_time": "2022-07-26T08:29:35.132501Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "import scipy.signal\n",
    "import time\n",
    "from src.environments.continuous.inverted_pendulum import environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c4323d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T08:29:39.356610Z",
     "start_time": "2022-07-26T08:29:38.142291Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e4a8fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T08:31:42.904397Z",
     "start_time": "2022-07-26T08:31:42.885636Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kwargs {'critic_optimizer': <keras.optimizer_v2.adam.Adam object at 0x000001D057114E80>, 'alpha': 0.001} locals {'self': <__main__.B object at 0x000001D05715A470>, 'environment': <function environment at 0x000001D04D4153F0>, 'loss_keys': [], 'epsilon': 1.0, 'epsilon_min': 0.01, 'epsilon_decay': 1e-05, 'kwargs': {'critic_optimizer': <keras.optimizer_v2.adam.Adam object at 0x000001D057114E80>, 'alpha': 0.001}}\n",
      "init_loggers\n",
      "('action_lower_bounds', array([-1.], dtype=float32))\n",
      "('action_space_mode', 'continuous')\n",
      "('action_upper_bounds', array([1.], dtype=float32))\n",
      "('actions', [0])\n",
      "('env', <TimeLimit<OrderEnforcing<InvertedPendulumBulletEnv<InvertedPendulumBulletEnv-v0>>>>)\n",
      "('epsilon', 1.0)\n",
      "('epsilon_', 1.0)\n",
      "('epsilon_decay', 1e-05)\n",
      "('epsilon_decay_', 1e-05)\n",
      "('epsilon_min', 0.01)\n",
      "('learning_log_loss_keys', [])\n",
      "('n_actions', 1)\n",
      "('n_inputs', 5)\n",
      "('observation_shape', (5,))\n",
      "('running_reward', <src.utils.running_reward.RunningReward object at 0x000001D057115030>)\n",
      "<TimeLimit<OrderEnforcing<InvertedPendulumBulletEnv<InvertedPendulumBulletEnv-v0>>>>\n",
      "EnvSpec(entry_point='pybullet_envs.gym_pendulum_envs:InvertedPendulumBulletEnv', reward_threshold=950.0, nondeterministic=False, max_episode_steps=1000, order_enforce=True, kwargs={}, namespace=None, name='InvertedPendulumBulletEnv', version=0)\n",
      "InvertedPendulumBulletEnv\n",
      "{'self': <__main__.B object at 0x000001D05715A470>, 'environment': <function environment at 0x000001D04D4153F0>, 'critic_optimizer': <keras.optimizer_v2.adam.Adam object at 0x000001D057114E80>, 'alpha': 0.001, '__class__': <class '__main__.B'>}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.B at 0x1d05715a470>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class B(Agent):\n",
    "    def __init__(self,environment,critic_optimizer =Adam(0.0001),alpha=0.001):\n",
    "        super(B,self).__init__(environment,critic_optimizer=critic_optimizer,alpha=alpha)\n",
    "        \n",
    "B(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d533af88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T08:28:34.063738Z",
     "start_time": "2022-07-26T08:28:34.050742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_Agent__init_environment',\n",
       "  <function src.agents.agent.Agent.__init_environment(self)>),\n",
       " ('_Agent__init_loggers',\n",
       "  <function src.agents.agent.Agent.__init_loggers(self)>),\n",
       " ('_Agent__init_reward_tracker',\n",
       "  <function src.agents.agent.Agent.__init_reward_tracker(self)>),\n",
       " ('__class__', type),\n",
       " ('__delattr__', <slot wrapper '__delattr__' of 'object' objects>),\n",
       " ('__dict__',\n",
       "  mappingproxy({'__module__': '__main__',\n",
       "                '__init__': <function __main__.B.__init__(self, environment, critic_optimizer=<keras.optimizer_v2.adam.Adam object at 0x00000158E67B1DB0>, alpha=0.001)>,\n",
       "                '__doc__': None})),\n",
       " ('__dir__', <method '__dir__' of 'object' objects>),\n",
       " ('__doc__', None),\n",
       " ('__eq__', <slot wrapper '__eq__' of 'object' objects>),\n",
       " ('__format__', <method '__format__' of 'object' objects>),\n",
       " ('__ge__', <slot wrapper '__ge__' of 'object' objects>),\n",
       " ('__getattribute__', <slot wrapper '__getattribute__' of 'object' objects>),\n",
       " ('__gt__', <slot wrapper '__gt__' of 'object' objects>),\n",
       " ('__hash__', <slot wrapper '__hash__' of 'object' objects>),\n",
       " ('__init__',\n",
       "  <function __main__.B.__init__(self, environment, critic_optimizer=<keras.optimizer_v2.adam.Adam object at 0x00000158E67B1DB0>, alpha=0.001)>),\n",
       " ('__init_subclass__', <function B.__init_subclass__>),\n",
       " ('__le__', <slot wrapper '__le__' of 'object' objects>),\n",
       " ('__lt__', <slot wrapper '__lt__' of 'object' objects>),\n",
       " ('__module__', '__main__'),\n",
       " ('__ne__', <slot wrapper '__ne__' of 'object' objects>),\n",
       " ('__new__', <function object.__new__(*args, **kwargs)>),\n",
       " ('__reduce__', <method '__reduce__' of 'object' objects>),\n",
       " ('__reduce_ex__', <method '__reduce_ex__' of 'object' objects>),\n",
       " ('__repr__', <slot wrapper '__repr__' of 'object' objects>),\n",
       " ('__setattr__', <slot wrapper '__setattr__' of 'object' objects>),\n",
       " ('__sizeof__', <method '__sizeof__' of 'object' objects>),\n",
       " ('__str__', <slot wrapper '__str__' of 'object' objects>),\n",
       " ('__subclasshook__', <function B.__subclasshook__>),\n",
       " ('__weakref__', <attribute '__weakref__' of 'Agent' objects>),\n",
       " ('after_learn_cycle',\n",
       "  <function src.agents.agent.Agent.after_learn_cycle(self)>),\n",
       " ('before_learn_cycle',\n",
       "  <function src.agents.agent.Agent.before_learn_cycle(self)>),\n",
       " ('before_learn_episode',\n",
       "  <function src.agents.agent.Agent.before_learn_episode(self)>),\n",
       " ('before_test_episode',\n",
       "  <function src.agents.agent.Agent.before_test_episode(self)>),\n",
       " ('choose_action',\n",
       "  <function src.agents.agent.Agent.choose_action(self, state)>),\n",
       " ('decrement_epsilon',\n",
       "  <function src.agents.agent.Agent.decrement_epsilon(self)>),\n",
       " ('did_finnish_learning',\n",
       "  <function src.agents.agent.Agent.did_finnish_learning(self, success_threshold, episode)>),\n",
       " ('learn', <function src.agents.agent.Agent.learn(self)>),\n",
       " ('learning_condition',\n",
       "  <function src.agents.agent.Agent.learning_condition(self, timesteps, timestep)>),\n",
       " ('on_learn_end',\n",
       "  <function src.agents.agent.Agent.on_learn_end(self, plot_results)>),\n",
       " ('on_learn_episode_end',\n",
       "  <function src.agents.agent.Agent.on_learn_episode_end(self, score, log_each_n_episodes, log_level, success_threshold)>),\n",
       " ('on_learn_start',\n",
       "  <function src.agents.agent.Agent.on_learn_start(self, timesteps, success_threshold, reset)>),\n",
       " ('on_test_episode_end',\n",
       "  <function src.agents.agent.Agent.on_test_episode_end(self, episode, score, render)>),\n",
       " ('on_test_episode_start',\n",
       "  <function src.agents.agent.Agent.on_test_episode_start(self)>),\n",
       " ('plot_learning_results',\n",
       "  <function src.agents.agent.Agent.plot_learning_results(self)>),\n",
       " ('test',\n",
       "  <function src.agents.agent.Agent.test(self, episodes=10, render=True)>),\n",
       " ('validate_learn',\n",
       "  <function src.agents.agent.Agent.validate_learn(self, timesteps, success_threshold, reset)>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import inspect\n",
    "\n",
    "inspect.getmembers(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fc3a2e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T07:26:07.489086Z",
     "start_time": "2022-07-26T07:26:07.484120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "d ={}\n",
    "d['x'] = 1\n",
    "for c in d.values():\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab71a3db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189f442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba37de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T07:14:50.177794Z",
     "start_time": "2022-07-26T07:14:50.156763Z"
    }
   },
   "outputs": [],
   "source": [
    "class A:\n",
    "    def __init__(self,**kwargs):\n",
    "        self.__boot(locals())\n",
    "    def __boot(self,d):\n",
    "        print(type(self).__name__, d)\n",
    "        \n",
    "class B(A):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(B,self).__init__(**kwargs)\n",
    "        \n",
    "B(a=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d8d95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T20:11:27.319197Z",
     "start_time": "2022-07-25T20:11:27.293988Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def discounted_cumulative_sums(x, discount):\n",
    "    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "\n",
    "class Buffer:\n",
    "    # Buffer for storing trajectories\n",
    "    def __init__(self, observation_dimensions, size, gamma=0.99, lam=0.95):\n",
    "        # Buffer initialization\n",
    "        self.observation_buffer = np.zeros(\n",
    "            (size, observation_dimensions), dtype=np.float32\n",
    "        )\n",
    "        self.action_buffer = np.zeros(size, dtype=np.int32)\n",
    "        self.advantage_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.reward_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.return_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.value_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobability_buffer = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "\n",
    "    def store(self, observation, action, reward, value, logprobability):\n",
    "        # Append one step of agent-environment interaction\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.value_buffer[self.pointer] = value\n",
    "        self.logprobability_buffer[self.pointer] = logprobability\n",
    "        self.pointer += 1\n",
    "\n",
    "    def finish_trajectory(self, last_value=0):\n",
    "        # Finish the trajectory by computing advantage estimates and rewards-to-go\n",
    "        path_slice = slice(self.trajectory_start_index, self.pointer)\n",
    "        rewards = np.append(self.reward_buffer[path_slice], last_value)\n",
    "        values = np.append(self.value_buffer[path_slice], last_value)\n",
    "\n",
    "        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n",
    "\n",
    "        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            deltas, self.gamma * self.lam\n",
    "        )\n",
    "        self.return_buffer[path_slice] = discounted_cumulative_sums(\n",
    "            rewards, self.gamma\n",
    "        )[:-1]\n",
    "\n",
    "        self.trajectory_start_index = self.pointer\n",
    "\n",
    "    def get(self):\n",
    "        # Get all data of the buffer and normalize the advantages\n",
    "        self.pointer, self.trajectory_start_index = 0, 0\n",
    "        advantage_mean, advantage_std = (\n",
    "            np.mean(self.advantage_buffer),\n",
    "            np.std(self.advantage_buffer),\n",
    "        )\n",
    "        self.advantage_buffer = (self.advantage_buffer - advantage_mean) / advantage_std\n",
    "        return (\n",
    "            self.observation_buffer,\n",
    "            self.action_buffer,\n",
    "            self.advantage_buffer,\n",
    "            self.return_buffer,\n",
    "            self.logprobability_buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def mlp(x, sizes, activation=tf.tanh, output_activation=None):\n",
    "    # Build a feedforward neural network\n",
    "    for size in sizes[:-1]:\n",
    "        x = layers.Dense(units=size, activation=activation)(x)\n",
    "    return layers.Dense(units=sizes[-1], activation=output_activation)(x)\n",
    "\n",
    "\n",
    "def logprobabilities(logits, a):\n",
    "    # Compute the log-probabilities of taking actions a by using the logits (i.e. the output of the actor)\n",
    "    logprobabilities_all = tf.nn.log_softmax(logits)\n",
    "    logprobability = tf.reduce_sum(\n",
    "        tf.one_hot(a, num_actions) * logprobabilities_all, axis=1\n",
    "    )\n",
    "    return logprobability\n",
    "\n",
    "\n",
    "# Sample action from actor\n",
    "@tf.function\n",
    "def sample_action(observation):\n",
    "    logits = actor(observation)\n",
    "    action = tf.squeeze(tf.random.categorical(logits, 1), axis=1)\n",
    "    return logits, action\n",
    "\n",
    "\n",
    "# Train the policy by maxizing the PPO-Clip objective\n",
    "@tf.function\n",
    "def train_policy(\n",
    "    observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "):\n",
    "\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        ratio = tf.exp(\n",
    "            logprobabilities(actor(observation_buffer), action_buffer)\n",
    "            - logprobability_buffer\n",
    "        )\n",
    "        min_advantage = tf.where(\n",
    "            advantage_buffer > 0,\n",
    "            (1 + clip_ratio) * advantage_buffer,\n",
    "            (1 - clip_ratio) * advantage_buffer,\n",
    "        )\n",
    "\n",
    "        policy_loss = -tf.reduce_mean(\n",
    "            tf.minimum(ratio * advantage_buffer, min_advantage)\n",
    "        )\n",
    "    policy_grads = tape.gradient(policy_loss, actor.trainable_variables)\n",
    "    policy_optimizer.apply_gradients(zip(policy_grads, actor.trainable_variables))\n",
    "\n",
    "    kl = tf.reduce_mean(\n",
    "        logprobability_buffer\n",
    "        - logprobabilities(actor(observation_buffer), action_buffer)\n",
    "    )\n",
    "    kl = tf.reduce_sum(kl)\n",
    "    return kl\n",
    "\n",
    "\n",
    "# Train the value function by regression on mean-squared error\n",
    "@tf.function\n",
    "def train_value_function(observation_buffer, return_buffer):\n",
    "    with tf.GradientTape() as tape:  # Record operations for automatic differentiation.\n",
    "        value_loss = tf.reduce_mean((return_buffer - critic(observation_buffer)) ** 2)\n",
    "    value_grads = tape.gradient(value_loss, critic.trainable_variables)\n",
    "    value_optimizer.apply_gradients(zip(value_grads, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7858920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T20:11:27.860537Z",
     "start_time": "2022-07-25T20:11:27.853542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the PPO algorithm\n",
    "steps_per_epoch = 1000\n",
    "epochs = 10000 \n",
    "gamma = 0.99\n",
    "clip_ratio = 0.2\n",
    "policy_learning_rate = 0.00025 #3e-4\n",
    "value_function_learning_rate = 0.00025 #1e-3\n",
    "train_policy_iterations = 10\n",
    "train_value_iterations = 10\n",
    "lam = 0.9 #97\n",
    "target_kl = 0.001\n",
    "hidden_sizes = (512, 256, 64)\n",
    "\n",
    "# True if you want to render the environment\n",
    "render = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e247ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T20:11:29.004672Z",
     "start_time": "2022-07-25T20:11:28.300277Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the environment and get the dimensionality of the\n",
    "# observation space and the number of possible actions\n",
    "env = gym.make(env_name) #gym.make(\"CartPole-v0\")\n",
    "observation_dimensions = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "# Initialize the buffer\n",
    "buffer = Buffer(observation_dimensions, steps_per_epoch)\n",
    "\n",
    "# Initialize the actor and the critic as keras models\n",
    "observation_input = keras.Input(shape=(observation_dimensions,), dtype=tf.float32)\n",
    "logits = mlp(observation_input, list(hidden_sizes) + [num_actions], tf.tanh, None)\n",
    "actor = keras.Model(inputs=observation_input, outputs=logits)\n",
    "value = tf.squeeze(\n",
    "    mlp(observation_input, list(hidden_sizes) + [1], tf.tanh, None), axis=1\n",
    ")\n",
    "critic = keras.Model(inputs=observation_input, outputs=value)\n",
    "\n",
    "# Initialize the policy and the value function optimizers\n",
    "policy_optimizer = Adam(learning_rate=policy_learning_rate)\n",
    "value_optimizer = Adam(learning_rate=value_function_learning_rate)\n",
    "\n",
    "# Initialize the observation, episode return and episode length\n",
    "observation, episode_return, episode_length = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf240b8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-25T20:11:28.921Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over the number of epochs\n",
    "scores_ = []\n",
    "episodes_ = 0\n",
    "#.append(score)\n",
    "for epoch in range(epochs):\n",
    "    # Initialize the sum of the returns, lengths and number of episodes for each epoch\n",
    "    sum_return = 0\n",
    "    sum_length = 0\n",
    "    num_episodes = 0\n",
    "\n",
    "    # Iterate over the steps of each epoch\n",
    "    for t in range(steps_per_epoch):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Get the logits, action, and take one step in the environment\n",
    "        observation = observation.reshape(1, -1)\n",
    "        logits, action = sample_action(observation)\n",
    "        observation_new, reward, done, _ = env.step(action[0].numpy())\n",
    "        episode_return += reward\n",
    "        episode_length += 1\n",
    "\n",
    "        # Get the value and log-probability of the action\n",
    "        value_t = critic(observation)\n",
    "        logprobability_t = logprobabilities(logits, action)\n",
    "\n",
    "        # Store obs, act, rew, v_t, logp_pi_t\n",
    "        buffer.store(observation, action, reward, value_t, logprobability_t)\n",
    "\n",
    "        # Update the observation\n",
    "        observation = observation_new\n",
    "\n",
    "        # Finish trajectory if reached to a terminal state\n",
    "        terminal = done\n",
    "        if terminal or (t == steps_per_epoch - 1):\n",
    "            last_value = 0 if done else critic(observation.reshape(1, -1))\n",
    "            buffer.finish_trajectory(last_value)\n",
    "            sum_return += episode_return\n",
    "            sum_length += episode_length\n",
    "            num_episodes += 1\n",
    "            episodes_ +=1\n",
    "            scores_.append(episode_return)\n",
    "            print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(episodes_, '?', episode_return, np.mean(scores_[-50:]), ''))\n",
    "\n",
    "            observation, episode_return, episode_length = env.reset(), 0, 0\n",
    "            \n",
    "            \n",
    "    # Get values from the buffer\n",
    "    (\n",
    "        observation_buffer,\n",
    "        action_buffer,\n",
    "        advantage_buffer,\n",
    "        return_buffer,\n",
    "        logprobability_buffer,\n",
    "    ) = buffer.get()\n",
    "\n",
    "    # Update the policy and implement early stopping using KL divergence\n",
    "    for _ in range(train_policy_iterations):\n",
    "        kl = train_policy(\n",
    "            observation_buffer, action_buffer, logprobability_buffer, advantage_buffer\n",
    "        )\n",
    "        if kl > 1.5 * target_kl:\n",
    "            # Early Stopping\n",
    "            break\n",
    "\n",
    "    # Update the value function\n",
    "    for _ in range(train_value_iterations):\n",
    "        train_value_function(observation_buffer, return_buffer)\n",
    "\n",
    "    # Print mean return and length for each epoch\n",
    "    print()\n",
    "    print(\n",
    "        f\" Epoch: {epoch + 1}. Mean Return: {round(np.mean(scores_[-50:]),3)}. Mean Length: {sum_length / num_episodes}\"\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec6777b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T16:16:34.395698Z",
     "start_time": "2022-07-25T16:16:34.395698Z"
    }
   },
   "outputs": [],
   "source": [
    "xxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8430d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T19:29:13.776734Z",
     "start_time": "2022-07-25T19:29:13.758344Z"
    }
   },
   "outputs": [],
   "source": [
    "sum_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580f168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T19:29:19.557464Z",
     "start_time": "2022-07-25T19:29:19.550469Z"
    }
   },
   "outputs": [],
   "source": [
    "num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744d3703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5634a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3642fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3a599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b12ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429fbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e73e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceceab6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:27:29.614573Z",
     "start_time": "2022-07-25T15:27:29.599571Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, input_shape, n_actions):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer_counter = 0\n",
    "        self.state_memory = np.zeros((self.buffer_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.buffer_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.buffer_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.buffer_size)\n",
    "        self.done_memory = np.zeros(self.buffer_size, dtype=np.bool)\n",
    "\n",
    "    def remember(self, state, action, reward, state_, done):\n",
    "        index = self.buffer_counter % self.buffer_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.done_memory[index] = done\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        max_mem = min(self.buffer_counter, self.buffer_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f5a54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:27:29.629573Z",
     "start_time": "2022-07-25T15:27:29.616573Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def MultiLayerPerceptron(policy=\"mlp\"):\n",
    "    layers = []\n",
    "    if type(policy) == str:\n",
    "        if policy == \"mlp\":\n",
    "            layers.append(Dense(256, activation='relu', name=\"mlp_dense_layer_0\"))\n",
    "            layers.append(Dense(256, activation='relu', name=\"mlp_dense_layer_1\"))\n",
    "    else:\n",
    "        for i,layer in enumerate(policy):\n",
    "            layer._name = 'mlp_custom_layer_{}'.format(i)\n",
    "            layers.append(layer)\n",
    "            \n",
    "    return layers\n",
    "        \n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                policy=\"mlp\",\n",
    "                n_actions=2,\n",
    "                name='critic'\n",
    "        ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.model_name = name\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        self.q = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        X = tf.concat([state, action], axis=1)\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "            \n",
    "        q = self.q(X)\n",
    "        return q\n",
    "\n",
    "class ValueNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 policy=\"mlp\",\n",
    "                 name='value',  \n",
    "        ):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "\n",
    "        self.model_name = name\n",
    "\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        self.v = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        X = state\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "\n",
    "        v = self.v(X)\n",
    "\n",
    "        return v\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, \n",
    "            policy=\"mlp\",\n",
    "            n_actions=2,\n",
    "            max_action=1, \n",
    "            name='actor', \n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.model_name = name\n",
    "        self.max_action = max_action\n",
    "        self.noise = 1e-6\n",
    "\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        \n",
    "        self.mu = Dense(n_actions, activation=None)\n",
    "        self.sigma = Dense(n_actions, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        X = state\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "\n",
    "        mu = self.mu(X)\n",
    "        sigma = self.sigma(X)\n",
    "        sigma = tf.clip_by_value(sigma, self.noise, 1)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_normal(self, state, reparameterize=True):\n",
    "        mu, sigma = self.call(state)\n",
    "        probabilities = tfp.distributions.Normal(mu, sigma)\n",
    "\n",
    "        if reparameterize:\n",
    "            actions = probabilities.sample() # + something else if you want to implement\n",
    "        else:\n",
    "            actions = probabilities.sample()\n",
    "\n",
    "        action = tf.math.tanh(actions)*self.max_action\n",
    "        log_probs = probabilities.log_prob(actions)\n",
    "        log_probs -= tf.math.log(1-tf.math.pow(action,2)+self.noise)\n",
    "        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)\n",
    "\n",
    "        return action, log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49770e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:27:29.660570Z",
     "start_time": "2022-07-25T15:27:29.630571Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.agent import Agent\n",
    "\n",
    "\n",
    "class SoftActorCriticAgent(Agent):\n",
    "    def __init__(self, \n",
    "            environment,\n",
    "            alpha=0.0003, \n",
    "            beta=0.0003, \n",
    "            gamma=0.99, \n",
    "            tau=0.005,\n",
    "            buffer_size=1000000, \n",
    "            policy=\"mlp\", \n",
    "            batch_size=256, \n",
    "            reward_scale=2, \n",
    "            loss_function = keras.losses.MSE, #keras.losses.Huber()\n",
    "    ):\n",
    "        super(SoftActorCriticAgent, self).__init__(environment,loss_keys=[\"actor\",\"value\",\"critic_1\",\"critic_2\"])\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.policy = policy\n",
    "        self.reward_scale = reward_scale\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.observation_shape, self.n_actions)\n",
    "            \n",
    "    def __init_networks(self):\n",
    "        self.actor = ActorNetwork(n_actions=self.n_actions,policy=self.policy, max_action=self.env.action_space.high)\n",
    "        self.critic_1 = CriticNetwork(n_actions=self.n_actions,policy=self.policy, name='critic_1')\n",
    "        self.critic_2 = CriticNetwork(n_actions=self.n_actions,policy=self.policy, name='critic_2')\n",
    "        self.value = ValueNetwork(name='value',policy=self.policy)\n",
    "        self.target_value = ValueNetwork(name='target_value',policy=self.policy)\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=self.alpha))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.critic_2.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.value.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.target_value.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.buffer.remember(state, action, reward, new_state, done)      \n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_value.weights\n",
    "        for i, weight in enumerate(self.value.weights):\n",
    "            weights.append(weight * tau + targets[i]*(1-tau))\n",
    "\n",
    "        self.target_value.set_weights(weights)\n",
    "        \n",
    "    def replay(self):\n",
    "        if self.buffer.buffer_counter < self.batch_size:\n",
    "            return\n",
    "    \n",
    "        state,action, reward, state_, done = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(state_, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "        \n",
    "        # Value network update\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = tf.squeeze(self.value(states),1)\n",
    "            value_= tf.squeeze(self.target_value(states_),1)\n",
    "            \n",
    "            current_policy_actions, log_probs = self.actor.sample_normal(states)\n",
    "            log_probs = tf.squeeze(log_probs,1)\n",
    "            \n",
    "            q1_new_policy = self.critic_1(states,current_policy_actions)\n",
    "            q2_new_policy = self.critic_2(states,current_policy_actions)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy,q2_new_policy))\n",
    "            \n",
    "            value_target = critic_value - log_probs\n",
    "            value_loss = 0.5 *self.loss_function(value,value_target)\n",
    "            \n",
    "            \n",
    "        value_network_gradient = tape.gradient(value_loss,self.value.trainable_variables)\n",
    "        self.value.optimizer.apply_gradients(zip(value_network_gradient, self.value.trainable_variables))\n",
    "        \n",
    "        # Actor network update\n",
    "        with tf.GradientTape() as tape:\n",
    "            # in the original paper, they reparameterize here. \n",
    "            new_policy_actions, log_probs = self.actor.sample_normal(states,reparameterize=True)\n",
    "            \n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            q1_new_policy = self.critic_1(states, new_policy_actions)\n",
    "            q2_new_policy = self.critic_2(states, new_policy_actions)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(\n",
    "                                        q1_new_policy, q2_new_policy), 1)\n",
    "        \n",
    "            actor_loss = log_probs - critic_value\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_network_gradient, self.actor.trainable_variables))\n",
    "\n",
    "        # Critic network update\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            \n",
    "            q_hat = self.reward_scale*reward + self.gamma*value_*(1-done)\n",
    "            q1_old_policy = tf.squeeze(self.critic_1(state, action), 1)\n",
    "            q2_old_policy = tf.squeeze(self.critic_2(state, action), 1)\n",
    "            critic_1_loss = 0.5 * self.loss_function(q1_old_policy, q_hat)\n",
    "            critic_2_loss = 0.5 * self.loss_function(q2_old_policy, q_hat)\n",
    "    \n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss,self.critic_1.trainable_variables)\n",
    "        critic_2_network_gradient = tape.gradient(critic_2_loss,self.critic_2.trainable_variables)\n",
    "\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(critic_2_network_gradient, self.critic_2.trainable_variables))\n",
    "\n",
    "        self.learning_log.step_loss({\n",
    "            \"actor\":actor_loss.numpy(),\n",
    "            \"value\":value_loss.numpy(),\n",
    "            \"critic_1\":critic_1_loss.numpy(),\n",
    "            \"critic_2\":critic_2_loss.numpy()\n",
    "        })\n",
    "        \n",
    "        self.update_network_parameters()\n",
    "        \n",
    "    def test(self, episodes=10, render=True, init_environment=False):\n",
    "        for episode in range(episodes):\n",
    "            try:\n",
    "                state = self.env.reset()\n",
    "            except:\n",
    "                self._Agent__init_environment()\n",
    "                state = self.env.reset()\n",
    "                \n",
    "            done = False\n",
    "            score = 0\n",
    "            \n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                # Sample action, probs and critic\n",
    "                action = self.choose_action(state)\n",
    "\n",
    "                # Step\n",
    "                state,reward,done, info = self.env.step(action)\n",
    "\n",
    "                # Get next state\n",
    "                score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.close()\n",
    "\n",
    "            self.learning_log.episode_test_log(score,episode)\n",
    "\n",
    "            \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50,):\n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    " \n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        \n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                state_, reward, done, info = self.env.step(action)\n",
    "                score += reward\n",
    "                self.remember(state, action, reward, state_, done)\n",
    "                self.replay()\n",
    "                state = state_\n",
    "            \n",
    "            self.running_reward.step(score)\n",
    "             # Log details\n",
    "            episode += 1\n",
    "            \n",
    "            self.learning_log.episode(\n",
    "                log_each_n_episodes,\n",
    "                score,\n",
    "                self.running_reward.reward, \n",
    "                log_level=log_level\n",
    "            )\n",
    "           \n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                break\n",
    "\n",
    "        if plot_results:\n",
    "            self.plot_learning_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21f6fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:33:21.780337Z",
     "start_time": "2022-07-25T15:27:29.661572Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "\n",
    "agent= SoftActorCriticAgent(environment)\n",
    "agent.learn(log_each_n_episodes=10, success_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d437e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T15:36:09.995395Z",
     "start_time": "2022-07-25T15:35:59.220126Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "agent.env = environment()\n",
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f596a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
