{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493ca506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:02:15.412260Z",
     "start_time": "2022-07-21T21:02:15.393829Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b54956",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:02:19.169437Z",
     "start_time": "2022-07-21T21:02:15.872601Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "from src.utils.gym_environment import GymEnvironment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f71b6a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:18:12.848870Z",
     "start_time": "2022-07-21T18:18:12.817911Z"
    }
   },
   "outputs": [],
   "source": [
    "#sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "#sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "#norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment, \n",
    "                alpha = 0.1,\n",
    "                gamma = 0.99,\n",
    "                eps = np.finfo(np.float32).eps.item(),\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                ):\n",
    "        \n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Environment\n",
    "        env = GymEnvironment(environment)\n",
    "        self.env = env.env\n",
    "        self.n_actions = env.n_actions\n",
    "        self.actions = env.actions\n",
    "        self.observation_shape = env.observation_shape\n",
    "        \n",
    "        self.__init_networks()\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        num_inputs = self.observation_shape[0]\n",
    "        num_actions = self.n_actions\n",
    "        num_hidden = 128\n",
    "\n",
    "        inputs = layers.Input(shape=(num_inputs,))\n",
    "        common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "        action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "        critic = layers.Dense(1)(common)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "    \n",
    "    def choose_action(self,action_probs):\n",
    "        \n",
    "        # Sample action from action probability distribution\n",
    "        action = np.random.choice(self.n_actions, p=np.squeeze(action_probs))\n",
    "        action_log_prob = tf.math.log(action_probs[0, action])\n",
    "\n",
    "        return action, action_log_prob\n",
    "    \n",
    "    def learn(self):\n",
    "        huber_loss = keras.losses.Huber()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        running_reward = 0\n",
    "        episode_count = 0\n",
    "        \n",
    "        while True:  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                for timestep in range(1, 1000):\n",
    "                    # env.render(); Adding this line would show the attempts\n",
    "                    # of the agent in a pop up window.\n",
    "\n",
    "                    state = tf.convert_to_tensor(state)\n",
    "                    state = tf.expand_dims(state, 0)\n",
    "\n",
    "                    # Predict action probabilities and estimated future rewards\n",
    "                    # from environment state\n",
    "                    action_probs, critic_value = self.model(state)\n",
    "                    critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "                    # Sample action from action probability distribution\n",
    "                    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "                    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "                    action, action_log_prob = self.choose_action(action_probs)\n",
    "\n",
    "                    action_probs_history.append(action_log_prob)\n",
    "                    \n",
    "                    #print({\"action\":action, \"action_log_prob\":action_log_prob})\n",
    "                    \n",
    "                    # Apply the sampled action in our environment\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    rewards_history.append(reward)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Update running reward to check condition for solving\n",
    "                running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "                # Calculate expected value from rewards\n",
    "                # - At each timestep what was the total reward received after that timestep\n",
    "                # - Rewards in the past are discounted by multiplying them with gamma\n",
    "                # - These are the labels for our critic\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                for r in rewards_history[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # Normalize\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                # Calculating loss values to update our network\n",
    "                history = zip(action_probs_history, critic_value_history, returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                for log_prob, value, ret in history:\n",
    "                    # At this point in history, the critic estimated that we would get a\n",
    "                    # total reward = `value` in the future. We took an action with log probability\n",
    "                    # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                    # The actor must be updated so that it predicts an action that leads to\n",
    "                    # high rewards (compared to critic's estimate) with high probability.\n",
    "                    diff = ret - value\n",
    "                    actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                    # The critic must be updated so that it predicts a better estimate of\n",
    "                    # the future rewards.\n",
    "                    critic_losses.append(\n",
    "                        huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "                # Backpropagation\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # Clear the loss and reward history\n",
    "                action_probs_history.clear()\n",
    "                critic_value_history.clear()\n",
    "                rewards_history.clear()\n",
    "\n",
    "            # Log details\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                template = \"running reward: {:.2f} at episode {}\"\n",
    "                print(template.format(running_reward, episode_count))\n",
    "\n",
    "            if running_reward > 195:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4db583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:11:52.114012Z",
     "start_time": "2022-07-21T18:07:45.760504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| CartPole-v1\n",
      "| Action space:\n",
      "|   * Discrete with high state-space\n",
      "| Dev notes:\n",
      "|   * Agents that track State/Action combinations like \n",
      "|     Q learning will fail due to high state space\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "running reward: 6.78 at episode 10\n",
      "running reward: 12.80 at episode 20\n",
      "running reward: 28.42 at episode 30\n",
      "running reward: 80.46 at episode 40\n",
      "running reward: 94.96 at episode 50\n",
      "running reward: 73.45 at episode 60\n",
      "running reward: 64.85 at episode 70\n",
      "running reward: 67.75 at episode 80\n",
      "running reward: 91.22 at episode 90\n",
      "running reward: 151.50 at episode 100\n",
      "running reward: 135.05 at episode 110\n",
      "running reward: 125.50 at episode 120\n",
      "running reward: 116.25 at episode 130\n",
      "running reward: 108.29 at episode 140\n",
      "running reward: 113.86 at episode 150\n",
      "running reward: 122.69 at episode 160\n",
      "running reward: 144.65 at episode 170\n",
      "running reward: 169.91 at episode 180\n",
      "running reward: 170.67 at episode 190\n",
      "running reward: 172.88 at episode 200\n",
      "running reward: 171.13 at episode 210\n",
      "running reward: 160.29 at episode 220\n",
      "running reward: 158.85 at episode 230\n",
      "running reward: 169.16 at episode 240\n",
      "Solved at episode 248!\n"
     ]
    }
   ],
   "source": [
    "from src.environments.discrete.cartpole import environment\n",
    "agent = Agent(environment)\n",
    "agent.learn()\n",
    "#{'action': 0, 'action_log_prob': <tf.Tensor: shape=(), dtype=float32, numpy=-0.6900733>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146ec6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42953654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T21:02:21.401383Z",
     "start_time": "2022-07-21T21:02:21.371826Z"
    }
   },
   "outputs": [],
   "source": [
    "#sigma = tf.layers.dense(hidden2, n_outputs, None, init_xavier)\n",
    "#sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "#norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,\n",
    "                 environment, \n",
    "                alpha = 0.1,\n",
    "                gamma = 0.99,\n",
    "                eps = np.finfo(np.float32).eps.item(),\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "                ):\n",
    "        \n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.eps = eps\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Environment\n",
    "        env = GymEnvironment(environment)\n",
    "        self.env = env.env\n",
    "        self.n_actions = env.n_actions\n",
    "        self.actions = env.actions\n",
    "        self.observation_shape = env.observation_shape\n",
    "        \n",
    "        self.__init_networks()\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        num_inputs = self.observation_shape[0]\n",
    "        num_actions = self.n_actions\n",
    "        num_hidden = 128\n",
    "\n",
    "        inputs = layers.Input(shape=(num_inputs,))\n",
    "        common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "        sigma = layers.Dense(1, activation=\"softplus\", name=\"sigma\")(common)\n",
    "        mu = layers.Dense(1, activation=\"tanh\" , name='mu')(common)\n",
    "        #sigma = tf.nn.softplus(sigma) + 1e-5\n",
    "        #norm_dist = tf.contrib.distributions.Normal(mu, sigma)\n",
    "        #action = layers.Dense(num_actions, activation=\"softmax\")(common)\n",
    "        critic = layers.Dense(1, activation=None ,name='critic')(common)\n",
    "        \n",
    "        actor = tf.keras.layers.Concatenate(axis=-1)([mu,sigma])\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[actor, critic])\n",
    "\n",
    "    \n",
    "    def choose_action(self,mu,sigma):\n",
    "        \n",
    "        # Sample action from action probability distribution\n",
    "        #action = np.random.choice(self.n_actions, p=np.squeeze(action_probs))\n",
    "        #action_log_prob = tf.math.log(action_probs[0, action])\n",
    "        #mu = tf.math.tanh(mu)\n",
    "        norm_dist = tfp.distributions.Normal(mu, sigma)\n",
    "        action = tf.squeeze(norm_dist.sample(1), axis=0)\n",
    "        action_log_prob = -(norm_dist.log_prob(action)+self.eps)\n",
    "        action = tf.clip_by_value(\n",
    "            action, self.env.action_space.low[0], \n",
    "            self.env.action_space.high[0])\n",
    "        \n",
    "        return np.array(action[0],dtype=np.float32), action_log_prob\n",
    "    \n",
    "    def learn(self):\n",
    "        huber_loss = keras.losses.Huber()\n",
    "        action_probs_history = []\n",
    "        critic_value_history = []\n",
    "        rewards_history = []\n",
    "        running_reward = 0\n",
    "        episode_count = 0\n",
    "        \n",
    "        while True:  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            with tf.GradientTape() as tape:\n",
    "                for timestep in range(1, 1000):\n",
    "                    # env.render(); Adding this line would show the attempts\n",
    "                    # of the agent in a pop up window.\n",
    "\n",
    "                    state = tf.convert_to_tensor(state)\n",
    "                    state = tf.expand_dims(state, 0)\n",
    "\n",
    "                    # Predict action probabilities and estimated future rewards\n",
    "                    # from environment state\n",
    "                    actor_value, critic_value = self.model(state)\n",
    "                    critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "                    #print('teste',tfp.distributions.Normal(actor_value))\n",
    "                    mu = actor_value[:,0:1]\n",
    "                    sigma = actor_value[:,1:]\n",
    "                    # Sample action from action probability distribution\n",
    "                    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "                    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "                    action, action_log_prob = self.choose_action(mu,sigma)\n",
    "\n",
    "                    action_probs_history.append(action_log_prob)\n",
    "                    #action_probs_history.append(actor_value)\n",
    "                    \n",
    "                    #print({\"action\":action, \"action_log_prob\":action_log_prob})\n",
    "                    \n",
    "                    # Apply the sampled action in our environment\n",
    "                    \n",
    "                    #print(a)\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    rewards_history.append(reward)\n",
    "                    episode_reward += reward\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                # Update running reward to check condition for solving\n",
    "                running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "                # Calculate expected value from rewards\n",
    "                # - At each timestep what was the total reward received after that timestep\n",
    "                # - Rewards in the past are discounted by multiplying them with gamma\n",
    "                # - These are the labels for our critic\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                for r in rewards_history[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # Normalize\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                # Calculating loss values to update our network\n",
    "                history = zip(action_probs_history, critic_value_history, returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                for log_prob, value, ret in history:\n",
    "                    # At this point in history, the critic estimated that we would get a\n",
    "                    # total reward = `value` in the future. We took an action with log probability\n",
    "                    # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                    # The actor must be updated so that it predicts an action that leads to\n",
    "                    # high rewards (compared to critic's estimate) with high probability.\n",
    "                    diff = ret - value\n",
    "                    actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                    # The critic must be updated so that it predicts a better estimate of\n",
    "                    # the future rewards.\n",
    "                    critic_losses.append(\n",
    "                        huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "                # Backpropagation\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # Clear the loss and reward history\n",
    "                action_probs_history.clear()\n",
    "                critic_value_history.clear()\n",
    "                rewards_history.clear()\n",
    "\n",
    "            # Log details\n",
    "            episode_count += 1\n",
    "            if episode_count % 10 == 0:\n",
    "                template = \"running reward: {:.2f} at episode {}\"\n",
    "                print(template.format(running_reward, episode_count))\n",
    "\n",
    "            if running_reward > 0:  # Condition to consider the task solved\n",
    "                print(\"Solved at episode {}!\".format(episode_count))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339f7a7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-21T21:02:22.392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| MountainCarContinuous-v0\n",
      "| Action space:\n",
      "|   * Continuous with low state-space\n",
      "| Dev notes:\n",
      "|   * Switched _max_episode_steps from 200 to 1000 so \n",
      "|     the agent can explore better.\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.mountain_car import environment\n",
    "\n",
    "\n",
    "agent = Agent(environment)\n",
    "agent.learn()\n",
    "\n",
    "#{'action': array([0.6679693], dtype=float32), 'action_log_prob': array([1.0068668], dtype=float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582a67f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-21T21:02:25.063Z"
    }
   },
   "outputs": [],
   "source": [
    "state = agent.env.reset()\n",
    "#state = agent.get_state(obs)\n",
    "done = False\n",
    "score = 0\n",
    "\n",
    "while not done:\n",
    "    agent.env.render()\n",
    "    actor_value, critic_value = agent.model(np.expand_dims(state,axis=0))\n",
    "    mu = actor_value[:,0:1]\n",
    "    sigma = actor_value[:,1:]\n",
    "    # Sample action from action probability distribution\n",
    "    #action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "    #action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "    action, action_log_prob = agent.choose_action(mu,sigma)\n",
    "    #action = mu#agent.max_action(state)\n",
    "            \n",
    "    # Step\n",
    "    obs_,reward,done, info = agent.env.step(action)\n",
    "            \n",
    "    # Get next state\n",
    "    score += reward\n",
    "    #state_ = agent.get_state(obs_)\n",
    "    state = obs_\n",
    "    # Set state as next state so the agent keeps \n",
    "    #state = state_\n",
    "    \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cace13",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-21T21:02:27.783Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "agent.env.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be6b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
