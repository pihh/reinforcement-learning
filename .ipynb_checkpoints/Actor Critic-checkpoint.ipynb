{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:33:51.038136Z",
     "start_time": "2022-07-22T09:33:51.017183Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:33:55.233824Z",
     "start_time": "2022-07-22T09:33:51.514779Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e8743d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:38:08.237346Z",
     "start_time": "2022-07-22T09:38:08.212752Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActorCriticAgent(Agent):\n",
    "    def __init__(self, \n",
    "                environment, \n",
    "                alpha = 0.01,\n",
    "                gamma = 0.99,\n",
    "                eps = np.finfo(np.float32).eps.item(),\n",
    "                optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "                critic_loss= tf.keras.losses.Huber()):\n",
    "        \n",
    "        super(ActorCriticAgent, self).__init__(environment)\n",
    "        \n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma \n",
    "        self.eps = eps \n",
    "        self.optimizer=optimizer\n",
    "        self.critic_loss = critic_loss\n",
    "        #type(tf.keras.optimizers.Adam(learning_rate=0.01)).__name__\n",
    "\n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = Buffer(['action_log_probs','critic_values','rewards'])\n",
    "            \n",
    "    def __init_networks(self):\n",
    "        num_inputs = self.observation_shape[0]\n",
    "        num_hidden = 128\n",
    "\n",
    "        inputs = Input(shape=(num_inputs,),name=\"actor_critic_inputs\")\n",
    "        common_layer = Dense(num_hidden, activation=\"relu\", name=\"actor_critic_common_layer\")(inputs)\n",
    "        \n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            action = Dense(self.n_actions, activation=\"softmax\")(common_layer)\n",
    "        elif self.action_space_mode == \"continuous\":\n",
    "            mu_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "            mu = Dense(self.n_actions, activation=\"tanh\" , name='mu',kernel_initializer=mu_init)(common_layer)\n",
    "            \n",
    "            sigma_init = tf.random_uniform_initializer(minval=self.eps, maxval=0.2)\n",
    "            sigma = Dense(self.n_actions, activation=\"softplus\", name=\"sigma\", kernel_initializer=sigma_init)(common_layer)\n",
    "            \n",
    "            action = Concatenate(axis=-1, name=\"actor_output\")([mu,sigma]) * self.action_upper_bounds\n",
    "\n",
    "        critic = Dense(1)(common_layer)\n",
    "\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "\n",
    "    def choose_action(self, state, deterministic=True):\n",
    "        action_probs, critic_value = self.model(state)\n",
    "        \n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            # DISCRETE SAMPLING\n",
    "            if deterministic:\n",
    "                action = np.argmax(np.squeeze(action_probs))\n",
    "                action_log_prob = action\n",
    "            else:\n",
    "                # Sample action from action probability distribution\n",
    "                action = np.random.choice(self.n_actions, p=np.squeeze(action_probs))\n",
    "                action_log_prob = tf.math.log(action_probs[0, action])\n",
    "\n",
    "\n",
    "        elif self.action_space_mode == \"continuous\":\n",
    "            # CONTINUOUS SAMPLING\n",
    "            mu = action_probs[:,0:self.n_actions]\n",
    "            sigma = action_probs[:,self.n_actions:]\n",
    "            \n",
    "            if deterministic:\n",
    "                action = mu\n",
    "                action_log_prob = action\n",
    "            else:\n",
    "                norm_dist = tfp.distributions.Normal(mu, sigma)\n",
    "                action = tf.squeeze(norm_dist.sample(self.n_actions), axis=0)\n",
    "                action_log_prob = -(norm_dist.log_prob(action)+self.eps)\n",
    "                action = tf.clip_by_value(\n",
    "                    action, self.env.action_space.low[0], \n",
    "                    self.env.action_space.high[0])\n",
    "\n",
    "                action = np.array(action[0],dtype=np.float32)\n",
    "        \n",
    "        return action, action_log_prob , critic_value\n",
    "    \n",
    "    def test(self, episodes=10, render=True):\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                state = tf.convert_to_tensor(state)\n",
    "                state = tf.expand_dims(state, 0)\n",
    "                \n",
    "                # Sample action, probs and critic\n",
    "                action, action_log_prob, critic_value = self.choose_action(state)\n",
    "\n",
    "                # Step\n",
    "                state,reward,done, info = self.env.step(action)\n",
    "\n",
    "                # Get next state\n",
    "                score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.close()\n",
    "\n",
    "            print(\"Test episode: {}, score: {:.2f}\".format(episode,score))\n",
    "    \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, log_each_n_episodes=100, success_threshold=False):\n",
    "        \n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    " \n",
    "        self.buffer.reset()\n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        \n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            with tf.GradientTape() as tape:\n",
    "                while not done:\n",
    "\n",
    "                    state = tf.convert_to_tensor(state)\n",
    "                    state = tf.expand_dims(state, 0)\n",
    "\n",
    "                    # Predict action probabilities and estimated future rewards\n",
    "                    # from environment state\n",
    "                    action, action_log_prob, critic_value = self.choose_action(state, deterministic=False)\n",
    "\n",
    "                    self.buffer.store('critic_values',critic_value[0, 0])\n",
    "\n",
    "                    # Sample action from action probability distribution\n",
    "                    self.buffer.store('action_log_probs',action_log_prob)\n",
    "\n",
    "                    # Apply the sampled action in our environment\n",
    "                    state, reward, done, _ = self.env.step(action)\n",
    "                    self.buffer.store('rewards',reward)\n",
    "\n",
    "                    score += reward\n",
    "                    timestep+=1\n",
    "                # Update running reward to check condition for solving\n",
    "                self.running_reward.step(score)\n",
    "\n",
    "                # Time discounted rewards\n",
    "                returns = []\n",
    "                discounted_sum = 0\n",
    "                \n",
    "                for r in self.buffer.get('rewards')[::-1]:\n",
    "                    discounted_sum = r + self.gamma * discounted_sum\n",
    "                    returns.insert(0, discounted_sum)\n",
    "\n",
    "                # Normalize\n",
    "                returns = np.array(returns)\n",
    "                returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "                returns = returns.tolist()\n",
    "\n",
    "                # Calculating loss values to update our network\n",
    "                history = zip(self.buffer.get('action_log_probs'), self.buffer.get('critic_values'), returns)\n",
    "                actor_losses = []\n",
    "                critic_losses = []\n",
    "                for log_prob, value, ret in history:\n",
    "                    # At this point in history, the critic estimated that we would get a\n",
    "                    # total reward = `value` in the future. We took an action with log probability\n",
    "                    # of `log_prob` and ended up recieving a total reward = `ret`.\n",
    "                    # The actor must be updated so that it predicts an action that leads to\n",
    "                    # high rewards (compared to critic's estimate) with high probability.\n",
    "                    diff = ret - value\n",
    "                    actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "                    # The critic must be updated so that it predicts a better estimate of\n",
    "                    # the future rewards.\n",
    "                    critic_losses.append(\n",
    "                        self.critic_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "                    )\n",
    "\n",
    "                # Backpropagation\n",
    "                loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "                #print(loss_value)\n",
    "                grads = tape.gradient(loss_value, self.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "                # Clear the loss and reward history\n",
    "                self.buffer.reset()\n",
    "\n",
    "            # Log details\n",
    "            episode += 1\n",
    "            if episode % log_each_n_episodes == 0 and episode > 0:\n",
    "                print('episode {}, running reward: {:.2f}, last reward: {:.2f}'.format(episode,self.running_reward.reward, score))\n",
    "\n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                    break\n",
    "\n",
    "        if plot_results:\n",
    "            self.plot_learning_results()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62df8d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:13:15.069892Z",
     "start_time": "2022-07-22T09:12:12.772446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| CartPole-v1\n",
      "| Action space: Discrete with high state-space\n",
      "| Environment beated threshold: 200\n",
      "| Dev notes:\n",
      "|   * Agents that track State/Action combinations like \n",
      "|     Q learning will fail due to high state space\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "episode 10, running reward: 9.60\n",
      "episode 20, running reward: 10.78\n",
      "episode 30, running reward: 14.09\n",
      "episode 40, running reward: 13.12\n",
      "episode 50, running reward: 12.54\n",
      "episode 60, running reward: 16.22\n",
      "episode 70, running reward: 22.04\n",
      "episode 80, running reward: 40.72\n",
      "episode 90, running reward: 84.90\n",
      "episode 100, running reward: 114.00\n",
      "Agent solved environment at the episode 109\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.environments.discrete.cartpole import environment\n",
    "agent_discrete = ActorCriticAgent(environment)\n",
    "agent_discrete.learn(log_each_n_episodes=10)\n",
    "\n",
    "#{'action': 0, 'action_log_prob': <tf.Tensor: shape=(), dtype=float32, numpy=-0.6900733>}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5042097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:08:51.925079Z",
     "start_time": "2022-07-22T09:08:45.284415Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test episode: 0, score: 500.00\n",
      "Test episode: 1, score: 500.00\n",
      "Test episode: 2, score: 500.00\n",
      "Test episode: 3, score: 500.00\n",
      "Test episode: 4, score: 500.00\n",
      "Test episode: 5, score: 500.00\n",
      "Test episode: 6, score: 500.00\n",
      "Test episode: 7, score: 500.00\n",
      "Test episode: 8, score: 500.00\n",
      "Test episode: 9, score: 500.00\n"
     ]
    }
   ],
   "source": [
    "agent_discrete.test(render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cd3cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0db7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c3c599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:51:26.886696Z",
     "start_time": "2022-07-22T09:50:51.300250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| MountainCarContinuous-v0\n",
      "| \n",
      "| Action space: Continuous with low state-space\n",
      "| Environment beated threshold: -150\n",
      "| Dev notes:\n",
      "|   * Switched _max_episode_steps from 200 to 1000 so \n",
      "|     the agent can explore better.\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "episode 1, running reward: -49.84, last reward: -46.84\n",
      "episode 2, running reward: -49.61, last reward: -45.17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m agent_continuous \u001b[38;5;241m=\u001b[39m ActorCriticAgent(environment,optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m))\n\u001b[0;32m      4\u001b[0m agent_continuous\u001b[38;5;241m.\u001b[39mrunning_reward\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43magent_continuous\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_each_n_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mActorCriticAgent.learn\u001b[1;34m(self, timesteps, plot_results, reset, log_each_n_episodes, success_threshold)\u001b[0m\n\u001b[0;32m    171\u001b[0m     actor_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m-\u001b[39mlog_prob \u001b[38;5;241m*\u001b[39m diff)  \u001b[38;5;66;03m# actor loss\u001b[39;00m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# The critic must be updated so that it predicts a better estimate of\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# the future rewards.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     critic_losses\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 176\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m    180\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(actor_losses) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28msum\u001b[39m(critic_losses)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\losses.py:141\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m   call_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx())\n\u001b[1;32m--> 141\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses_utils\u001b[38;5;241m.\u001b[39mcompute_weighted_loss(\n\u001b[0;32m    143\u001b[0m     losses, sample_weight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reduction())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\losses.py:245\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    242\u001b[0m   y_pred, y_true \u001b[38;5;241m=\u001b[39m losses_utils\u001b[38;5;241m.\u001b[39msqueeze_or_expand_dimensions(y_pred, y_true)\n\u001b[0;32m    244\u001b[0m ag_fn \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mtf_convert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mcontrol_status_ctx())\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ag_fn(y_true, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fn_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\losses.py:1696\u001b[0m, in \u001b[0;36mhuber\u001b[1;34m(y_true, y_pred, delta)\u001b[0m\n\u001b[0;32m   1694\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(y_true, dtype\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mfloatx())\n\u001b[0;32m   1695\u001b[0m delta \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(delta, dtype\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mfloatx())\n\u001b[1;32m-> 1696\u001b[0m error \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1697\u001b[0m abs_error \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mabs(error)\n\u001b[0;32m   1698\u001b[0m half \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;241m0.5\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mabs_error\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\math_ops.py:548\u001b[0m, in \u001b[0;36msubtract\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.subtract\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubtract\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39mregister_binary_elementwise_api\n\u001b[0;32m    546\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubtract\u001b[39m(x, y, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 548\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:11125\u001b[0m, in \u001b[0;36msub\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m  11123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m  11124\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m> 11125\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  11126\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSub\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m  11127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m  11128\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.mountain_car import environment\n",
    "\n",
    "agent_continuous = ActorCriticAgent(environment,optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "agent_continuous.learn(log_each_n_episodes=1)\n",
    "\n",
    "#{'action': array([0.6679693], dtype=float32), 'action_log_prob': array([1.0068668], dtype=float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256f9259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:05:15.264295Z",
     "start_time": "2022-07-22T09:05:15.264295Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_continuous.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb4dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
