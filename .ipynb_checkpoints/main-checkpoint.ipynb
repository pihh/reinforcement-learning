{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f64d680c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T15:59:23.530560Z",
     "start_time": "2022-07-21T15:59:23.503565Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c3dac7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T15:59:27.482486Z",
     "start_time": "2022-07-21T15:59:27.191731Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from src.utils.gym_environment import GymEnvironment\n",
    "\n",
    "def environment():\n",
    "    return gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72b829a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T15:59:31.209179Z",
     "start_time": "2022-07-21T15:59:28.566785Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Handles storage of state/action/reward and transitions\n",
    "class ReplayBuffer():\n",
    "    def __init__(\n",
    "            self,\n",
    "            environment,\n",
    "            buffer_size,\n",
    "            buffers=['state','new_state',\"action\",\"reward\", \"done\"]\n",
    "    ):\n",
    "        \n",
    "        self.buffer_position = 0\n",
    "        self.buffer_size = buffer_size\n",
    "        self.discrete = environment.action_space_mode == 'Discrete'\n",
    "        \n",
    "        self.__init_memory(environment)\n",
    "        # if discrete, the action space must be a one-hot encoded \n",
    "        \n",
    "    def __init_memory(self,environment):\n",
    "        dtype = np.int8 if self.discrete else np.float32\n",
    "        \n",
    "        if len(environment.observation_shape) ==1:\n",
    "            input_shape = environment.observation_shape[0]\n",
    "        else:\n",
    "            input_shape = environment.observation_shape[0]*environment.observation_shape[1]\n",
    "            \n",
    "        self.states = np.zeros((self.buffer_size,input_shape ))\n",
    "        self.new_states = np.zeros((self.buffer_size,input_shape ))    \n",
    "        self.actions = np.zeros((self.buffer_size, environment.n_actions), dtype=dtype)\n",
    "        self.rewards = np.zeros(self.buffer_size)\n",
    "        self.dones = np.zeros(self.buffer_size, dtype=np.float32)\n",
    "        \n",
    "    def remember(self,state,action,reward,state_, done):\n",
    "        # once we hit buffer_size entries, we want to add it to the beginning \n",
    "        index = self.buffer_position % self.buffer_size\n",
    "        \n",
    "        self.states[index] = state\n",
    "        self.new_states[index] = state_\n",
    "        self.rewards[index] = reward\n",
    "        self.dones[index] = 1- int(done)\n",
    "        \n",
    "        # When discrete store one hot\n",
    "        if self.discrete:\n",
    "            actions = np.zeros(self.actions.shape[1])\n",
    "            actions[action] = 1.0 \n",
    "            self.actions[index] = actions\n",
    "        else:\n",
    "            self.actions[index] = action\n",
    "        \n",
    "        self.buffer_position +=1\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        max_position = min(self.buffer_position, self.buffer_size)\n",
    "        batch = np.random.choice(max_position, batch_size)\n",
    "        \n",
    "        states = self.states[batch]\n",
    "        states_ = self.new_states[batch]\n",
    "        rewards = self.rewards[batch]\n",
    "        dones = self.dones[batch]\n",
    "        actions = self.actions[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9d36dda0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:18:38.013359Z",
     "start_time": "2022-07-21T16:18:37.984701Z"
    }
   },
   "outputs": [],
   "source": [
    "class DqnAgent():\n",
    "    def __init__(\n",
    "        self,\n",
    "        environment,\n",
    "        alpha = 0.0005,\n",
    "        gamma = 0.99,\n",
    "        epsilon = 1.0,\n",
    "        epsilon_decay=0.9996,\n",
    "        \n",
    "        buffer_size=1000000,\n",
    "        batch_size=64,\n",
    "        optimizer = Adam(learning_rate=0.0005),\n",
    "        fully_connected_layer_configuration = 'MlpPolicy',\n",
    "    ):\n",
    "        # Args\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Environment\n",
    "        env = GymEnvironment(environment)\n",
    "        self.env = env.env\n",
    "        self.n_actions = env.n_actions\n",
    "        self.actions = env.actions\n",
    "        self.observation_shape = env.observation_shape\n",
    "        self.action_space_mode = env.action_space_mode\n",
    "    \n",
    "        # Boot\n",
    "        self.__init_model() \n",
    "        self.__init_memory(env)\n",
    "        \n",
    "    def __init_model(self):\n",
    "        model = Sequential([\n",
    "            Dense(256, input_shape=self.observation_shape, activation=\"relu\"),\n",
    "            Dense(256, activation=\"relu\"),\n",
    "            Dense(self.n_actions)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=self.optimizer, loss=\"mse\")\n",
    "        \n",
    "        #self.model = model\n",
    "        self.q_eval = model\n",
    "        \n",
    "    def __init_memory(self,environment):\n",
    "        self.memory = ReplayBuffer(environment,self.buffer_size)\n",
    "        \n",
    "    def decrement_eps(self):\n",
    "        self.epsilon = self.epsilon * self.epsilon_decay if self.epsilon > 0.01 else 0.01\n",
    "        \n",
    "    def remember(self,state,action,reward,new_state,done):\n",
    "        self.memory.remember(state,action,reward,new_state,done)\n",
    "        \n",
    "    def get_state(self,obs):\n",
    "        return obs\n",
    "        \n",
    "    def choose_action(self,state):\n",
    "        state = state[np.newaxis,:]\n",
    "        rand = np.random.random()\n",
    "        if rand < self.epsilon:\n",
    "            if self.action_space_mode == \"Discrete\":\n",
    "                action = np.random.choice(self.actions)\n",
    "            else:\n",
    "                action = agent.env.action_space.sample()\n",
    "        else:\n",
    "            if self.action_space_mode == \"Discrete\":\n",
    "                actions = self.q_eval.predict(state)\n",
    "                action = np.argmax(actions)\n",
    "            else:\n",
    "                actions = self.q_eval.predict(state)\n",
    "                action = action\n",
    "            \n",
    "        return action\n",
    "    def replay(self):\n",
    "        if self.memory.buffer_position < self.batch_size:\n",
    "            return\n",
    "        \n",
    "\n",
    "        state, action, reward, new_state, done = self.memory.sample(self.batch_size)\n",
    "\n",
    "        # TODO - this is in discrete only\n",
    "        print(action)\n",
    "        action_values = np.array(self.actions, dtype=np.int8)\n",
    "        action_indices = np.dot(action,action_values)\n",
    "        \n",
    "        q_eval = self.q_eval.predict(state)\n",
    "        q_next = self.q_eval.predict(new_state)\n",
    "        \n",
    "        q_target = q_eval.copy()\n",
    "        \n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        \n",
    "        q_target[batch_index,action_indices] = reward + self.gamma * np.max(q_next,axis=1)* done\n",
    "        \n",
    "        _ = self.q_eval.fit(state,q_target,verbose=0)\n",
    "        \n",
    "        self.decrement_eps()\n",
    "        \n",
    "    def learn(self, timesteps=-1, success_threshold=150, plot_results=True):\n",
    "        obs = self.env.reset()\n",
    "        \n",
    "        self.total_rewards = []\n",
    "        self.avg_rewards = []\n",
    "        \n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "\n",
    "        # Loop condition\n",
    "        def learning_condition():\n",
    "            if timesteps == -1:\n",
    "                return True\n",
    "            else: \n",
    "                return timesteps > timestep\n",
    "        \n",
    "        while learning_condition():\n",
    "\n",
    "            # Choose action\n",
    "            action = self.choose_action(obs)\n",
    "            \n",
    "            # Step\n",
    "            obs_,reward,done, info = self.env.step(action)\n",
    "            \n",
    "            # Get next state\n",
    "            score += reward\n",
    "            \n",
    "            self.remember(obs,action,reward,obs_,done)\n",
    "            obs = obs_\n",
    "            \n",
    "            self.replay()\n",
    "\n",
    "            \n",
    "            if done:\n",
    "                # Loop episode state\n",
    "                if episode % 2 == 0 and episode > 0:\n",
    "                    print('episode',episode,'score',score,'epsilon %:.3f',self.epsilon)\n",
    "                \n",
    "                # Update pointers\n",
    "                self.total_rewards.append(score)\n",
    "                \n",
    "                # Track reward evolution\n",
    "                if len(self.total_rewards) > 100:\n",
    "                    avg_reward = np.mean(self.total_rewards[-100:])\n",
    "                    self.avg_rewards.append(avg_reward)\n",
    "                    \n",
    "                    # Break loop if average reward is greater than success threshold\n",
    "                    if avg_reward > success_threshold:\n",
    "                        print('Agent solved environment at the episode {}'.format(episode))\n",
    "                        break\n",
    "                \n",
    "                # Reset environment\n",
    "                score = 0\n",
    "                episode +=1\n",
    "                obs = self.env.reset()\n",
    "                #state = self.get_state(obs)\n",
    "                \n",
    "            # Update timestep counter\n",
    "            timestep+=1\n",
    "        \n",
    "        if plot_results:\n",
    "            plt.plot(self.avg_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "312e9929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:18:38.322924Z",
     "start_time": "2022-07-21T16:18:38.318923Z"
    }
   },
   "outputs": [],
   "source": [
    "#agent = DqnAgent(environment,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "159e2b0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:18:38.588205Z",
     "start_time": "2022-07-21T16:18:38.575177Z"
    }
   },
   "outputs": [],
   "source": [
    "#agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7367717b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:18:38.821509Z",
     "start_time": "2022-07-21T16:18:38.807509Z"
    }
   },
   "outputs": [],
   "source": [
    "def environment():\n",
    "    return gym.make('LunarLanderContinuous-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d1033c40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:18:39.324775Z",
     "start_time": "2022-07-21T16:18:39.160234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06728631  0.6769024 ]\n",
      " [-0.01320568  0.30705336]\n",
      " [-0.06728631  0.6769024 ]\n",
      " [-0.5538533  -0.1811342 ]\n",
      " [-0.06728631  0.6769024 ]]\n",
      "WARNING:tensorflow:6 out of the last 73 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000219519FFD00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[ 0.10909441 -0.06484571]\n",
      " [ 0.11106436 -0.06592897]\n",
      " [ 0.10909441 -0.06484571]\n",
      " [ 0.1152625  -0.06834333]\n",
      " [ 0.10909441 -0.06484571]]\n",
      "\n",
      "[0 1 2 3 4]\n",
      "\n",
      "[[-0.06728631  0.6769024 ]\n",
      " [-0.01320568  0.30705336]\n",
      " [-0.06728631  0.6769024 ]\n",
      " [-0.5538533  -0.1811342 ]\n",
      " [-0.06728631  0.6769024 ]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [90]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m agent2 \u001b[38;5;241m=\u001b[39m DqnAgent(environment,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43magent2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36mDqnAgent.learn\u001b[1;34m(self, timesteps, success_threshold, plot_results)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremember(obs,action,reward,obs_,done)\n\u001b[0;32m    137\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs_\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;66;03m# Loop episode state\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m episode \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[1;32mIn [86]\u001b[0m, in \u001b[0;36mDqnAgent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m    101\u001b[0m action_indices\u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m--> 102\u001b[0m q_target[batch_index,action_indices] \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(q_next,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m done\n\u001b[0;32m    104\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_eval\u001b[38;5;241m.\u001b[39mfit(state,q_target,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecrement_eps()\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "agent2 = DqnAgent(environment,batch_size=5)\n",
    "agent2.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a442c56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T15:58:15.829931Z",
     "start_time": "2022-07-21T15:58:15.815977Z"
    }
   },
   "outputs": [],
   "source": [
    "gym.make('LunarLanderContinuous-v2').action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "895be052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:04:22.477571Z",
     "start_time": "2022-07-21T16:04:22.455794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.2882689e-04,  1.4102881e+00,  2.2365194e-02,  4.8845876e-02,\n",
       "        -2.1515167e-03, -4.0259656e-02,  0.0000000e+00,  0.0000000e+00],\n",
       "       dtype=float32),\n",
       " 1.8001698641777761,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.step([0.9,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a930462d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:05:08.453518Z",
     "start_time": "2022-07-21T16:05:07.695100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04110719, -0.17545287]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.q_eval.predict(np.expand_dims(agent.env.reset(),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98b0a186",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:06:39.456794Z",
     "start_time": "2022-07-21T16:06:39.446559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00695028,  1.4209193 , -0.7040039 ,  0.4443818 ,  0.00806044,\n",
       "         0.1594675 ,  0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.reset()[np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33d07b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T16:07:37.522757Z",
     "start_time": "2022-07-21T16:07:37.511750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.2968304 , -0.19083126], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f03d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
