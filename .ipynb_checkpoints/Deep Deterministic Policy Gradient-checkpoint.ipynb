{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f5a87b",
   "metadata": {},
   "source": [
    "### Quick theory\n",
    "Just like the Actor-Critic method, we have two networks:\n",
    "\n",
    "* Actor - It proposes an action given a state.\n",
    "* Critic - It predicts if the action is good (positive value) or bad (negative value) given a state and an action.\n",
    "\n",
    "DDPG uses two more techniques not present in the original DQN:\n",
    "\n",
    "1. Uses two Target networks.\n",
    "        Why? Because it add stability to training. In short, we are learning from estimated targets and Target networks are updated slowly, hence keeping our estimated targets stable.Conceptually, this is like saying, \"I have an idea of how to play this well, I'm going to try it out for a bit until I find something better\", as opposed to saying \"I'm going to re-learn how to play this entire game after every move\". See this StackOverflow answer.\n",
    "\n",
    "2. Uses Experience Replay.\n",
    "\n",
    "### Losses:\n",
    "\n",
    "**Critic loss** - Mean Squared Error of y - Q(s, a) where y is the expected return as seen by the Target network, and Q(s, a) is action value predicted by the Critic network. y is a moving target that the critic model tries to achieve; we make this target stable by updating the Target model slowly.\n",
    "\n",
    "**Actor loss** - This is computed using the mean of the value given by the Critic network for the actions taken by the Actor network. We seek to maximize this quantity.\n",
    "\n",
    "Hence we update the Actor network so that it produces actions that get the maximum predicted value as seen by the Critic, for a given state.\n",
    "\n",
    "### Initialization:\n",
    "The initialization for last layer of the Actor must be between -0.003 and 0.003 as this prevents us from getting 1 or -1 output values in the initial stages, which would squash our gradients to zero, as we use the tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:46.673827Z",
     "start_time": "2022-07-22T11:01:46.669617Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:46.850736Z",
     "start_time": "2022-07-22T11:01:46.845708Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a26d83a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:47.020377Z",
     "start_time": "2022-07-22T11:01:47.013376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  [2.]\n",
      "Min Value of Action ->  [-2.]\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high#[0]\n",
    "lower_bound = env.action_space.low#[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3264a114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:47.165415Z",
     "start_time": "2022-07-22T11:01:47.158387Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To implement better exploration by the Actor network, we use noisy perturbations, \n",
    "specifically an Ornstein-Uhlenbeck process for generating noise, as described in the paper. \n",
    "It samples noise from a correlated normal distribution.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c167df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:01:47.305457Z",
     "start_time": "2022-07-22T11:01:47.299457Z"
    }
   },
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "135761ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:03:26.596582Z",
     "start_time": "2022-07-22T11:03:26.572988Z"
    }
   },
   "outputs": [],
   "source": [
    "class DdpgAgent(Agent):\n",
    "    def __init__(self,\n",
    "                 environment,\n",
    "                 gamma = 0.99,\n",
    "                 tau= 0.005,\n",
    "                 std_dev = 0.2,\n",
    "                 critic_lr = 0.002,\n",
    "                 actor_lr = 0.001,\n",
    "                 buffer_size=50000,\n",
    "                 batch_size=64,\n",
    "                 critic_optimizer = tf.keras.optimizers.Adam,\n",
    "                 actor_optimizer = tf.keras.optimizers.Adam,\n",
    "        ):\n",
    "        super(DdpgAgent,self).__init__(environment)\n",
    "        \n",
    "        self.std_dev = std_dev\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.critic_lr = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "\n",
    "        self.noise = OUActionNoise(mean=np.zeros(self.n_actions), std_deviation=float(std_dev) * np.ones(self.n_actions))\n",
    "        \n",
    "        self.critic_optimizer = critic_optimizer(critic_lr)\n",
    "        self.actor_optimizer = actor_optimizer(actor_lr)\n",
    "\n",
    "        # Discount factor for future rewards\n",
    "        self.gamma = gamma\n",
    "        # Used to update target networks\n",
    "        self.tau = tau\n",
    "\n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = Buffer(self.buffer_size, self.batch_size)\n",
    "            \n",
    "    def __init_networks(self):\n",
    "        \n",
    "        def create_actor():\n",
    "            # Initialize weights between -3e-3 and 3-e3\n",
    "            last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "            inputs = layers.Input(shape=self.env.observation_space.shape)\n",
    "            out = layers.Flatten()(inputs)\n",
    "            out = layers.Dense(256, activation=\"relu\")(out)\n",
    "            out = layers.Dense(256, activation=\"relu\")(out)\n",
    "            outputs = layers.Dense(self.n_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "            # Our upper bound is 2.0 for Pendulum.\n",
    "            outputs = outputs * self.action_upper_bounds\n",
    "            return tf.keras.Model(inputs, outputs)\n",
    "        \n",
    "        def create_critic():\n",
    "            # State as input\n",
    "            state_input = layers.Input(shape=self.env.observation_space.shape)\n",
    "            state_out = layers.Flatten()(state_input)\n",
    "            state_out = layers.Dense(16, activation=\"relu\")(state_out)\n",
    "            state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "            # Action as input\n",
    "            action_input = layers.Input(shape=(self.n_actions))\n",
    "            action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "            # Both are passed through seperate layer before concatenating\n",
    "            concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "            out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "            out = layers.Dense(256, activation=\"relu\")(out)\n",
    "            outputs = layers.Dense(1)(out)\n",
    "\n",
    "            # Outputs single value for give state-action\n",
    "            return tf.keras.Model([state_input, action_input], outputs)\n",
    "        \n",
    "        self.actor = create_actor()\n",
    "        self.target_actor = create_actor()\n",
    "        \n",
    "        self.critic = create_critic()\n",
    "        self.target_critic = create_critic()\n",
    "        \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "    \n",
    "    def choose_action(self,state):\n",
    "        state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        sampled_actions = tf.squeeze(self.actor(state))\n",
    "        noise = self.noise()\n",
    "        # Adding noise to action\n",
    "        sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "        # We make sure action is within bounds\n",
    "        legal_action = np.clip(sampled_actions, self.action_lower_bounds, self.action_upper_bounds)\n",
    "\n",
    "        return [np.squeeze(legal_action)]\n",
    "    \n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target(self,target_weights, weights, tau):\n",
    "        for (a, b) in zip(target_weights, weights):\n",
    "            a.assign(b * tau + a * (1 - tau))\n",
    "            \n",
    "    @tf.function\n",
    "    def update(self,\n",
    "        state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + self.gamma * self.target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.critic([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.critic.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(state_batch, training=True)\n",
    "            critic_value = self.critic([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.actor.trainable_variables)\n",
    "        )\n",
    "            \n",
    "    def replay(self):\n",
    "\n",
    "        record_range = min(self.buffer.buffer_counter, self.buffer.buffer_capacity)\n",
    "        \n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.buffer.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.buffer.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.buffer.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.buffer.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.buffer.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "        \n",
    "    def test(self, episodes=10, render=True):\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                    \n",
    "                action = self.choose_action(state)\n",
    "\n",
    "                # Step\n",
    "                state,reward,done, info = self.env.step(action)\n",
    "\n",
    "                # Get next state\n",
    "                score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.close()\n",
    "\n",
    "            print(\"Test episode: {}, score: {:.2f}\".format(episode,score)) \n",
    "    \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, log_each_n_episodes=100, success_threshold=False):\n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    "\n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            prev_state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                \n",
    "                action = self.choose_action(prev_state)\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                self.buffer.record((prev_state, action, reward, state))\n",
    "                self.replay()\n",
    "                self.update_target(self.target_actor.variables, self.actor.variables, self.tau)\n",
    "                self.update_target(self.target_critic.variables, self.critic.variables, self.tau)\n",
    "                \n",
    "                score += reward\n",
    "                timestep+=1\n",
    "                prev_state=state\n",
    "                \n",
    " \n",
    "            self.running_reward.step(score)\n",
    "            # Log details\n",
    "            episode += 1\n",
    "            if episode % log_each_n_episodes == 0 and episode > 0:\n",
    "                print('episode {}, running reward: {:.2f}, last reward: {:.2f}'.format(episode,self.running_reward.reward, score))\n",
    "\n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                break\n",
    "                        \n",
    "                prev_state = state\n",
    "\n",
    "        if plot_results:\n",
    "            self.plot_learning_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1c265c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:14:45.288230Z",
     "start_time": "2022-07-22T11:03:27.637018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| Pendulum-v1\n",
      "| \n",
      "| Action space: Continuous with low state-space\n",
      "| Environment beated threshold: -200\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "episode 1, running reward: -65.81, last reward: -1316.26\n",
      "episode 2, running reward: -141.50, last reward: -1579.49\n",
      "episode 3, running reward: -218.18, last reward: -1675.20\n",
      "episode 4, running reward: -277.80, last reward: -1410.63\n",
      "episode 5, running reward: -327.33, last reward: -1268.31\n",
      "episode 6, running reward: -377.54, last reward: -1331.58\n",
      "episode 7, running reward: -428.02, last reward: -1387.09\n",
      "episode 8, running reward: -471.11, last reward: -1289.82\n",
      "episode 9, running reward: -526.14, last reward: -1571.71\n",
      "episode 10, running reward: -562.35, last reward: -1250.40\n",
      "episode 11, running reward: -579.30, last reward: -901.38\n",
      "episode 12, running reward: -583.09, last reward: -655.09\n",
      "episode 13, running reward: -592.88, last reward: -778.85\n",
      "episode 14, running reward: -599.21, last reward: -719.53\n",
      "episode 15, running reward: -576.15, last reward: -137.87\n",
      "episode 16, running reward: -554.25, last reward: -138.31\n",
      "episode 17, running reward: -589.66, last reward: -1262.32\n",
      "episode 18, running reward: -566.56, last reward: -127.70\n",
      "episode 19, running reward: -538.30, last reward: -1.44\n",
      "episode 20, running reward: -530.51, last reward: -382.45\n",
      "episode 21, running reward: -576.52, last reward: -1450.68\n",
      "episode 22, running reward: -563.18, last reward: -309.70\n",
      "episode 23, running reward: -552.95, last reward: -358.62\n",
      "episode 24, running reward: -531.59, last reward: -125.75\n",
      "episode 25, running reward: -517.33, last reward: -246.29\n",
      "episode 26, running reward: -503.88, last reward: -248.32\n",
      "episode 27, running reward: -484.80, last reward: -122.32\n",
      "episode 28, running reward: -472.86, last reward: -245.94\n",
      "episode 29, running reward: -455.63, last reward: -128.43\n",
      "episode 30, running reward: -458.81, last reward: -519.10\n",
      "episode 31, running reward: -459.70, last reward: -476.63\n",
      "episode 32, running reward: -461.43, last reward: -494.34\n",
      "episode 33, running reward: -462.32, last reward: -479.16\n",
      "episode 34, running reward: -445.70, last reward: -129.92\n",
      "episode 35, running reward: -429.90, last reward: -129.72\n",
      "episode 36, running reward: -420.99, last reward: -251.66\n",
      "episode 37, running reward: -406.60, last reward: -133.35\n",
      "episode 38, running reward: -411.23, last reward: -499.17\n",
      "episode 39, running reward: -409.54, last reward: -377.46\n",
      "episode 40, running reward: -395.80, last reward: -134.68\n",
      "episode 41, running reward: -388.20, last reward: -243.77\n",
      "episode 42, running reward: -375.49, last reward: -134.11\n",
      "episode 43, running reward: -374.53, last reward: -356.30\n",
      "episode 44, running reward: -362.24, last reward: -128.64\n",
      "episode 45, running reward: -350.44, last reward: -126.25\n",
      "episode 46, running reward: -339.70, last reward: -135.71\n",
      "episode 47, running reward: -335.03, last reward: -246.15\n",
      "episode 48, running reward: -330.00, last reward: -234.53\n",
      "episode 49, running reward: -320.10, last reward: -131.93\n",
      "episode 50, running reward: -310.51, last reward: -128.33\n",
      "episode 51, running reward: -301.57, last reward: -131.75\n",
      "episode 52, running reward: -301.11, last reward: -292.40\n",
      "episode 53, running reward: -292.69, last reward: -132.66\n",
      "episode 54, running reward: -289.83, last reward: -235.44\n",
      "episode 55, running reward: -281.90, last reward: -131.35\n",
      "episode 56, running reward: -274.54, last reward: -134.55\n",
      "episode 57, running reward: -273.16, last reward: -246.95\n",
      "episode 58, running reward: -266.19, last reward: -133.83\n",
      "episode 59, running reward: -269.44, last reward: -331.21\n",
      "episode 60, running reward: -272.83, last reward: -337.18\n",
      "episode 61, running reward: -265.76, last reward: -131.38\n",
      "episode 62, running reward: -258.68, last reward: -124.19\n",
      "episode 63, running reward: -257.78, last reward: -240.65\n",
      "episode 64, running reward: -245.37, last reward: -9.70\n",
      "episode 65, running reward: -248.01, last reward: -298.09\n",
      "episode 66, running reward: -241.97, last reward: -127.16\n",
      "episode 67, running reward: -230.37, last reward: -10.03\n",
      "episode 68, running reward: -219.24, last reward: -7.79\n",
      "episode 69, running reward: -214.78, last reward: -130.12\n",
      "episode 70, running reward: -210.56, last reward: -130.34\n",
      "episode 71, running reward: -206.47, last reward: -128.64\n",
      "episode 72, running reward: -213.64, last reward: -349.95\n",
      "episode 73, running reward: -209.02, last reward: -121.18\n",
      "episode 74, running reward: -215.22, last reward: -333.16\n",
      "episode 75, running reward: -218.79, last reward: -286.62\n",
      "episode 76, running reward: -219.94, last reward: -241.63\n",
      "episode 77, running reward: -215.37, last reward: -128.65\n",
      "episode 78, running reward: -211.02, last reward: -128.32\n",
      "episode 79, running reward: -212.68, last reward: -244.17\n",
      "episode 80, running reward: -208.41, last reward: -127.37\n",
      "episode 81, running reward: -216.07, last reward: -361.63\n",
      "episode 82, running reward: -211.68, last reward: -128.22\n",
      "episode 83, running reward: -207.55, last reward: -129.05\n",
      "episode 84, running reward: -203.14, last reward: -119.45\n",
      "episode 85, running reward: -209.35, last reward: -327.27\n",
      "episode 86, running reward: -205.39, last reward: -130.09\n",
      "episode 87, running reward: -201.19, last reward: -121.39\n",
      "episode 88, running reward: -197.30, last reward: -123.52\n",
      "episode 89, running reward: -193.80, last reward: -127.15\n",
      "episode 90, running reward: -196.25, last reward: -242.81\n",
      "episode 91, running reward: -198.39, last reward: -239.18\n",
      "episode 92, running reward: -194.53, last reward: -121.19\n",
      "episode 93, running reward: -190.95, last reward: -122.95\n",
      "episode 94, running reward: -193.14, last reward: -234.71\n",
      "episode 95, running reward: -183.63, last reward: -2.90\n",
      "episode 96, running reward: -180.49, last reward: -120.78\n",
      "episode 97, running reward: -177.80, last reward: -126.80\n",
      "episode 98, running reward: -175.02, last reward: -122.19\n",
      "episode 99, running reward: -166.38, last reward: -2.10\n",
      "episode 100, running reward: -181.75, last reward: -473.95\n",
      "episode 101, running reward: -172.94, last reward: -5.37\n",
      "episode 102, running reward: -176.63, last reward: -246.85\n",
      "episode 103, running reward: -180.09, last reward: -245.81\n",
      "episode 104, running reward: -177.54, last reward: -129.07\n",
      "episode 105, running reward: -174.66, last reward: -119.93\n",
      "episode 106, running reward: -172.00, last reward: -121.53\n",
      "episode 107, running reward: -169.65, last reward: -124.93\n",
      "episode 108, running reward: -167.53, last reward: -127.35\n",
      "episode 109, running reward: -165.12, last reward: -119.30\n",
      "episode 110, running reward: -175.30, last reward: -368.59\n",
      "episode 111, running reward: -178.50, last reward: -239.45\n",
      "episode 112, running reward: -181.51, last reward: -238.57\n",
      "episode 113, running reward: -178.75, last reward: -126.40\n",
      "episode 114, running reward: -170.02, last reward: -4.07\n",
      "episode 115, running reward: -168.06, last reward: -130.85\n",
      "episode 116, running reward: -166.11, last reward: -129.05\n",
      "episode 117, running reward: -176.89, last reward: -381.73\n",
      "episode 118, running reward: -174.24, last reward: -123.94\n",
      "episode 119, running reward: -171.78, last reward: -124.91\n",
      "episode 120, running reward: -174.98, last reward: -235.80\n",
      "episode 121, running reward: -172.39, last reward: -123.25\n",
      "episode 122, running reward: -182.07, last reward: -366.03\n",
      "episode 123, running reward: -179.40, last reward: -128.56\n",
      "episode 124, running reward: -176.54, last reward: -122.37\n",
      "episode 125, running reward: -180.64, last reward: -258.38\n",
      "episode 126, running reward: -189.77, last reward: -363.20\n",
      "episode 127, running reward: -186.62, last reward: -126.85\n",
      "episode 128, running reward: -189.67, last reward: -247.71\n",
      "episode 129, running reward: -186.14, last reward: -118.92\n",
      "episode 130, running reward: -176.90, last reward: -1.38\n",
      "episode 131, running reward: -174.20, last reward: -122.84\n",
      "episode 132, running reward: -165.59, last reward: -2.13\n",
      "episode 133, running reward: -163.46, last reward: -122.91\n",
      "episode 134, running reward: -161.09, last reward: -116.09\n",
      "episode 135, running reward: -169.24, last reward: -324.07\n",
      "episode 136, running reward: -172.46, last reward: -233.74\n",
      "episode 137, running reward: -169.90, last reward: -121.28\n",
      "episode 138, running reward: -179.11, last reward: -354.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 139, running reward: -176.22, last reward: -121.34\n",
      "episode 140, running reward: -180.17, last reward: -255.04\n",
      "episode 141, running reward: -183.56, last reward: -248.15\n",
      "episode 142, running reward: -180.71, last reward: -126.52\n",
      "episode 143, running reward: -171.84, last reward: -3.33\n",
      "episode 144, running reward: -175.47, last reward: -244.29\n",
      "episode 145, running reward: -173.04, last reward: -126.90\n",
      "episode 146, running reward: -170.83, last reward: -128.97\n",
      "episode 147, running reward: -168.54, last reward: -125.01\n",
      "episode 148, running reward: -160.22, last reward: -2.15\n",
      "episode 149, running reward: -170.68, last reward: -369.38\n",
      "episode 150, running reward: -180.41, last reward: -365.21\n",
      "episode 151, running reward: -177.57, last reward: -123.58\n",
      "episode 152, running reward: -168.82, last reward: -2.57\n",
      "episode 153, running reward: -172.64, last reward: -245.33\n",
      "episode 154, running reward: -169.99, last reward: -119.63\n",
      "episode 155, running reward: -173.06, last reward: -231.36\n",
      "episode 156, running reward: -171.12, last reward: -134.22\n",
      "episode 157, running reward: -168.79, last reward: -124.49\n",
      "episode 158, running reward: -166.46, last reward: -122.20\n",
      "episode 159, running reward: -163.88, last reward: -114.83\n",
      "episode 160, running reward: -167.72, last reward: -240.83\n",
      "episode 161, running reward: -165.59, last reward: -125.10\n",
      "episode 162, running reward: -163.47, last reward: -123.14\n",
      "episode 163, running reward: -161.39, last reward: -121.96\n",
      "episode 164, running reward: -159.82, last reward: -129.89\n",
      "episode 165, running reward: -163.58, last reward: -234.97\n",
      "episode 166, running reward: -167.75, last reward: -247.11\n",
      "episode 167, running reward: -165.46, last reward: -121.97\n",
      "episode 168, running reward: -175.26, last reward: -361.44\n",
      "episode 169, running reward: -172.64, last reward: -122.84\n",
      "episode 170, running reward: -182.46, last reward: -368.99\n",
      "episode 171, running reward: -186.79, last reward: -269.12\n",
      "episode 172, running reward: -183.69, last reward: -124.82\n",
      "episode 173, running reward: -180.71, last reward: -123.95\n",
      "episode 174, running reward: -178.12, last reward: -128.96\n",
      "episode 175, running reward: -169.38, last reward: -3.41\n",
      "episode 176, running reward: -172.16, last reward: -224.94\n",
      "episode 177, running reward: -169.78, last reward: -124.62\n",
      "episode 178, running reward: -167.92, last reward: -132.48\n",
      "episode 179, running reward: -176.91, last reward: -347.76\n",
      "episode 180, running reward: -185.61, last reward: -350.86\n",
      "episode 181, running reward: -182.36, last reward: -120.70\n",
      "episode 182, running reward: -179.64, last reward: -127.97\n",
      "episode 183, running reward: -181.90, last reward: -224.81\n",
      "episode 184, running reward: -173.07, last reward: -5.29\n",
      "episode 185, running reward: -170.45, last reward: -120.62\n",
      "episode 186, running reward: -168.15, last reward: -124.45\n",
      "episode 187, running reward: -185.37, last reward: -512.65\n",
      "episode 188, running reward: -182.91, last reward: -136.17\n",
      "episode 189, running reward: -180.75, last reward: -139.63\n",
      "episode 190, running reward: -183.08, last reward: -227.28\n",
      "episode 191, running reward: -191.70, last reward: -355.52\n",
      "episode 192, running reward: -194.84, last reward: -254.50\n",
      "episode 193, running reward: -215.94, last reward: -616.81\n",
      "episode 194, running reward: -205.15, last reward: -0.14\n",
      "episode 195, running reward: -207.42, last reward: -250.63\n",
      "episode 196, running reward: -209.55, last reward: -249.94\n",
      "episode 197, running reward: -210.42, last reward: -226.99\n",
      "episode 198, running reward: -205.98, last reward: -121.57\n",
      "episode 199, running reward: -208.04, last reward: -247.33\n",
      "episode 200, running reward: -203.67, last reward: -120.60\n",
      "episode 201, running reward: -199.70, last reward: -124.14\n",
      "episode 202, running reward: -195.84, last reward: -122.59\n",
      "episode 203, running reward: -197.38, last reward: -226.74\n",
      "episode 204, running reward: -198.62, last reward: -222.10\n",
      "episode 205, running reward: -201.26, last reward: -251.39\n",
      "episode 206, running reward: -203.61, last reward: -248.31\n",
      "episode 207, running reward: -211.56, last reward: -362.53\n",
      "episode 208, running reward: -213.08, last reward: -242.03\n",
      "episode 209, running reward: -208.77, last reward: -126.79\n",
      "episode 210, running reward: -216.28, last reward: -359.08\n",
      "episode 211, running reward: -211.80, last reward: -126.63\n",
      "episode 212, running reward: -212.72, last reward: -230.19\n",
      "episode 213, running reward: -207.82, last reward: -114.79\n",
      "episode 214, running reward: -203.40, last reward: -119.43\n",
      "episode 215, running reward: -199.69, last reward: -129.10\n",
      "episode 216, running reward: -195.55, last reward: -116.95\n",
      "episode 217, running reward: -196.78, last reward: -220.08\n",
      "episode 218, running reward: -198.55, last reward: -232.15\n",
      "episode 219, running reward: -206.34, last reward: -354.33\n",
      "episode 220, running reward: -196.04, last reward: -0.47\n",
      "episode 221, running reward: -198.72, last reward: -249.69\n",
      "episode 222, running reward: -195.15, last reward: -127.16\n",
      "episode 223, running reward: -191.33, last reward: -118.77\n",
      "episode 224, running reward: -188.05, last reward: -125.83\n",
      "episode 225, running reward: -184.64, last reward: -119.70\n",
      "episode 226, running reward: -181.72, last reward: -126.39\n",
      "episode 227, running reward: -198.42, last reward: -515.61\n",
      "episode 228, running reward: -201.00, last reward: -250.03\n",
      "episode 229, running reward: -191.13, last reward: -3.62\n",
      "episode 230, running reward: -187.59, last reward: -120.34\n",
      "episode 231, running reward: -189.95, last reward: -234.82\n",
      "episode 232, running reward: -186.41, last reward: -119.21\n",
      "episode 233, running reward: -183.15, last reward: -121.07\n",
      "episode 234, running reward: -187.82, last reward: -276.62\n",
      "episode 235, running reward: -190.33, last reward: -237.98\n",
      "episode 236, running reward: -186.62, last reward: -116.21\n",
      "episode 237, running reward: -183.42, last reward: -122.53\n",
      "episode 238, running reward: -180.51, last reward: -125.16\n",
      "episode 239, running reward: -182.90, last reward: -228.35\n",
      "episode 240, running reward: -174.01, last reward: -5.23\n",
      "episode 241, running reward: -171.63, last reward: -126.38\n",
      "episode 242, running reward: -169.48, last reward: -128.63\n",
      "episode 243, running reward: -166.94, last reward: -118.56\n",
      "episode 244, running reward: -171.70, last reward: -262.20\n",
      "episode 245, running reward: -186.60, last reward: -469.74\n",
      "episode 246, running reward: -188.76, last reward: -229.77\n",
      "episode 247, running reward: -191.14, last reward: -236.32\n",
      "episode 248, running reward: -187.81, last reward: -124.63\n",
      "episode 249, running reward: -210.61, last reward: -643.67\n",
      "episode 250, running reward: -223.49, last reward: -468.26\n",
      "episode 251, running reward: -218.84, last reward: -130.57\n",
      "episode 252, running reward: -208.04, last reward: -2.77\n",
      "episode 253, running reward: -203.64, last reward: -120.01\n",
      "episode 254, running reward: -205.12, last reward: -233.24\n",
      "episode 255, running reward: -201.08, last reward: -124.45\n",
      "episode 256, running reward: -197.13, last reward: -122.08\n",
      "episode 257, running reward: -198.99, last reward: -234.19\n",
      "episode 258, running reward: -195.50, last reward: -129.31\n",
      "episode 259, running reward: -192.23, last reward: -130.07\n",
      "episode 260, running reward: -189.08, last reward: -129.15\n",
      "episode 261, running reward: -185.71, last reward: -121.80\n",
      "episode 262, running reward: -188.52, last reward: -241.85\n",
      "episode 263, running reward: -191.01, last reward: -238.40\n",
      "episode 264, running reward: -200.78, last reward: -386.28\n",
      "episode 265, running reward: -190.84, last reward: -2.11\n",
      "episode 266, running reward: -187.46, last reward: -123.25\n",
      "episode 267, running reward: -183.93, last reward: -116.72\n",
      "episode 268, running reward: -180.64, last reward: -118.17\n",
      "episode 269, running reward: -183.37, last reward: -235.34\n",
      "episode 270, running reward: -180.70, last reward: -129.86\n",
      "episode 271, running reward: -186.12, last reward: -289.07\n",
      "episode 272, running reward: -189.18, last reward: -247.38\n",
      "episode 273, running reward: -186.18, last reward: -129.10\n",
      "episode 274, running reward: -182.78, last reward: -118.31\n",
      "episode 275, running reward: -179.71, last reward: -121.36\n",
      "episode 276, running reward: -170.94, last reward: -4.23\n",
      "episode 277, running reward: -168.80, last reward: -128.07\n",
      "episode 278, running reward: -172.20, last reward: -236.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 279, running reward: -170.13, last reward: -130.90\n",
      "episode 280, running reward: -162.04, last reward: -8.27\n",
      "episode 281, running reward: -160.30, last reward: -127.20\n",
      "episode 282, running reward: -165.27, last reward: -259.78\n",
      "episode 283, running reward: -169.55, last reward: -250.79\n",
      "episode 284, running reward: -161.40, last reward: -6.63\n",
      "episode 285, running reward: -159.55, last reward: -124.49\n",
      "episode 286, running reward: -163.88, last reward: -246.08\n",
      "episode 287, running reward: -168.24, last reward: -251.03\n",
      "episode 288, running reward: -166.35, last reward: -130.43\n",
      "episode 289, running reward: -172.95, last reward: -298.48\n",
      "episode 290, running reward: -170.29, last reward: -119.66\n",
      "episode 291, running reward: -161.95, last reward: -3.53\n",
      "episode 292, running reward: -165.84, last reward: -239.81\n",
      "episode 293, running reward: -169.62, last reward: -241.37\n",
      "episode 294, running reward: -180.27, last reward: -382.65\n",
      "episode 295, running reward: -182.56, last reward: -225.96\n",
      "episode 296, running reward: -185.90, last reward: -249.45\n",
      "episode 297, running reward: -188.62, last reward: -240.36\n",
      "episode 298, running reward: -185.32, last reward: -122.46\n",
      "episode 299, running reward: -176.08, last reward: -0.58\n",
      "episode 300, running reward: -173.16, last reward: -117.64\n",
      "episode 301, running reward: -182.58, last reward: -361.62\n",
      "episode 302, running reward: -190.99, last reward: -350.82\n",
      "episode 303, running reward: -193.51, last reward: -241.42\n",
      "episode 304, running reward: -231.93, last reward: -961.82\n",
      "episode 305, running reward: -220.51, last reward: -3.48\n",
      "episode 306, running reward: -209.72, last reward: -4.74\n",
      "episode 307, running reward: -199.52, last reward: -5.74\n",
      "episode 308, running reward: -195.40, last reward: -117.07\n",
      "episode 309, running reward: -191.56, last reward: -118.76\n",
      "episode 310, running reward: -182.09, last reward: -1.97\n",
      "episode 311, running reward: -179.28, last reward: -125.91\n",
      "episode 312, running reward: -176.50, last reward: -123.80\n",
      "episode 313, running reward: -173.44, last reward: -115.27\n",
      "episode 314, running reward: -170.74, last reward: -119.48\n",
      "episode 315, running reward: -174.19, last reward: -239.71\n",
      "episode 316, running reward: -172.01, last reward: -130.55\n",
      "episode 317, running reward: -169.14, last reward: -114.56\n",
      "episode 318, running reward: -166.94, last reward: -125.25\n",
      "episode 319, running reward: -176.63, last reward: -360.65\n",
      "episode 320, running reward: -174.26, last reward: -129.30\n",
      "episode 321, running reward: -171.75, last reward: -124.08\n",
      "episode 322, running reward: -169.58, last reward: -128.30\n",
      "episode 323, running reward: -167.52, last reward: -128.43\n",
      "episode 324, running reward: -175.25, last reward: -322.13\n",
      "episode 325, running reward: -172.78, last reward: -125.76\n",
      "episode 326, running reward: -170.42, last reward: -125.52\n",
      "episode 327, running reward: -168.27, last reward: -127.42\n",
      "episode 328, running reward: -166.14, last reward: -125.69\n",
      "episode 329, running reward: -164.03, last reward: -124.10\n",
      "episode 330, running reward: -162.04, last reward: -124.08\n",
      "episode 331, running reward: -154.05, last reward: -2.29\n",
      "episode 332, running reward: -152.52, last reward: -123.55\n",
      "episode 333, running reward: -151.00, last reward: -122.10\n",
      "episode 334, running reward: -149.61, last reward: -123.10\n",
      "episode 335, running reward: -142.34, last reward: -4.15\n",
      "episode 336, running reward: -152.84, last reward: -352.49\n",
      "episode 337, running reward: -164.42, last reward: -384.37\n",
      "episode 338, running reward: -162.54, last reward: -126.85\n",
      "episode 339, running reward: -166.80, last reward: -247.62\n",
      "episode 340, running reward: -164.45, last reward: -119.94\n",
      "episode 341, running reward: -168.52, last reward: -245.85\n",
      "episode 342, running reward: -166.26, last reward: -123.18\n",
      "episode 343, running reward: -163.89, last reward: -118.88\n",
      "episode 344, running reward: -155.85, last reward: -3.18\n",
      "episode 345, running reward: -164.67, last reward: -332.28\n",
      "episode 346, running reward: -162.72, last reward: -125.69\n",
      "episode 347, running reward: -165.86, last reward: -225.43\n",
      "episode 348, running reward: -208.72, last reward: -1023.10\n",
      "episode 349, running reward: -204.72, last reward: -128.63\n",
      "episode 350, running reward: -200.45, last reward: -119.49\n",
      "episode 351, running reward: -190.67, last reward: -4.72\n",
      "episode 352, running reward: -181.25, last reward: -2.30\n",
      "episode 353, running reward: -183.67, last reward: -229.68\n",
      "episode 354, running reward: -180.91, last reward: -128.41\n",
      "episode 355, running reward: -178.39, last reward: -130.55\n",
      "episode 356, running reward: -181.66, last reward: -243.85\n",
      "episode 357, running reward: -178.79, last reward: -124.18\n",
      "episode 358, running reward: -181.62, last reward: -235.44\n",
      "episode 359, running reward: -190.91, last reward: -367.43\n",
      "episode 360, running reward: -187.38, last reward: -120.20\n",
      "episode 361, running reward: -184.87, last reward: -137.20\n",
      "episode 362, running reward: -187.07, last reward: -229.00\n",
      "episode 363, running reward: -190.07, last reward: -246.89\n",
      "episode 364, running reward: -186.52, last reward: -119.25\n",
      "episode 365, running reward: -183.64, last reward: -128.92\n",
      "episode 366, running reward: -180.48, last reward: -120.33\n",
      "episode 367, running reward: -183.85, last reward: -247.84\n",
      "episode 368, running reward: -180.80, last reward: -122.89\n",
      "episode 369, running reward: -183.27, last reward: -230.24\n",
      "episode 370, running reward: -186.93, last reward: -256.41\n",
      "episode 371, running reward: -189.30, last reward: -234.45\n",
      "episode 372, running reward: -185.81, last reward: -119.50\n",
      "episode 373, running reward: -182.48, last reward: -119.19\n",
      "episode 374, running reward: -185.59, last reward: -244.70\n",
      "episode 375, running reward: -182.95, last reward: -132.73\n",
      "episode 376, running reward: -179.53, last reward: -114.55\n",
      "episode 377, running reward: -177.03, last reward: -129.46\n",
      "episode 378, running reward: -174.34, last reward: -123.40\n",
      "episode 379, running reward: -171.75, last reward: -122.45\n",
      "episode 380, running reward: -169.11, last reward: -119.03\n",
      "episode 381, running reward: -172.73, last reward: -241.48\n",
      "episode 382, running reward: -170.41, last reward: -126.31\n",
      "episode 383, running reward: -168.32, last reward: -128.49\n",
      "episode 384, running reward: -172.52, last reward: -252.33\n",
      "episode 385, running reward: -176.49, last reward: -252.03\n",
      "episode 386, running reward: -174.09, last reward: -128.49\n",
      "episode 387, running reward: -171.58, last reward: -123.94\n",
      "episode 388, running reward: -174.58, last reward: -231.58\n",
      "episode 389, running reward: -172.37, last reward: -130.25\n",
      "episode 390, running reward: -163.89, last reward: -2.85\n",
      "episode 391, running reward: -169.01, last reward: -266.30\n",
      "episode 392, running reward: -166.31, last reward: -114.92\n",
      "episode 393, running reward: -169.86, last reward: -237.44\n",
      "episode 394, running reward: -167.52, last reward: -122.90\n",
      "episode 395, running reward: -159.18, last reward: -0.74\n",
      "episode 396, running reward: -157.40, last reward: -123.68\n",
      "episode 397, running reward: -155.51, last reward: -119.46\n",
      "episode 398, running reward: -153.83, last reward: -122.00\n",
      "episode 399, running reward: -152.27, last reward: -122.63\n",
      "episode 400, running reward: -150.56, last reward: -118.11\n",
      "episode 401, running reward: -149.50, last reward: -129.28\n",
      "episode 402, running reward: -148.45, last reward: -128.45\n",
      "episode 403, running reward: -147.58, last reward: -131.07\n",
      "episode 404, running reward: -152.40, last reward: -244.00\n",
      "episode 405, running reward: -151.20, last reward: -128.39\n",
      "episode 406, running reward: -149.66, last reward: -120.51\n",
      "episode 407, running reward: -148.17, last reward: -119.71\n",
      "episode 408, running reward: -146.92, last reward: -123.20\n",
      "episode 409, running reward: -151.58, last reward: -240.10\n",
      "episode 410, running reward: -150.44, last reward: -128.96\n",
      "episode 411, running reward: -149.00, last reward: -121.49\n",
      "episode 412, running reward: -141.62, last reward: -1.45\n",
      "episode 413, running reward: -140.88, last reward: -126.75\n",
      "episode 414, running reward: -140.24, last reward: -128.08\n",
      "episode 415, running reward: -139.09, last reward: -117.30\n",
      "episode 416, running reward: -138.21, last reward: -121.51\n",
      "episode 417, running reward: -137.05, last reward: -114.92\n",
      "episode 418, running reward: -136.61, last reward: -128.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 419, running reward: -135.98, last reward: -123.97\n",
      "episode 420, running reward: -135.15, last reward: -119.34\n",
      "episode 421, running reward: -140.52, last reward: -242.56\n",
      "episode 422, running reward: -139.85, last reward: -127.12\n",
      "episode 423, running reward: -149.84, last reward: -339.77\n",
      "episode 424, running reward: -160.35, last reward: -359.97\n",
      "episode 425, running reward: -158.68, last reward: -126.95\n",
      "episode 426, running reward: -156.77, last reward: -120.54\n",
      "episode 427, running reward: -155.29, last reward: -127.14\n",
      "episode 428, running reward: -158.99, last reward: -229.18\n",
      "episode 429, running reward: -151.07, last reward: -0.77\n",
      "episode 430, running reward: -149.38, last reward: -117.23\n",
      "episode 431, running reward: -153.97, last reward: -241.15\n",
      "episode 432, running reward: -152.54, last reward: -125.26\n",
      "episode 433, running reward: -157.79, last reward: -257.58\n",
      "episode 434, running reward: -162.76, last reward: -257.22\n",
      "episode 435, running reward: -173.17, last reward: -370.94\n",
      "episode 436, running reward: -171.07, last reward: -131.30\n",
      "episode 437, running reward: -168.87, last reward: -126.96\n",
      "episode 438, running reward: -172.32, last reward: -237.95\n",
      "episode 439, running reward: -170.17, last reward: -129.27\n",
      "episode 440, running reward: -161.97, last reward: -6.16\n",
      "episode 441, running reward: -160.42, last reward: -131.02\n",
      "episode 442, running reward: -152.60, last reward: -3.91\n",
      "episode 443, running reward: -157.51, last reward: -250.89\n",
      "episode 444, running reward: -156.17, last reward: -130.61\n",
      "episode 445, running reward: -154.76, last reward: -127.96\n",
      "episode 446, running reward: -153.28, last reward: -125.33\n",
      "episode 447, running reward: -157.39, last reward: -235.30\n",
      "episode 448, running reward: -161.23, last reward: -234.29\n",
      "episode 449, running reward: -159.24, last reward: -121.33\n",
      "episode 450, running reward: -162.46, last reward: -223.79\n",
      "episode 451, running reward: -154.36, last reward: -0.39\n",
      "episode 452, running reward: -158.70, last reward: -241.18\n",
      "episode 453, running reward: -157.04, last reward: -125.52\n",
      "episode 454, running reward: -155.53, last reward: -126.77\n",
      "episode 455, running reward: -153.80, last reward: -120.93\n",
      "episode 456, running reward: -163.23, last reward: -342.38\n",
      "episode 457, running reward: -166.51, last reward: -228.85\n",
      "episode 458, running reward: -175.36, last reward: -343.55\n",
      "episode 459, running reward: -172.77, last reward: -123.55\n",
      "episode 460, running reward: -176.56, last reward: -248.50\n",
      "episode 461, running reward: -186.76, last reward: -380.70\n",
      "episode 462, running reward: -183.56, last reward: -122.78\n",
      "episode 463, running reward: -180.36, last reward: -119.44\n",
      "episode 464, running reward: -177.47, last reward: -122.58\n",
      "episode 465, running reward: -180.02, last reward: -228.50\n",
      "episode 466, running reward: -194.39, last reward: -467.31\n",
      "episode 467, running reward: -190.53, last reward: -117.19\n",
      "episode 468, running reward: -199.11, last reward: -362.29\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minverted_pendulum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environment\n\u001b[0;32m      2\u001b[0m agent \u001b[38;5;241m=\u001b[39m DdpgAgent(environment)\n\u001b[1;32m----> 3\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_each_n_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36mDdpgAgent.learn\u001b[1;34m(self, timesteps, plot_results, reset, log_each_n_episodes, success_threshold)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mrecord((prev_state, action, reward, state))\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay()\n\u001b[1;32m--> 190\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_critic\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39mvariables, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau)\n\u001b[0;32m    193\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "agent = DdpgAgent(environment)\n",
    "agent.learn(log_each_n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50d936a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T11:15:06.595677Z",
     "start_time": "2022-07-22T11:15:06.540951Z"
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [47]\u001b[0m, in \u001b[0;36mDdpgAgent.test\u001b[1;34m(self, episodes, render)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 158\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_action(state)\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# Step\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\gym\\core.py:286\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\gym\\envs\\classic_control\\pendulum.py:208\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    205\u001b[0m gfxdraw\u001b[38;5;241m.\u001b[39mfilled_circle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, offset, offset, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m scale), (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf \u001b[38;5;241m=\u001b[39m pygame\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mflip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msurf, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    210\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mevent\u001b[38;5;241m.\u001b[39mpump()\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97c37bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
