{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:15:28.985941Z",
     "start_time": "2022-07-29T11:15:28.957004Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:15:33.477134Z",
     "start_time": "2022-07-29T11:15:28.986978Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Lambda\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca7b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a1df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3cb2c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:15:33.492500Z",
     "start_time": "2022-07-29T11:15:33.478135Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.size = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.size = 0\n",
    "        self.states.clear()\n",
    "        self.actions.clear()\n",
    "        self.rewards.clear()\n",
    "\n",
    "    def remember(self, state, action_onehot, reward ):\n",
    "        self.size +=1\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action_onehot)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "#     def sample(self, batch_size=64):\n",
    "#         max_mem = min(self.buffer_counter, self.buffer_size)\n",
    "\n",
    "#         batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "#         states = self.state_memory[batch]\n",
    "#         states_ = self.new_state_memory[batch]\n",
    "#         actions = self.action_memory[batch]\n",
    "#         rewards = self.reward_memory[batch]\n",
    "#         dones = self.done_memory[batch]\n",
    "\n",
    "#         return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "456dff8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T11:15:33.524501Z",
     "start_time": "2022-07-29T11:15:33.494501Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.agent import Agent\n",
    "from src.utils.networks import CommonLayer\n",
    "    \n",
    "\n",
    "class A2CAgent(Agent):\n",
    "    def __init__(self,\n",
    "                environment,\n",
    "                gamma = 0.99,\n",
    "                policy=\"mlp\",\n",
    "                actor_optimizer=RMSprop(0.0001),\n",
    "                critic_optimizer=RMSprop(0.0001),\n",
    "                std_bound = [1e-2, 1.0],\n",
    "                batch_size=64\n",
    "                ):\n",
    "        super(A2CAgent, self).__init__(environment,args=locals())\n",
    "        \n",
    "        # Args\n",
    "        self.gamma = gamma\n",
    "        self.std_bound = std_bound\n",
    "        self.batch_size = batch_size\n",
    "        self.policy = policy \n",
    "        self.actor_optimizer=actor_optimizer\n",
    "        self.critic_optimizer=critic_optimizer\n",
    "\n",
    "        # Bootstrap\n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        self._add_models_to_config([self.actor,self.critic])\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        X_input = Input(shape=self.observation_shape) \n",
    "        X = CommonLayer(X_input,self.policy)\n",
    "        \n",
    "        action = Dense(self.n_actions, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "        value = Dense(1, kernel_initializer='he_uniform')(X)\n",
    "        \n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            action = Dense(self.n_actions, activation=\"softmax\", kernel_initializer='he_uniform')(X)\n",
    "            self.actor = Model(inputs = X_input, outputs = action)\n",
    "            self.actor.compile(loss='categorical_crossentropy', optimizer=self.actor_optimizer)\n",
    "        else:\n",
    "            mu = Dense(self.n_actions, activation=\"tanh\", kernel_initializer='he_uniform')(X)\n",
    "            mu = Lambda(lambda x: x * self.action_bound)(mu)\n",
    "            sigma = Dense(self.n_actions, activation=\"softplus\", kernel_initializer='he_uniform')(X)\n",
    "            \n",
    "            self.actor = Model(inputs = X_input, outputs = Concatenate()([mu,sigma]))\n",
    "            self.actor.compile(loss=self.continuous_actor_loss, optimizer=self.actor_optimizer)\n",
    "        \n",
    "        self.critic = Model(inputs = X_input, outputs = value)\n",
    "        self.critic.compile(loss='mse', optimizer=self.critic_optimizer)\n",
    "    \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = ReplayBuffer()\n",
    "        \n",
    "    def log_pdf(self,mu, sigma, action):\n",
    "        std = tf.clip_by_value(sigma, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / var - 0.5 * tf.math.log(\n",
    "            var * 2 * np.pi\n",
    "        )\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "    \n",
    "    def continuous_actor_loss(self, y_true, y_pred):\n",
    "        actions, advantages = y_true[:, :self.n_actions], y_true[:, self.n_actions:]\n",
    "        mu,sigma = y_pred[:,:1], y_pred[:,1:]\n",
    "        log_policy_pdf = self.log_pdf(mu,sigma,actions)\n",
    "        loss_policy = log_policy_pdf * advantages\n",
    "        \n",
    "        return tf.reduce_sum(-loss_policy)\n",
    "\n",
    "    def act(self,state):\n",
    "        if self.action_space_mode == \"discrete\":\n",
    "            prediction = self.actor.predict(state)[0]\n",
    "            action = np.random.choice(self.n_actions, p=prediction)\n",
    "            action_onehot = np.zeros([self.n_actions])\n",
    "            action_onehot[action] = 1\n",
    "        else:\n",
    "            prediction = self.actor.predict(state)[0]\n",
    "            mu = prediction[0]\n",
    "            sigma = prediction[1]\n",
    "            action = np.random.normal(mu, sigma,self.n_actions)\n",
    "            action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "            action_onehot = action\n",
    "        return action, action_onehot, prediction\n",
    "    \n",
    "    def discount_rewards(self, reward):\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * self.gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        \n",
    "        return discounted_r\n",
    "    \n",
    "    def replay(self):\n",
    "\n",
    "        if self.buffer.size > 1:\n",
    "            # reshape memory to appropriate shape for training\n",
    "            states = np.vstack(self.buffer.states)\n",
    "            actions = np.vstack(self.buffer.actions)\n",
    "\n",
    "            # Compute discounted rewards\n",
    "            discounted_r = self.discount_rewards(self.buffer.rewards)\n",
    "\n",
    "            # Get Critic network predictions\n",
    "            values = self.critic.predict(states)[:, 0]\n",
    "            # Compute advantages\n",
    "            advantages = discounted_r - values\n",
    "            # training Actor and Critic networks\n",
    "\n",
    "\n",
    "            if self.action_space_mode == \"discrete\":\n",
    "                self.actor.fit(states, actions, sample_weight=advantages, epochs=1, verbose=0)\n",
    "            else:\n",
    "                self.actor.fit(states,np.concatenate([actions,np.reshape(advantages,newshape=(len(advantages),1))],axis=1), epochs=1,verbose=0)\n",
    "\n",
    "            self.critic.fit(states, discounted_r, epochs=1, verbose=0)\n",
    "            # reset training memory\n",
    "            self.buffer.reset()\n",
    "        \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50):\n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    " \n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        \n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                \n",
    "                state = np.expand_dims(state, axis=0)\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                self.buffer.remember(state, action_onehot, reward)\n",
    "                # Update current state\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                timestep +=1\n",
    "                \n",
    "                if self.buffer.size >= self.batch_size:\n",
    "                    self.replay()\n",
    "            \n",
    "            # Episode ended\n",
    "            self.running_reward.step(score)\n",
    "            episode += 1\n",
    "            \n",
    "            self.learning_log.episode(\n",
    "                log_each_n_episodes,\n",
    "                score,\n",
    "                self.running_reward.reward, \n",
    "                log_level=log_level\n",
    "            )\n",
    "            # If done stop\n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                break\n",
    "                \n",
    "            # Else learn more\n",
    "            self.replay()\n",
    "        \n",
    "        # End of trainig\n",
    "        self.env.close()\n",
    "        \n",
    "        if plot_results:\n",
    "            self.plot_learning_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1076056",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-29T11:15:28.961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| CartPole-v1\n",
      "| Action space: Discrete with high state-space\n",
      "| Environment beated threshold: 200\n",
      "| Dev notes:\n",
      "|   * Agents that track State/Action combinations like \n",
      "|     Q learning will fail due to high state space\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n",
      "Episode * 50 * Moving Avg Reward is ==> 23.980 * Last Reward was ==> 11.000\n",
      "Episode * 100 * Moving Avg Reward is ==> 26.140 * Last Reward was ==> 45.000\n",
      "Episode * 150 * Moving Avg Reward is ==> 29.220 * Last Reward was ==> 56.000\n",
      "Episode * 200 * Moving Avg Reward is ==> 26.960 * Last Reward was ==> 26.000\n",
      "Episode * 250 * Moving Avg Reward is ==> 32.220 * Last Reward was ==> 63.000\n",
      "Episode * 300 * Moving Avg Reward is ==> 35.940 * Last Reward was ==> 11.000\n",
      "Episode * 350 * Moving Avg Reward is ==> 34.560 * Last Reward was ==> 33.000\n",
      "Episode * 400 * Moving Avg Reward is ==> 40.880 * Last Reward was ==> 53.000\n",
      "Episode * 450 * Moving Avg Reward is ==> 38.420 * Last Reward was ==> 53.000\n",
      "Episode * 500 * Moving Avg Reward is ==> 41.920 * Last Reward was ==> 25.000\n",
      "Episode * 550 * Moving Avg Reward is ==> 47.300 * Last Reward was ==> 34.000\n",
      "Episode * 600 * Moving Avg Reward is ==> 46.660 * Last Reward was ==> 98.000\n",
      "Episode * 650 * Moving Avg Reward is ==> 44.640 * Last Reward was ==> 50.000\n",
      "Episode * 700 * Moving Avg Reward is ==> 48.580 * Last Reward was ==> 43.000\n",
      "Episode * 750 * Moving Avg Reward is ==> 59.320 * Last Reward was ==> 53.000\n",
      "Episode * 800 * Moving Avg Reward is ==> 68.300 * Last Reward was ==> 71.000\n",
      "Episode * 850 * Moving Avg Reward is ==> 71.640 * Last Reward was ==> 60.000\n",
      "Episode * 900 * Moving Avg Reward is ==> 65.060 * Last Reward was ==> 82.000\n",
      "Episode * 950 * Moving Avg Reward is ==> 61.520 * Last Reward was ==> 41.000\n",
      "Episode * 1000 * Moving Avg Reward is ==> 86.260 * Last Reward was ==> 59.000\n",
      "Episode * 1050 * Moving Avg Reward is ==> 90.720 * Last Reward was ==> 40.000\n"
     ]
    }
   ],
   "source": [
    "from src.environments.discrete.cartpole import environment\n",
    "agent = A2CAgent(environment)\n",
    "agent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21a3f5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-07-29T11:15:28.962Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "agent = A2CAgent(environment)\n",
    "agent.learn()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d29358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
