{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602e667f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:45:48.453797Z",
     "start_time": "2022-08-02T18:45:48.427797Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca80c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:45:48.549796Z",
     "start_time": "2022-08-02T18:45:48.455797Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c0862e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:45:48.741797Z",
     "start_time": "2022-08-02T18:45:48.550797Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from src.environments.discrete.cartpole import environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb52acf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:50:04.042755Z",
     "start_time": "2022-08-02T18:50:04.017143Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.agents.ppo.buffer import PpoBuffer\n",
    "from src.agents.ppo.networks import get_actor_model,get_critic_model,act\n",
    "\n",
    "from src.utils.networks import gaussian_likelihood\n",
    "from src.utils.temporal_difference import discounted_rewards\n",
    "from src.utils.temporal_difference import generalized_advantage_estimation\n",
    "\n",
    "tf.compat.v1.disable_eager_execution() \n",
    "\n",
    "class PpoAgent(Agent):\n",
    "    def __init__(self, \n",
    "                environment,\n",
    "                epochs = 10, # training epochs\n",
    "                shuffle=True,\n",
    "                batch_size = 1000,\n",
    "                gamma=0.99,\n",
    "                lamda=0.9,\n",
    "                loss_clipping = 0.2,\n",
    "                loss_entropy = 0.001,\n",
    "                actor_optimizer=Adam,\n",
    "                critic_optimizer=Adam,\n",
    "                actor_learning_rate=0.001,\n",
    "                critic_learning_rate=0.001,\n",
    "                actor_kernel_initializer=False, #tf.random_normal_initializer(stddev=0.01),\n",
    "                critic_kernel_initializer=False, #\"he_uniform\",\n",
    "                policy=\"mlp\"\n",
    "                ):\n",
    "        super(PpoAgent, self).__init__(environment,args=locals())\n",
    "        \n",
    "        # HP\n",
    "        self.epochs=epochs  \n",
    "        self.shuffle=shuffle\n",
    "        self.batch_size=batch_size \n",
    "        self.gamma=gamma\n",
    "        self.lamda=lamda\n",
    "        self.loss_clipping=loss_clipping\n",
    "        self.loss_entropy=loss_entropy \n",
    "        self.actor_optimizer=actor_optimizer\n",
    "        self.actor_learning_rate=actor_learning_rate\n",
    "        self.actor_kernel_initializer=actor_kernel_initializer\n",
    "        self.critic_optimizer=critic_optimizer\n",
    "        self.critic_learning_rate=critic_learning_rate\n",
    "        self.critic_kernel_initializer=critic_kernel_initializer\n",
    "        self.policy=policy\n",
    "        \n",
    "        self.log_std = -0.5 * np.ones(self.n_actions, dtype=np.float32)\n",
    "        self.std = np.exp(self.log_std)\n",
    "        \n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        self.__init_actions()\n",
    "        \n",
    "        self._add_models_to_config([self.actor,self.critic])\n",
    "        self._init_tensorboard()\n",
    "        \n",
    "    def __init_networks(self):\n",
    "        self.actor = get_actor_model(\n",
    "            self.observation_shape,\n",
    "            self.n_actions,\n",
    "            self.loss_clipping,\n",
    "            self.loss_entropy,\n",
    "            optimizer=self.actor_optimizer,\n",
    "            learning_rate=self.actor_learning_rate,\n",
    "            kernel_initializer=self.actor_kernel_initializer,\n",
    "            policy=self.policy,\n",
    "            continuous=self.action_space_mode == 'continuous',\n",
    "        )\n",
    "        \n",
    "        self.critic = get_critic_model(\n",
    "            self.observation_shape,\n",
    "            self.loss_clipping,\n",
    "            optimizer=self.critic_optimizer,\n",
    "            learning_rate=self.critic_learning_rate,\n",
    "            kernel_initializer=self.critic_kernel_initializer,\n",
    "            policy=self.policy,\n",
    "        )\n",
    "    \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = PpoBuffer()\n",
    "        \n",
    "    def __init_actions(self):\n",
    "        self.act = act(\n",
    "            self.actor,\n",
    "            self.n_actions, \n",
    "            std=self.std, \n",
    "            log_std=self.log_std, \n",
    "            continuous=self.action_space_mode == 'continuous'\n",
    "        )\n",
    "        \n",
    "        self.discount_rewards = discounted_rewards(self.gamma)\n",
    "        self.get_gaes = generalized_advantage_estimation(self.gamma,self.lamda)\n",
    "        self.gaussian_likelihood = gaussian_likelihood(self.n_actions,lib=\"numpy\")\n",
    "        \n",
    "        \n",
    "    def critic_predict(self, state):\n",
    "        #print(state.shape,np.zeros((state.shape[0], 1)).shape)\n",
    "        return self.critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "        \n",
    "    def replay(self,buffer):\n",
    "        print('replay')\n",
    "        \n",
    "        states = np.array(buffer.states,dtype=np.float32)\n",
    "        actions = np.array(buffer.actions,dtype=np.float32)\n",
    "        rewards = np.array(buffer.rewards,dtype=np.float32)\n",
    "        predictions = np.array(buffer.predictions,dtype=np.float32)\n",
    "        dones= buffer.dones\n",
    "        next_states= np.array(buffer.next_states,dtype=np.float32)\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.critic_predict(states)\n",
    "        next_values = self.critic_predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        #discounted_r = self.discount_rewards(rewards)\n",
    "        #advantages = np.vstack(discounted_r - values)\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "    \n",
    "        print('advantages',advantages[0])\n",
    "        print('preds',predictions[0],np.mean(predictions))\n",
    "        print('actions',actions[0],np.mean(actions))\n",
    "        print('states',states[0])\n",
    "        y_true = np.array(np.hstack([advantages, predictions, actions]),dtype=np.float32)\n",
    "        \n",
    "        print('states')\n",
    "        print(states[0:3], type(states))\n",
    "        print('y_true')\n",
    "        print(y_true[0:3])\n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.actor.fit(states, y_true, epochs=self.epochs, verbose=1, shuffle=self.shuffle)\n",
    "        c_loss = self.critic.fit([states, values], target, epochs=self.epochs, verbose=1, shuffle=self.shuffle)\n",
    "\n",
    "        print()\n",
    "        print('actor loss', np.mean(a_loss.history['loss']))\n",
    "        print('critic loss',np.mean(c_loss.history['loss']))\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "#         self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "#         self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        \n",
    "        if self.action_space_mode == \"continuous\":\n",
    "            # calculate loss parameters (should be done in loss, but couldn't find working way how to do that with disabled eager execution)\n",
    "            pred = self.actor.predict(states)\n",
    "            log_std = -0.5 * np.ones(self.n_actions, dtype=np.float32)\n",
    "            logp = self.gaussian_likelihood(actions, pred, log_std)\n",
    "            approx_kl = np.mean(predictions - logp)\n",
    "            approx_ent = np.mean(-logp)\n",
    "            \n",
    "#             self.writer.add_scalar('Data/approx_kl_per_replay', approx_kl, self.replay_count)\n",
    "#             self.writer.add_scalar('Data/approx_ent_per_replay', approx_ent, self.replay_count)\n",
    "        \n",
    "        self.replay_count += 1\n",
    "        \n",
    "    def learn_single_process(self,timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50,):\n",
    "        episode = 0\n",
    "        episodes=100000\n",
    "        state = self.env.reset()\n",
    "        #state = np.expand_dims(state,axis=0) #np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score = False, 0\n",
    "        avg = []\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            self.buffer.reset()\n",
    "                               \n",
    "            for t in range(self.batch_size):\n",
    "                # self.env.render()\n",
    "                # Actor picks an action\n",
    "                \n",
    "                state = np.expand_dims(state,axis=0)   \n",
    "                \n",
    "                # Action data is action for continuous and onehot for discrete\n",
    "                # prediction_data is logp_t[0] for continuous and prediction for discrete\n",
    "                action, action_data, prediction_data = self.act(state)\n",
    "                \n",
    "                #print(\"action\",action, \"action_data\",action_data, \"prediction_data\",prediction_data)\n",
    "                \n",
    "                #print(state,action,action_data,prediction_data)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                \n",
    "                # Memorize (state, next_states, action, reward, done, logp_ts) for training\n",
    "                \n",
    "                self.buffer.states.append(state)\n",
    "                self.buffer.next_states.append(np.expand_dims(next_state,axis=0))\n",
    "                # on continuous add action on discrete one_hots\n",
    "                self.buffer.actions.append(action_data)\n",
    "                self.buffer.rewards.append(reward)\n",
    "                self.buffer.dones.append(done)\n",
    "                self.buffer.predictions.append(prediction_data)\n",
    "                \n",
    "                # Update current state shape\n",
    "                #state = np.expand_dims(state,axis=0) #np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    \n",
    "                    avg.append(score)\n",
    "                    if episode % 50 ==0:\n",
    "                        print('episode',episode,'score',score, 'avg', np.mean(avg[-100:]))\n",
    "                    episode += 1\n",
    "#                     average, SAVING = self.PlotModel(score, self.episode)\n",
    "#                     print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/average_score',  average, self.episode)\n",
    "                    \n",
    "                     # Step reward, tensorboard log score, print progress\n",
    "                    \n",
    "                    #self.on_learn_episode_end(score,log_each_n_episodes,log_level,success_threshold)\n",
    "                    state, done, score = self.env.reset(), False, 0\n",
    "                    #state = np.expand_dims(state,axis=0) #np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            self.replay(self.buffer)\n",
    "            \n",
    "            if episode >= episodes:\n",
    "                break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "        \n",
    "    def learn_multi_process(self,timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50,n_workers=cpu_count()):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, success_threshold=False, log_level=1, log_each_n_episodes=50, n_workers=cpu_count()):\n",
    "        self.replay_count = 0\n",
    "        if n_workers == 1:\n",
    "            self.learn_single_process(timesteps, plot_results, reset, success_threshold, log_level, log_each_n_episodes)\n",
    "        else: \n",
    "            self.learn_multi_process(timesteps, plot_results, reset, success_threshold, log_level, log_each_n_episodes,n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf62cabf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:50:04.681228Z",
     "start_time": "2022-08-02T18:50:04.673222Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from src.environments.discrete.cartpole import environment\n",
    "\n",
    "#agent = PpoAgent(environment,batch_size=1024)\n",
    "#agent.learn(log_each_n_episodes=100,n_workers=1, success_threshold=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec77eb0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:50:41.127889Z",
     "start_time": "2022-08-02T18:50:05.376728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    | ---------------------------------\n",
      "    | InvertedPendulumBulletEnv-v0\n",
      "    | \n",
      "    | Action space: Continuous with high action-space\n",
      "    | Environment beated threshold: 200\n",
      "    | Dev notes:\n",
      "    |   * Doesn't work with multiprocessing\n",
      "    | ----------------------------------------------------------   \n",
      "\n",
      "    \n",
      "episode 0 score 26.0 avg 26.0\n",
      "replay\n",
      "advantages [0.90173694]\n",
      "preds [-0.46798822] -0.58594644\n",
      "actions [0.1902129] 0.009857004\n",
      "states [0.         0.         0.9999627  0.00863607 0.        ]\n",
      "states\n",
      "[[ 0.0000000e+00  0.0000000e+00  9.9996269e-01  8.6360741e-03\n",
      "   0.0000000e+00]\n",
      " [ 4.1279980e-04  2.5018169e-02  9.9996936e-01  7.8284256e-03\n",
      "  -4.8950087e-02]\n",
      " [ 1.7069184e-03  7.8431427e-02  9.9998635e-01  5.2264696e-03\n",
      "  -1.5769768e-01]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[ 0.9017369  -0.46798822  0.1902129 ]\n",
      " [ 0.8776804  -0.6387278   0.4023589 ]\n",
      " [ 0.8506782  -0.7494096   0.49329892]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 348us/sample - loss: 0.3309\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 108us/sample - loss: 0.3309\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 111us/sample - loss: 0.3309\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 106us/sample - loss: 0.3309\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3309\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3309\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3309\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 0.3309\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 0.3309\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 0.3309\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 318us/sample - loss: 246.1522\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 58us/sample - loss: 23.3336\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 23.5839\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 55us/sample - loss: 23.3397\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 53us/sample - loss: 23.3320\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 58us/sample - loss: 23.3320\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 57us/sample - loss: 23.3320\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 23.3320\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 55us/sample - loss: 23.3320\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 57us/sample - loss: 23.3320\n",
      "\n",
      "actor loss 0.3308733350364491\n",
      "critic loss 45.640163159370424\n",
      "\n",
      "\n",
      "episode 50 score 27.0 avg 27.84313725490196\n",
      "replay\n",
      "advantages [0.58607538]\n",
      "preds [-0.62049353] -0.5884453\n",
      "actions [-0.40725532] -0.017076949\n",
      "states [ 0.02022672  0.19963014  0.9989784  -0.04519067 -0.46103227]\n",
      "states\n",
      "[[ 0.02022672  0.19963014  0.9989784  -0.04519067 -0.46103227]\n",
      " [ 0.02264886  0.14679618  0.99868804 -0.05120717 -0.36506099]\n",
      " [ 0.02551385  0.17363605  0.9982919  -0.05842297 -0.43798047]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[ 0.58607537 -0.62049353 -0.40725532]\n",
      " [ 0.439009   -0.47806266  0.18640588]\n",
      " [ 0.3991966  -0.672492   -0.45408255]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 71us/sample - loss: 0.3309\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 101us/sample - loss: 0.3309\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 102us/sample - loss: 0.3309\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 74us/sample - loss: 0.3309\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3309\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 0.3309\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 61us/sample - loss: 0.3309\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 0.3309\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3309\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 0.3309\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 54us/sample - loss: 30.0020\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 55us/sample - loss: 22.5307\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 57us/sample - loss: 21.1784\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 57us/sample - loss: 20.9762\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 54us/sample - loss: 20.9452\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 54us/sample - loss: 20.9359\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 20.9349\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 56us/sample - loss: 20.9348\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 57us/sample - loss: 20.9349\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 56us/sample - loss: 20.9350\n",
      "\n",
      "actor loss 0.3308707332005724\n",
      "critic loss 22.030793669819833\n",
      "\n",
      "\n",
      "episode 100 score 43.0 avg 27.15\n",
      "replay\n",
      "advantages [0.50191348]\n",
      "preds [-0.5602921] -0.5847814\n",
      "actions [0.33697414] 0.030544803\n",
      "states [0.02188429 0.11812893 0.99686754 0.07908893 0.17790143]\n",
      "states\n",
      "[[0.02188429 0.11812893 0.99686754 0.07908893 0.17790143]\n",
      " [0.02451974 0.15972419 0.9967117  0.08103024 0.11803395]\n",
      " [0.02669335 0.13173375 0.996433   0.08438774 0.20418462]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[ 0.5019135  -0.5602921   0.33697414]\n",
      " [ 0.44537437 -0.47205755 -0.18321323]\n",
      " [ 0.38191858 -0.5569882   0.3331829 ]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 87us/sample - loss: 0.2792\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 89us/sample - loss: 0.2792\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 71us/sample - loss: 0.2792\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 0.2792\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.2792\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 74us/sample - loss: 0.2792\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 0.2792\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 78us/sample - loss: 0.2792\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 73us/sample - loss: 0.2792\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 76us/sample - loss: 0.2792\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 25.9761\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 86.1140\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 21.7547\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 21.7280\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 21.7178\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 21.7088\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 21.7011\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 21.6948\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 21.6897\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 21.6869\n",
      "\n",
      "actor loss 0.27923409587237985\n",
      "critic loss 28.57719116508961\n",
      "\n",
      "\n",
      "episode 150 score 28.0 avg 26.16\n",
      "replay\n",
      "advantages [0.64702213]\n",
      "preds [-0.42061433] -0.5905335\n",
      "actions [0.05521774] 0.10315698\n",
      "states [-0.02206976 -0.18462424  0.9962487   0.08653568  0.55235523]\n",
      "states\n",
      "[[-0.02206976 -0.18462424  0.9962487   0.08653568  0.55235523]\n",
      " [-0.02505091 -0.18067594  0.9953848   0.09596422  0.5738222 ]\n",
      " [-0.02785935 -0.17020847  0.99441236  0.10556533  0.58486503]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[ 0.6470221  -0.42061433  0.05521774]\n",
      " [ 0.5862192  -0.41931176  0.10690242]\n",
      " [ 0.517978   -0.8460137   0.6508887 ]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 94us/sample - loss: 0.3326\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 0s 95us/sample - loss: 0.3326\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 80us/sample - loss: 0.3326\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 0.3326\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 0.3326\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 0.3326\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 0.3326\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 0.3326\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 0.3326\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3326\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 19.0987\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 19.0012\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 18.9840\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 18.9699\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 18.9590\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 18.9499\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 18.9478\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 18.9508\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 18.9472\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 18.9530\n",
      "\n",
      "actor loss 0.332649026112631\n",
      "critic loss 18.976146069169044\n",
      "\n",
      "\n",
      "episode 200 score 25.0 avg 23.13\n",
      "replay\n",
      "advantages [-1.82465012]\n",
      "preds [-0.4877426] -0.57945365\n",
      "actions [0.36607558] 0.15054226\n",
      "states [ 0.04433046  0.5096462   0.98517805 -0.17153482 -1.3422103 ]\n",
      "states\n",
      "[[ 0.04433046  0.5096462   0.98517805 -0.17153482 -1.3422103 ]\n",
      " [ 0.05363625  0.5639869   0.9805943  -0.19604808 -1.511442  ]\n",
      " [ 0.          0.          0.9985802  -0.05326881  0.        ]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[-1.8246502  -0.4877426   0.36607558]\n",
      " [-2.2136989  -0.41901734  0.13346517]\n",
      " [ 0.85379314 -0.74733657  0.63263   ]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 94us/sample - loss: 0.3353\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 94us/sample - loss: 0.3353\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 93us/sample - loss: 0.3353\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 76us/sample - loss: 0.3353\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 75us/sample - loss: 0.3353\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3353\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 0.3353\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3353\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3353\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3353\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 60us/sample - loss: 15.6688\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 15.5815\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 15.5429\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 58us/sample - loss: 15.5161\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 15.5166\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.5121\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.5123\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.5148\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 15.5150\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 15.5117\n",
      "\n",
      "actor loss 0.3352843624074012\n",
      "critic loss 15.539180865883827\n",
      "\n",
      "\n",
      "replay\n",
      "advantages [0.97293348]\n",
      "preds [-0.7834465] -0.5889683\n",
      "actions [0.6544004] 0.12906578\n",
      "states [0.02020994 0.06549249 0.99740076 0.07205366 0.22792903]\n",
      "states\n",
      "[[ 0.02020994  0.06549249  0.99740076  0.07205366  0.22792903]\n",
      " [ 0.02268223  0.14983551  0.99730927  0.07330949  0.07631237]\n",
      " [ 0.02620552  0.21353267  0.9973475   0.07278741 -0.03172573]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[ 0.9729335  -0.7834465   0.6544004 ]\n",
      " [ 0.94608545 -0.5993068   0.5008203 ]\n",
      " [ 0.915953   -0.7000279   0.59129786]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3342\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3342\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 75us/sample - loss: 0.3342\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 75us/sample - loss: 0.3342\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 0.3342\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3342\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3342\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 0.3342\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3342\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 70us/sample - loss: 0.3342\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 17.5604\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 17.4597\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 17.4176\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 60us/sample - loss: 17.4005\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 61us/sample - loss: 17.4063\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 17.4039\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 17.4030\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 59us/sample - loss: 17.4062\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 58us/sample - loss: 17.4030\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 17.4013\n",
      "\n",
      "actor loss 0.3341832705307752\n",
      "critic loss 17.426201340556144\n",
      "\n",
      "\n",
      "episode 250 score 22.0 avg 22.95\n",
      "replay\n",
      "advantages [-1.9701508]\n",
      "preds [-0.4203282] -0.58060354\n",
      "actions [0.08085912] 0.09849936\n",
      "states [ 0.03888434 -0.10733264  0.9857788   0.16804796  1.0179741 ]\n",
      "states\n",
      "[[ 0.03888434 -0.10733264  0.9857788   0.16804796  1.0179741 ]\n",
      " [ 0.0371847  -0.10300831  0.98267156  0.1853555   1.0657258 ]\n",
      " [ 0.          0.          0.99844253  0.05578997  0.        ]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[-1.9701508  -0.4203282   0.08085912]\n",
      " [-2.3488343  -0.43403864  0.21823926]\n",
      " [ 1.0388424  -0.4958865  -0.12510428]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 104us/sample - loss: 0.3337\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 107us/sample - loss: 0.3337\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 80us/sample - loss: 0.3337\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 74us/sample - loss: 0.3337\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3337\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3337\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 82us/sample - loss: 0.3337\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3337\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 75us/sample - loss: 0.3337\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3337\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 54us/sample - loss: 18.2441\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 54us/sample - loss: 18.1409\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 57us/sample - loss: 18.0797\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 0s 61us/sample - loss: 18.0662\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 18.0688\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 55us/sample - loss: 18.0690\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 61us/sample - loss: 18.0617\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 55us/sample - loss: 18.0762\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 18.0669\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 58us/sample - loss: 18.0685\n",
      "\n",
      "actor loss 0.3336546417558566\n",
      "critic loss 18.09419158399105\n",
      "\n",
      "\n",
      "episode 300 score 16.0 avg 24.7\n",
      "replay\n",
      "advantages [0.8732991]\n",
      "preds [-0.41924423] -0.58134085\n",
      "actions [0.13417193] 0.1531798\n",
      "states [0.         0.         0.9985525  0.05378614 0.        ]\n",
      "states\n",
      "[[ 0.0000000e+00  0.0000000e+00  9.9855250e-01  5.3786136e-02\n",
      "   0.0000000e+00]\n",
      " [ 2.5814815e-04  1.5645342e-02  9.9856484e-01  5.3556282e-02\n",
      "  -1.3950666e-02]\n",
      " [ 2.0568182e-03  1.0901030e-01  9.9872839e-01  5.0414551e-02\n",
      "  -1.9066590e-01]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[ 0.8732991  -0.41924423  0.13417193]\n",
      " [ 0.8247879  -0.85504013  0.71561897]\n",
      " [ 0.7703421  -0.52009964 -0.12365004]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3340\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 74us/sample - loss: 0.3340\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 0.3340\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3340\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3340\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 70us/sample - loss: 0.3340\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 70us/sample - loss: 0.3340\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 70us/sample - loss: 0.3340\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 77us/sample - loss: 0.3340\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 77us/sample - loss: 0.3340\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 15.8614\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 15.7479\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 15.6901\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 61us/sample - loss: 15.6916\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.6933\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 61us/sample - loss: 15.6932\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.6947\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 15.6884\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 15.6912\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 15.6899\n",
      "\n",
      "actor loss 0.33401354528032245\n",
      "critic loss 15.714180612564087\n",
      "\n",
      "\n",
      "episode 350 score 22.0 avg 23.76\n",
      "replay\n",
      "advantages [-0.34871672]\n",
      "preds [-0.81480277] -0.5797812\n",
      "actions [-0.41919804] 0.13223487\n",
      "states [ 0.07916842  0.31693518  0.996959   -0.07792806 -0.63918275]\n",
      "states\n",
      "[[ 0.07916842  0.31693518  0.996959   -0.07792806 -0.63918275]\n",
      " [ 0.08352015  0.26374155  0.99620533 -0.08703439 -0.55378765]\n",
      " [ 0.08942615  0.3579394   0.9950037  -0.09983826 -0.7794073 ]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[-0.3487167  -0.81480277 -0.41919804]\n",
      " [-0.539879   -0.8463071   0.68123734]\n",
      " [-0.754427   -0.49448872 -0.1152807 ]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 118us/sample - loss: 0.3345\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 94us/sample - loss: 0.3345\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 81us/sample - loss: 0.3345\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 74us/sample - loss: 0.3345\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 74us/sample - loss: 0.3345\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 82us/sample - loss: 0.3345\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 79us/sample - loss: 0.3345\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 78us/sample - loss: 0.3345\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 71us/sample - loss: 0.3345\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3345\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 16.2801\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 16.1696\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 16.0997\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 16.1007\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 16.0987\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 16.1000\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 61us/sample - loss: 16.0984\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 61us/sample - loss: 16.1054\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 16.1012\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 16.1024\n",
      "\n",
      "actor loss 0.33448711396194997\n",
      "critic loss 16.125613552331924\n",
      "\n",
      "\n",
      "episode 400 score 27.0 avg 24.21\n",
      "replay\n",
      "advantages [1.14274893]\n",
      "preds [-0.51746494] -0.57695675\n",
      "actions [0.3617211] 0.08980404\n",
      "states [ 0.04095447  0.2742066   0.9992618   0.03841664 -0.22866812]\n",
      "states\n",
      "[[ 0.04095447  0.2742066   0.9992618   0.03841664 -0.22866812]\n",
      " [ 0.04624592  0.3206937   0.99944675  0.03325996 -0.31272715]\n",
      " [ 0.05076755  0.27403826  0.9995527   0.02990505 -0.20342933]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[ 1.142749   -0.51746494  0.3617211 ]\n",
      " [ 1.1416731  -0.66853786 -0.3360598 ]\n",
      " [ 1.1404657  -0.4202201   0.06177122]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 71us/sample - loss: 0.3321\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 71us/sample - loss: 0.3321\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 73us/sample - loss: 0.3321\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 73us/sample - loss: 0.3321\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 80us/sample - loss: 0.3321\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 76us/sample - loss: 0.3321\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 73us/sample - loss: 0.3321\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 0.3321\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 70us/sample - loss: 0.3321\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 74us/sample - loss: 0.3321\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 17.1344\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 17.0013\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 16.9162\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 16.9192\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 16.9154\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 16.9212\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 16.9153\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 16.9181\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 16.9211\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 16.9193\n",
      "\n",
      "actor loss 0.33209075033664703\n",
      "critic loss 16.94814921915531\n",
      "\n",
      "\n",
      "episode 450 score 29.0 avg 24.75\n",
      "replay\n",
      "advantages [-1.27068234]\n",
      "preds [-0.41895315] -0.5880228\n",
      "actions [0.11274725] 0.11383117\n",
      "states [-0.0108423   0.06505385  0.98557365  0.1692471   0.49404195]\n",
      "states\n",
      "[[-0.0108423   0.06505385  0.98557365  0.1692471   0.49404195]\n",
      " [-0.00963532  0.07315014  0.9840421   0.17793582  0.53470874]\n",
      " [-0.00740844  0.1349624   0.98264027  0.18552132  0.46751237]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[-1.2706823  -0.41895315  0.11274725]\n",
      " [-1.5723263  -0.6408695   0.5201165 ]\n",
      " [-1.9108716  -0.47432274  0.31789267]]\n",
      "Train on 1024 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 93us/sample - loss: 0.3343\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 102us/sample - loss: 0.3343\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 76us/sample - loss: 0.3343\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3343\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3343\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3343\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3343\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 75us/sample - loss: 0.3343\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3343\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3343\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 16.0513\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.8981\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.8366\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.8360\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 15.8498\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 65us/sample - loss: 15.8368\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 15.8389\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 66us/sample - loss: 15.8436\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 62us/sample - loss: 15.8436\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 15.8470\n",
      "\n",
      "actor loss 0.3342961736023426\n",
      "critic loss 15.86817529797554\n",
      "\n",
      "\n",
      "replay\n",
      "advantages [-1.60231295]\n",
      "preds [-0.5612245] -0.58089864\n",
      "actions [-0.22630444] 0.08500254\n",
      "states [-0.02046729 -0.12489657  0.98424953 -0.17678486 -0.5398339 ]\n",
      "states\n",
      "[[-0.02046729 -0.12489657  0.98424953 -0.17678486 -0.5398339 ]\n",
      " [-0.02290708 -0.14786601  0.98260134 -0.18572712 -0.5510857 ]\n",
      " [-0.02467427 -0.10710258  0.98040146 -0.1970102  -0.69670427]] <class 'numpy.ndarray'>\n",
      "y_true\n",
      "[[-1.6023129  -0.5612245  -0.22630444]\n",
      " [-1.9425474  -0.4512374   0.25140747]\n",
      " [-2.3244042  -0.5964176  -0.26410997]]\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 104us/sample - loss: 0.3337\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 87us/sample - loss: 0.3337\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 79us/sample - loss: 0.3337\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 75us/sample - loss: 0.3337\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 79us/sample - loss: 0.3337\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 75us/sample - loss: 0.3337\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 70us/sample - loss: 0.3337\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 0.3337\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 69us/sample - loss: 0.3337\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 72us/sample - loss: 0.3337\n",
      "Train on 1024 samples\n",
      "Epoch 1/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 16.1741\n",
      "Epoch 2/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 15.9908\n",
      "Epoch 3/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 15.9509\n",
      "Epoch 4/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 15.9355\n",
      "Epoch 5/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 15.9344\n",
      "Epoch 6/10\n",
      "1024/1024 [==============================] - 0s 67us/sample - loss: 15.9349\n",
      "Epoch 7/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 15.9372\n",
      "Epoch 8/10\n",
      "1024/1024 [==============================] - 0s 68us/sample - loss: 15.9343\n",
      "Epoch 9/10\n",
      "1024/1024 [==============================] - 0s 64us/sample - loss: 15.9346\n",
      "Epoch 10/10\n",
      "1024/1024 [==============================] - 0s 63us/sample - loss: 15.9340\n",
      "\n",
      "actor loss 0.3337256824364886\n",
      "critic loss 15.966064351797105\n",
      "\n",
      "\n",
      "episode 500 score 17.0 avg 24.48\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minverted_pendulum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environment\n\u001b[0;32m      2\u001b[0m agent_1 \u001b[38;5;241m=\u001b[39m PpoAgent(\n\u001b[0;32m      3\u001b[0m     environment,\n\u001b[0;32m      4\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      5\u001b[0m     actor_learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      6\u001b[0m     critic_learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m,\n\u001b[0;32m      7\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43magent_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_each_n_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mPpoAgent.learn\u001b[1;34m(self, timesteps, plot_results, reset, success_threshold, log_level, log_each_n_episodes, n_workers)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn_single_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuccess_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_each_n_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn_multi_process(timesteps, plot_results, reset, success_threshold, log_level, log_each_n_episodes,n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mPpoAgent.learn_single_process\u001b[1;34m(self, timesteps, plot_results, reset, success_threshold, log_level, log_each_n_episodes)\u001b[0m\n\u001b[0;32m    181\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(state,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)   \n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Action data is action for continuous and onehot for discrete\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# prediction_data is logp_t[0] for continuous and prediction for discrete\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m action, action_data, prediction_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m#print(\"action\",action, \"action_data\",action_data, \"prediction_data\",prediction_data)\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m#print(state,action,action_data,prediction_data)\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Retrieve new state, reward, and whether the state is terminal\u001b[39;00m\n\u001b[0;32m    191\u001b[0m next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32m~\\Development\\GIT\\tmp\\reinforcement-learning\\src\\agents\\ppo\\networks.py:94\u001b[0m, in \u001b[0;36mact.<locals>.fn\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action, action_onehot, prediction\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     \n\u001b[0;32m     93\u001b[0m     \u001b[38;5;66;03m# Use the network to predict the next action to take, using the model\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     low, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;66;03m# -1 and 1 are boundaries of tanh\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     action \u001b[38;5;241m=\u001b[39m pred \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(low, high, size\u001b[38;5;241m=\u001b[39mpred\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m*\u001b[39m std\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py:970\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    969\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:700\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.predict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    697\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[0;32m    698\u001b[0m x, _, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_standardize_user_data(\n\u001b[0;32m    699\u001b[0m     x, check_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, steps_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m, steps\u001b[38;5;241m=\u001b[39msteps)\n\u001b[1;32m--> 700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:377\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_index, batch_logs)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    379\u001b[0m   batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\backend.py:4275\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4270\u001b[0m     symbol_vals \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol_vals \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4271\u001b[0m     feed_symbols \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_symbols \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4272\u001b[0m     session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session):\n\u001b[0;32m   4273\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4275\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches):])\n\u001b[0;32m   4278\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4280\u001b[0m     fetched[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4281\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1480\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1479\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1480\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1484\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "agent_1 = PpoAgent(\n",
    "    environment,\n",
    "    batch_size=1024,\n",
    "    actor_learning_rate=0.1,\n",
    "    critic_learning_rate=0.1,\n",
    "    epochs=10)\n",
    "agent_1.learn(log_each_n_episodes=1,n_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027f06a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.328856Z",
     "start_time": "2022-08-02T18:46:08.328856Z"
    }
   },
   "outputs": [],
   "source": [
    "xxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5025163b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.330854Z",
     "start_time": "2022-08-02T18:46:08.330854Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_1.log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341de877",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.331851Z",
     "start_time": "2022-08-02T18:46:08.331851Z"
    }
   },
   "outputs": [],
   "source": [
    "states= np.array([[-0.00518034, -0.08861306,  0.9955604,  -0.0941247,   0.05484154],\n",
    " [-0.007649 ,  -0.14961523 , 0.99579173 ,-0.09164527 , 0.15092096],\n",
    " [-0.0087505 , -0.06675814 , 0.99571204 ,-0.09250706 ,-0.05245256]],dtype=np.float32) \n",
    "y_true= np.array([[ 0.59678435, -0.74026847, -0.48596317,],\n",
    " [ 0.5485846,  -0.8950667 ,  0.59213954],\n",
    " [ 0.49422464, -0.45643705, -0.16582818]],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3706ee6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.333852Z",
     "start_time": "2022-08-02T18:46:08.333852Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_1.actor.fit(states,y_true,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243da94d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.334851Z",
     "start_time": "2022-08-02T18:46:08.334851Z"
    }
   },
   "outputs": [],
   "source": [
    "agent_2.Actor.Actor.fit(states,y_true,epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdbd65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.335851Z",
     "start_time": "2022-08-02T18:46:08.335851Z"
    }
   },
   "outputs": [],
   "source": [
    "state = agent.env.reset()\n",
    "np.reshape(state, [1, state.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52942132",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.336827Z",
     "start_time": "2022-08-02T18:46:08.336827Z"
    }
   },
   "outputs": [],
   "source": [
    "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747276a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.338829Z",
     "start_time": "2022-08-02T18:46:08.338829Z"
    }
   },
   "outputs": [],
   "source": [
    "import gfootball.env as football_env\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "clipping_val = 0.2\n",
    "critic_discount = 0.5\n",
    "entropy_beta = 0.001\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "    \n",
    "state = env.reset()\n",
    "state_dims = env.observation_space.shape\n",
    "\n",
    "state_size = state_dims\n",
    "n_actions = env.action_space.n\n",
    "action_space = n_actions\n",
    "input_shape = env.observation_space.shape\n",
    "\n",
    "dummy_n = np.zeros((1, 1, n_actions))\n",
    "dummy_1 = np.zeros((1, 1, 1))\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='./logs/')\n",
    "\n",
    "\n",
    "target_reached = False\n",
    "best_reward = 0\n",
    "iters = 0\n",
    "max_iters = 50\n",
    "training_batch=128\n",
    "episode = 0\n",
    "replay_count = 0\n",
    "shuffle = False\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "def get_advantages(values, masks, rewards):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * lmbda * masks[i] * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "\n",
    "def critic_PPO2_loss(values):\n",
    "    def loss(y_true, y_pred):\n",
    "        LOSS_CLIPPING = clipping_val\n",
    "        clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "        v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "        v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "        value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "        #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "        return value_loss\n",
    "    return loss\n",
    "\n",
    "def critic_ppo_loss(y_true, y_pred):\n",
    "    value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "    return value_loss\n",
    "\n",
    "def actor_ppo_loss(y_true, y_pred):\n",
    "    # Defined in https://arxiv.org/abs/1707.06347\n",
    "    advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+action_space], y_true[:, 1+action_space:]\n",
    "    LOSS_CLIPPING = clipping_val\n",
    "    ENTROPY_LOSS = entropy_beta\n",
    "        \n",
    "    prob = actions * y_pred\n",
    "    old_prob = actions * prediction_picks\n",
    "\n",
    "    prob = K.clip(prob, 1e-10, 1.0)\n",
    "    old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "    ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "    p1 = ratio * advantages\n",
    "    p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "    actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "    entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "    entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "    total_loss = actor_loss - entropy\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def actor_ppo_loss_continuous(y_true, y_pred):\n",
    "    advantages, logp_old_ph, actions  = y_true[:, :1], y_true[:, 1:1+action_space], y_true[:, 1+action_space]\n",
    "    LOSS_CLIPPING = clipping_val\n",
    "    logp = gaussian_likelihood(actions, y_pred)\n",
    "\n",
    "    ratio = K.exp(logp - logp_old_ph)\n",
    "\n",
    "    p1 = ratio * advantages\n",
    "    p2 = tf.where(advantages > 0, (1.0 + LOSS_CLIPPING)*advantages, (1.0 - LOSS_CLIPPING)*advantages) # minimum advantage\n",
    "\n",
    "    actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "    return actor_loss\n",
    "\n",
    "def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "    log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "    pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "    return K.sum(pre_sum, axis=1)\n",
    "    \n",
    "def get_common_layer(X_input, model=\"MLP\"):\n",
    "    # Shared CNN layers:\n",
    "    if model==\"CNN\":\n",
    "        X = Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")(X_input)\n",
    "        X = MaxPooling1D(pool_size=2)(X)\n",
    "        X = Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(X)\n",
    "        X = MaxPooling1D(pool_size=2)(X)\n",
    "        X = Flatten()(X)\n",
    "\n",
    "    # Shared LSTM layers:\n",
    "    elif model==\"LSTM\":\n",
    "        X = LSTM(512, return_sequences=True)(X_input)\n",
    "        X = LSTM(256)(X)\n",
    "\n",
    "    # Shared Dense layers:\n",
    "    else:\n",
    "        X = Flatten()(X_input)\n",
    "        X = Dense(512, activation=\"relu\")(X)\n",
    "        \n",
    "    return X\n",
    "\n",
    "def get_model_actor_simple(input_shape=input_shape, n_actions=n_actions, continuous=False):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    X = get_common_layer(X_input)\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "    if continuous:\n",
    "        output = Dense(n_actions,activation=\"tanh\")(X)\n",
    "    else:\n",
    "        output = Dense(n_actions, activation=\"softmax\")(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = output)\n",
    "    model.compile(loss=actor_ppo_loss, optimizer=Adam(lr=0.00025))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_critic_simple(input_shape=input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    V = get_common_layer(X_input)\n",
    "    V = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "    V = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "    V = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "    value = Dense(1, activation=None)(V)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs = value)\n",
    "    model.compile(loss=critic_ppo_loss, optimizer=Adam(lr=0.00025))\n",
    "    return model\n",
    "    \n",
    "actor = get_model_actor_simple()\n",
    "critic = get_model_critic_simple()\n",
    "\n",
    "def act(state):\n",
    "    \"\"\" example:\n",
    "    pred = np.array([0.05, 0.85, 0.1])\n",
    "    action_size = 3\n",
    "    np.random.choice(a, p=pred)\n",
    "    result>>> 1, because it have the highest probability to be taken\n",
    "    \"\"\"\n",
    "    # Use the network to predict the next action to take, using the model\n",
    "    prediction = actor.predict(state)[0]\n",
    "    action = np.random.choice(n_actions, p=prediction)\n",
    "    action_onehot = np.zeros([n_actions])\n",
    "    action_onehot[action] = 1\n",
    "    return action, action_onehot, prediction\n",
    "\n",
    "def discount_rewards(reward):#gaes is better\n",
    "    # Compute the gamma-discounted rewards over an episode\n",
    "    # We apply the discount and normalize it to avoid big variability of rewards\n",
    "    gamma = 0.99    # discount rate\n",
    "    running_add = 0\n",
    "    discounted_r = np.zeros_like(reward)\n",
    "    for i in reversed(range(0,len(reward))):\n",
    "        running_add = running_add * gamma + reward[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "    discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "    return discounted_r\n",
    "\n",
    "def get_gaes(rewards, dones, values, next_values, gamma = 0.99, lamda = 0.9, normalize=True):\n",
    "    deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "    deltas = np.stack(deltas)\n",
    "    gaes = copy.deepcopy(deltas)\n",
    "    for t in reversed(range(len(deltas) - 1)):\n",
    "        gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "    target = gaes + values\n",
    "    if normalize:\n",
    "        gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "    return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "   \n",
    "    limit = 0\n",
    "    while not done:\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "        action = np.argmax(action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        #print('test reward',reward)\n",
    "        limit += 1\n",
    "        if limit > 20:\n",
    "            break\n",
    "    print('testing...', total_reward)\n",
    "    return total_reward\n",
    "\n",
    "def critic_predict(state):\n",
    "    return critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "def replay(states, actions, rewards, predictions, dones, next_states):\n",
    "    \n",
    "    print('states.shape0',np.array(states).shape)\n",
    "    # reshape memory to appropriate shape for training\n",
    "    states = np.vstack(states)\n",
    "    next_states = np.vstack(next_states)\n",
    "    actions = np.vstack(actions)\n",
    "    predictions = np.vstack(predictions)\n",
    "\n",
    "    print('states.shape1',np.array(states).shape)\n",
    "    # Get Critic network predictions \n",
    "    values = critic_predict(states)\n",
    "    next_values = critic_predict(next_states)\n",
    "\n",
    "    # Compute discounted rewards and advantages\n",
    "    #discounted_r = self.discount_rewards(rewards)\n",
    "    #advantages = np.vstack(discounted_r - values)\n",
    "    advantages, target = get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "    \n",
    "#     print('advantages')\n",
    "#     print(advantages)\n",
    "        \n",
    "#     print('')\n",
    "#     print('targets')\n",
    "#     print(target)\n",
    "    '''\n",
    "        pylab.plot(advantages,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        '''\n",
    "    # stack everything to numpy array\n",
    "    # pack all advantages, predictions and actions to y_true and when they are received\n",
    "    # in custom PPO loss function we unpack it\n",
    "    y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "    # training Actor and Critic networks\n",
    "        \n",
    "    print('advantages',advantages[0])\n",
    "    print('preds',predictions[0])\n",
    "    print('actions',actions[0])\n",
    "    print()\n",
    " \n",
    "    a_loss = actor.fit(states, y_true, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "    print('actor loss',np.mean(a_loss.history['loss']))\n",
    "    print()\n",
    "\n",
    "    c_loss = critic.fit(states, target, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "    print('critic loss',np.mean(c_loss.history['loss']))\n",
    "    print()\n",
    "#     self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "#     self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "#     self.replay_count += 1\n",
    "    #replay_count += 1\n",
    "\n",
    "def run_batch(): # train every self.Training_batch episodes\n",
    "    scores_ = []\n",
    "    episodes_ = []\n",
    "    averages_= [] \n",
    "    episode = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1,state_size[0]])\n",
    "    done, score, SAVING = False, 0, ''\n",
    "    while True:\n",
    "        # Instantiate or reset games memory\n",
    "        states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "        for t in range(training_batch):\n",
    "           # env.render()\n",
    "            # Actor picks an action\n",
    "            action, action_onehot, prediction = act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Memorize (state, action, reward) for training\n",
    "            states.append(state)\n",
    "            next_states.append(np.reshape(next_state, [1, state_size[0]]))\n",
    "            actions.append(action_onehot)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            predictions.append(prediction)\n",
    "            # Update current state\n",
    "            state = np.reshape(next_state, [1, state_size[0]])\n",
    "            score += reward\n",
    "            if done:\n",
    "                episode += 1\n",
    "                SAVING = False\n",
    "                scores_.append(score)\n",
    "                averages_.append(sum(scores_[-50:]) / len(scores_[-50:]))\n",
    "\n",
    "                print('score', averages_[-1])\n",
    "#                     average, SAVING = self.PlotModel(score, self.episode)\n",
    "#                     print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "\n",
    "                state, done, score, SAVING = env.reset(), False, 0, ''\n",
    "                state = np.reshape(state, [1, state_size[0]])\n",
    "   \n",
    "        replay(states, actions, rewards, predictions, dones, next_states)\n",
    "        if episode >= 1000:\n",
    "            break\n",
    "    env.close()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9de909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.339832Z",
     "start_time": "2022-08-02T18:46:08.339832Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "run_batch() # train as PPO, train every epesode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840f3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.341826Z",
     "start_time": "2022-08-02T18:46:08.341826Z"
    }
   },
   "outputs": [],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb5ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.342826Z",
     "start_time": "2022-08-02T18:46:08.342826Z"
    }
   },
   "outputs": [],
   "source": [
    "type(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69088a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.343825Z",
     "start_time": "2022-08-02T18:46:08.343825Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f'GPUs {gpus}')\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass\n",
    "\n",
    "class Environment(Process):\n",
    "    def __init__(self, env_idx, child_conn, env_name, state_size, action_size, visualize=False):\n",
    "        super(Environment, self).__init__()\n",
    "        self.env = gym.make(env_name)\n",
    "        self.is_render = visualize\n",
    "        self.env_idx = env_idx\n",
    "        self.child_conn = child_conn\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def run(self):\n",
    "        super(Environment, self).run()\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        self.child_conn.send(state)\n",
    "        while True:\n",
    "            action = self.child_conn.recv()\n",
    "            if self.is_render and self.env_idx == 0:\n",
    "                self.env.render()\n",
    "\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            self.child_conn.send([state, reward, done, info])\n",
    "\n",
    "\n",
    "class Actor_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        ENTROPY_LOSS = 0.001\n",
    "        \n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = K.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "\n",
    "class Critic_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        V = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "        V = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        V = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[self.critic_PPO2_loss(old_values)], optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def critic_PPO2_loss(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "            LOSS_CLIPPING = 0.2\n",
    "            clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = 10000 # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = 0.00025\n",
    "        self.epochs = 10 # training epochs\n",
    "        self.shuffle=False\n",
    "        self.Training_batch = 1000\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n",
    "        \n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        \n",
    "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n",
    "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\" example:\n",
    "        pred = np.array([0.05, 0.85, 0.1])\n",
    "        action_size = 3\n",
    "        np.random.choice(a, p=pred)\n",
    "        result>>> 1, because it have the highest probability to be taken\n",
    "        \"\"\"\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        return action, action_onehot, prediction\n",
    "\n",
    "    def discount_rewards(self, reward):#gaes is better\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        # We apply the discount and normalize it to avoid big variability of rewards\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.9, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
    "        print('replay')\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        #discounted_r = self.discount_rewards(rewards)\n",
    "        #advantages = np.vstack(discounted_r - values)\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "        '''\n",
    "        pylab.plot(advantages,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        # pack all advantages, predictions and actions to y_true and when they are received\n",
    "        # in custom PPO loss function we unpack it\n",
    "        y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "            \n",
    "        print('advantages',advantages[0])\n",
    "        print('preds',predictions[0])\n",
    "        print('actions',actions[0])\n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "\n",
    "        #a_loss = actor.fit(states, y_true, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "        print('actor loss',np.mean(a_loss.history['loss']))\n",
    "        print()\n",
    "\n",
    "        #c_loss = critic.fit(states, target, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "        print('critic loss',np.mean(c_loss.history['loss']))\n",
    "        print()\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.replay_count += 1\n",
    " \n",
    "    def load(self):\n",
    "        self.Actor.Actor.load_weights(self.Actor_name)\n",
    "        self.Critic.Critic.load_weights(self.Critic_name)\n",
    "\n",
    "    def save(self):\n",
    "#         self.Actor.Actor.save_weights(self.Actor_name)\n",
    "#         self.Critic.Critic.save_weights(self.Critic_name)\n",
    "        pass\n",
    "\n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "    \n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores_.append(score)\n",
    "        self.episodes_.append(episode)\n",
    "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes_, self.scores_, 'b')\n",
    "            pylab.plot(self.episodes_, self.average_, 'r')\n",
    "            pylab.title(self.env_name+\" PPO training cycle\", fontsize=18)\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.grid(True)\n",
    "                pylab.savefig(self.env_name+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "        # saving best models\n",
    "        if self.average_[-1] >= self.max_average:\n",
    "            self.max_average = self.average_[-1]\n",
    "            self.save()\n",
    "            SAVING = \"SAVING\"\n",
    "            # decreaate learning rate every saved model\n",
    "            self.lr *= 0.95\n",
    "            K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
    "            K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
    "        else:\n",
    "            SAVING = \"\"\n",
    "\n",
    "        return self.average_[-1], SAVING\n",
    "    \n",
    "    def run(self): # train only when episode is finished\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            while not done:\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "                    \n",
    "                    self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "    def run_batch(self): # train every self.Training_batch episodes\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            for t in range(self.Training_batch):\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                \n",
    "                print(state,action,action_onehot,prediction)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "                    \n",
    "            self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()  \n",
    "\n",
    "        \n",
    "    def run_multiprocesses(self, num_worker = 4):\n",
    "        works, parent_conns, child_conns = [], [], []\n",
    "        for idx in range(num_worker):\n",
    "            parent_conn, child_conn = Pipe()\n",
    "            work = Environment(idx, child_conn, self.env_name, self.state_size[0], self.action_size, True)\n",
    "            work.start()\n",
    "            works.append(work)\n",
    "            parent_conns.append(parent_conn)\n",
    "            child_conns.append(child_conn)\n",
    "\n",
    "        states =        [[] for _ in range(num_worker)]\n",
    "        next_states =   [[] for _ in range(num_worker)]\n",
    "        actions =       [[] for _ in range(num_worker)]\n",
    "        rewards =       [[] for _ in range(num_worker)]\n",
    "        dones =         [[] for _ in range(num_worker)]\n",
    "        predictions =   [[] for _ in range(num_worker)]\n",
    "        score =         [0 for _ in range(num_worker)]\n",
    "\n",
    "        state = [0 for _ in range(num_worker)]\n",
    "        for worker_id, parent_conn in enumerate(parent_conns):\n",
    "            state[worker_id] = parent_conn.recv()\n",
    "\n",
    "        while self.episode < self.EPISODES:\n",
    "            predictions_list = self.Actor.predict(np.reshape(state, [num_worker, self.state_size[0]]))\n",
    "            actions_list = [np.random.choice(self.action_size, p=i) for i in predictions_list]\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                parent_conn.send(actions_list[worker_id])\n",
    "                action_onehot = np.zeros([self.action_size])\n",
    "                action_onehot[actions_list[worker_id]] = 1\n",
    "                actions[worker_id].append(action_onehot)\n",
    "                predictions[worker_id].append(predictions_list[worker_id])\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                next_state, reward, done, _ = parent_conn.recv()\n",
    "\n",
    "                states[worker_id].append(state[worker_id])\n",
    "                next_states[worker_id].append(next_state)\n",
    "                rewards[worker_id].append(reward)\n",
    "                dones[worker_id].append(done)\n",
    "                state[worker_id] = next_state\n",
    "                score[worker_id] += reward\n",
    "\n",
    "                if done:\n",
    "                    average, SAVING = self.PlotModel(score[worker_id], self.episode)\n",
    "                    print(\"episode: {}/{}, worker: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, worker_id, score[worker_id], average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/score_per_episode', score[worker_id], self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/learning_rate', self.lr, self.episode)\n",
    "                    score[worker_id] = 0\n",
    "                    if(self.episode < self.EPISODES):\n",
    "                        self.episode += 1\n",
    "                        \n",
    "            for worker_id in range(num_worker):\n",
    "                if len(states[worker_id]) >= self.Training_batch:\n",
    "                    self.replay(states[worker_id], actions[worker_id], rewards[worker_id], predictions[worker_id], dones[worker_id], next_states[worker_id])\n",
    "                    \n",
    "                    states[worker_id] = []\n",
    "                    next_states[worker_id] = []\n",
    "                    actions[worker_id] = []\n",
    "                    rewards[worker_id] = []\n",
    "                    dones[worker_id] = []\n",
    "                    predictions[worker_id] = []\n",
    "\n",
    "        # terminating processes after while loop\n",
    "        works.append(work)\n",
    "        for work in works:\n",
    "            work.terminate()\n",
    "            print('TERMINATED:', work)\n",
    "            work.join()\n",
    "            \n",
    "\n",
    "    def test(self, test_episodes = 100):\n",
    "        self.load()\n",
    "        for e in range(100):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size[0]])\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state)[0])\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, test_episodes, score))\n",
    "                    break\n",
    "        self.env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_name = 'CartPole-v1'\n",
    "    agent = PPOAgent(env_name)\n",
    "    agent.run_batch() # train as PPO, train every epesode\n",
    "    #agent.run_batch() # train as PPO, train every batch, trains better\n",
    "    #agent.run_multiprocesses(num_worker = 8)  # train PPO multiprocessed (fastest)\n",
    "    #agent.test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f0cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T17:31:49.273773Z",
     "start_time": "2022-08-02T17:31:49.273773Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740114c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:46:08.344825Z",
     "start_time": "2022-08-02T18:46:08.344825Z"
    }
   },
   "outputs": [],
   "source": [
    "#================================================================\n",
    "#\n",
    "#   File name   : BipedalWalker-v3_PPO\n",
    "#   Author      : PyLessons\n",
    "#   Created date: 2020-10-18\n",
    "#   Website     : https://pylessons.com/\n",
    "#   GitHub      : https://github.com/pythonlessons/Reinforcement_Learning\n",
    "#   Description : BipedalWalker-v3 PPO continuous agent\n",
    "#   TensorFlow  : 2.3.1\n",
    "#\n",
    "#================================================================\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import sys\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f'GPUs {gpus}')\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass\n",
    "\n",
    "class Environment(Process):\n",
    "    def __init__(self, env_idx, child_conn, env_name, state_size, action_size, visualize=False):\n",
    "        super(Environment, self).__init__()\n",
    "        self.env = gym.make(env_name)\n",
    "        self.is_render = visualize\n",
    "        self.env_idx = env_idx\n",
    "        self.child_conn = child_conn\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def run(self):\n",
    "        super(Environment, self).run()\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        self.child_conn.send(state)\n",
    "        while True:\n",
    "            action = self.child_conn.recv()\n",
    "            #if self.is_render and self.env_idx == 0:\n",
    "                #self.env.render()\n",
    "\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            self.child_conn.send([state, reward, done, info])\n",
    "\n",
    "\n",
    "class Actor_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "        \n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"tanh\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss_continuous, optimizer=optimizer(lr=lr))\n",
    "        #print(self.Actor.summary())\n",
    "\n",
    "    def ppo_loss_continuous(self, y_true, y_pred):\n",
    "        tf.print(\"\\n y_true:\", type(y_true), output_stream=sys.stdout)\n",
    "        advantages, actions, logp_old_ph, = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        logp = self.gaussian_likelihood(actions, y_pred)\n",
    "\n",
    "        ratio = K.exp(logp - logp_old_ph)\n",
    "\n",
    "        p1 = ratio * advantages\n",
    "        p2 = tf.where(advantages > 0, (1.0 + LOSS_CLIPPING)*advantages, (1.0 - LOSS_CLIPPING)*advantages) # minimum advantage\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "    def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "        log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "        pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "        return K.sum(pre_sum, axis=1)\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "\n",
    "class Critic_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        V = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        V = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(V)\n",
    "        V = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[self.critic_PPO2_loss(old_values)], optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def critic_PPO2_loss(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "            LOSS_CLIPPING = 0.2\n",
    "            clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "    \n",
    "\n",
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name, model_name=\"\"):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = 200000 # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = 0.00025\n",
    "        self.epochs = 10 # training epochs\n",
    "        self.shuffle = True\n",
    "        self.Training_batch = 512\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n",
    "        \n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        \n",
    "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n",
    "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n",
    "        #self.load() # uncomment to continue training from old weights\n",
    "\n",
    "        # do not change bellow\n",
    "        self.log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "        self.std = np.exp(self.log_std)\n",
    "\n",
    "\n",
    "    def act(self, state):\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        pred = self.Actor.predict(state)\n",
    "\n",
    "        low, high = -1.0, 1.0 # -1 and 1 are boundaries of tanh\n",
    "        action = pred + np.random.uniform(low, high, size=pred.shape) * self.std\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        logp_t = self.gaussian_likelihood(action, pred, self.log_std)\n",
    "        print('act',self.std, self.log_std)\n",
    "        print('logp_t',logp_t,'action',action, 'pred',pred)\n",
    "        print()\n",
    "        return action, logp_t\n",
    "\n",
    "    def gaussian_likelihood(self, action, pred, log_std):\n",
    "        # https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py\n",
    "        pre_sum = -0.5 * (((action-pred)/(np.exp(log_std)+1e-8))**2 + 2*log_std + np.log(2*np.pi)) \n",
    "        return np.sum(pre_sum, axis=1)\n",
    "\n",
    "    def discount_rewards(self, reward):#gaes is better\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        # We apply the discount and normalize it to avoid big variability of rewards\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.90, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def replay(self, states, actions, rewards, dones, next_states, logp_ts):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        logp_ts = np.vstack(logp_ts)\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        #discounted_r = self.discount_rewards(rewards)\n",
    "        #advantages = np.vstack(discounted_r - values)\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "        '''\n",
    "        pylab.plot(adv,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        if str(episode)[-2:] == \"00\": pylab.savefig(self.env_name+\"_\"+self.episode+\".png\")\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        # pack all advantages, predictions and actions to y_true and when they are received\n",
    "        # in custom loss function we unpack it\n",
    "        y_true = np.hstack([advantages, actions, logp_ts])\n",
    "        print('states')\n",
    "        print(states[0:3])\n",
    "        print('y_true')\n",
    "        print(y_true[0:3])\n",
    "            \n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=1, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=1, shuffle=self.shuffle)\n",
    "\n",
    "        print('advantages',advantages[0])\n",
    "        print('preds',logp_ts[0], np.mean(logp_ts))\n",
    "        print('actions',actions[0], np.mean(actions))\n",
    "        print()\n",
    "        print('actor loss', np.mean(a_loss.history['loss']))\n",
    "        print('critic loss',np.mean(c_loss.history['loss']))\n",
    "        print(a_loss.history['loss'])\n",
    "        print()\n",
    "        # calculate loss parameters (should be done in loss, but couldn't find working way how to do that with disabled eager execution)\n",
    "        pred = self.Actor.predict(states)\n",
    "        log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "        logp = self.gaussian_likelihood(actions, pred, log_std)\n",
    "        approx_kl = np.mean(logp_ts - logp)\n",
    "        approx_ent = np.mean(-logp)\n",
    "\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/approx_kl_per_replay', approx_kl, self.replay_count)\n",
    "        self.writer.add_scalar('Data/approx_ent_per_replay', approx_ent, self.replay_count)\n",
    "        self.replay_count += 1\n",
    " \n",
    "    def load(self):\n",
    "        self.Actor.Actor.load_weights(self.Actor_name)\n",
    "        self.Critic.Critic.load_weights(self.Critic_name)\n",
    "\n",
    "    def save(self):\n",
    "#         self.Actor.Actor.save_weights(self.Actor_name)\n",
    "#         self.Critic.Critic.save_weights(self.Critic_name)\n",
    "        pass\n",
    "\n",
    "    def run_batch(self):\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, dones, logp_ts = [], [], [], [], [], []\n",
    "            for t in range(self.Training_batch):\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, logp_t = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action[0])\n",
    "                # Memorize (state, next_states, action, reward, done, logp_ts) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                logp_ts.append(logp_t[0])\n",
    "                # Update current state shape\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/average_score',  average, self.episode)\n",
    "                    \n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            self.replay(states, actions, rewards, dones, next_states, logp_ts)\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "import pybullet_envs\n",
    "\n",
    "ENV_NAME = 'InvertedPendulumBulletEnv-v0'\n",
    "\n",
    "# newest gym fixed bugs in 'BipedalWalker-v2' and now it's called 'BipedalWalker-v3'\n",
    "env_name = ENV_NAME\n",
    "agent_2 = PPOAgent(env_name)\n",
    "agent_2.run_batch() # train as PPO\n",
    "    #agent.run_multiprocesses(num_worker = 16)  # train PPO multiprocessed (fastest)\n",
    "    #agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1e23e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:01:58.797991Z",
     "start_time": "2022-08-02T18:01:58.785992Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1c27a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
