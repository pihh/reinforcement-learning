{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602e667f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:49:52.141673Z",
     "start_time": "2022-07-28T13:49:52.107837Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747276a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T15:37:13.415134Z",
     "start_time": "2022-08-02T15:37:09.811615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\flatbuffers\\compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n",
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import gfootball.env as football_env\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "clipping_val = 0.2\n",
    "critic_discount = 0.5\n",
    "entropy_beta = 0.001\n",
    "gamma = 0.99\n",
    "lmbda = 0.95\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "    \n",
    "state = env.reset()\n",
    "state_dims = env.observation_space.shape\n",
    "\n",
    "state_size = state_dims\n",
    "n_actions = env.action_space.n\n",
    "action_space = n_actions\n",
    "input_shape = env.observation_space.shape\n",
    "\n",
    "dummy_n = np.zeros((1, 1, n_actions))\n",
    "dummy_1 = np.zeros((1, 1, 1))\n",
    "\n",
    "tensor_board = TensorBoard(log_dir='./logs/')\n",
    "\n",
    "\n",
    "target_reached = False\n",
    "best_reward = 0\n",
    "iters = 0\n",
    "max_iters = 50\n",
    "training_batch=1000\n",
    "episode = 0\n",
    "replay_count = 0\n",
    "shuffle = False\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "def get_advantages(values, masks, rewards):\n",
    "    returns = []\n",
    "    gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "        gae = delta + gamma * lmbda * masks[i] * gae\n",
    "        returns.insert(0, gae + values[i])\n",
    "\n",
    "    adv = np.array(returns) - values[:-1]\n",
    "    return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "\n",
    "def critic_PPO2_loss(values):\n",
    "    def loss(y_true, y_pred):\n",
    "        LOSS_CLIPPING = clipping_val\n",
    "        clipped_value_loss = values + K.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "        v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "        v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "        value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "        #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "        return value_loss\n",
    "    return loss\n",
    "\n",
    "def critic_ppo_loss(y_true, y_pred):\n",
    "    value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "    return value_loss\n",
    "\n",
    "def actor_ppo_loss(y_true, y_pred):\n",
    "    # Defined in https://arxiv.org/abs/1707.06347\n",
    "    advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+action_space], y_true[:, 1+action_space:]\n",
    "    LOSS_CLIPPING = clipping_val\n",
    "    ENTROPY_LOSS = entropy_beta\n",
    "        \n",
    "    prob = actions * y_pred\n",
    "    old_prob = actions * prediction_picks\n",
    "\n",
    "    prob = K.clip(prob, 1e-10, 1.0)\n",
    "    old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "    ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "        \n",
    "    p1 = ratio * advantages\n",
    "    p2 = K.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "    actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "    entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "    entropy = ENTROPY_LOSS * K.mean(entropy)\n",
    "        \n",
    "    total_loss = actor_loss - entropy\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def actor_ppo_loss_continuous(y_true, y_pred):\n",
    "    advantages, logp_old_ph, actions  = y_true[:, :1], y_true[:, 1:1+action_space], y_true[:, 1+action_space]\n",
    "    LOSS_CLIPPING = clipping_val\n",
    "    logp = gaussian_likelihood(actions, y_pred)\n",
    "\n",
    "    ratio = K.exp(logp - logp_old_ph)\n",
    "\n",
    "    p1 = ratio * advantages\n",
    "    p2 = tf.where(advantages > 0, (1.0 + LOSS_CLIPPING)*advantages, (1.0 - LOSS_CLIPPING)*advantages) # minimum advantage\n",
    "\n",
    "    actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "    return actor_loss\n",
    "\n",
    "def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "    log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "    pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "    return K.sum(pre_sum, axis=1)\n",
    "    \n",
    "def get_common_layer(X_input, model=\"MLP\"):\n",
    "    # Shared CNN layers:\n",
    "    if model==\"CNN\":\n",
    "        X = Conv1D(filters=64, kernel_size=6, padding=\"same\", activation=\"tanh\")(X_input)\n",
    "        X = MaxPooling1D(pool_size=2)(X)\n",
    "        X = Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"tanh\")(X)\n",
    "        X = MaxPooling1D(pool_size=2)(X)\n",
    "        X = Flatten()(X)\n",
    "\n",
    "    # Shared LSTM layers:\n",
    "    elif model==\"LSTM\":\n",
    "        X = LSTM(512, return_sequences=True)(X_input)\n",
    "        X = LSTM(256)(X)\n",
    "\n",
    "    # Shared Dense layers:\n",
    "    else:\n",
    "        X = Flatten()(X_input)\n",
    "        X = Dense(512, activation=\"relu\")(X)\n",
    "        \n",
    "    return X\n",
    "\n",
    "def get_model_actor_simple(input_shape=input_shape, n_actions=n_actions, continuous=False):\n",
    "\n",
    "    X_input = Input(input_shape)\n",
    "    X = get_common_layer(X_input)\n",
    "    X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "    X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "    X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "    if continuous:\n",
    "        output = Dense(n_actions,activation=\"tanh\")(X)\n",
    "    else:\n",
    "        output = Dense(n_actions, activation=\"softmax\")(X)\n",
    "\n",
    "    model = Model(inputs = X_input, outputs = output)\n",
    "    model.compile(loss=actor_ppo_loss, optimizer=Adam(lr=0.00025))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model_critic_simple(input_shape=input_shape):\n",
    "    X_input = Input(input_shape)\n",
    "    V = get_common_layer(X_input)\n",
    "    V = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "    V = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "    V = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "    value = Dense(1, activation=None)(V)\n",
    "\n",
    "    model = Model(inputs=X_input, outputs = value)\n",
    "    model.compile(loss=critic_ppo_loss, optimizer=Adam(lr=0.00025))\n",
    "    return model\n",
    "    \n",
    "actor = get_model_actor_simple()\n",
    "critic = get_model_critic_simple()\n",
    "\n",
    "def act(state):\n",
    "    \"\"\" example:\n",
    "    pred = np.array([0.05, 0.85, 0.1])\n",
    "    action_size = 3\n",
    "    np.random.choice(a, p=pred)\n",
    "    result>>> 1, because it have the highest probability to be taken\n",
    "    \"\"\"\n",
    "    # Use the network to predict the next action to take, using the model\n",
    "    prediction = actor.predict(state)[0]\n",
    "    action = np.random.choice(n_actions, p=prediction)\n",
    "    action_onehot = np.zeros([n_actions])\n",
    "    action_onehot[action] = 1\n",
    "    return action, action_onehot, prediction\n",
    "\n",
    "def discount_rewards(reward):#gaes is better\n",
    "    # Compute the gamma-discounted rewards over an episode\n",
    "    # We apply the discount and normalize it to avoid big variability of rewards\n",
    "    gamma = 0.99    # discount rate\n",
    "    running_add = 0\n",
    "    discounted_r = np.zeros_like(reward)\n",
    "    for i in reversed(range(0,len(reward))):\n",
    "        running_add = running_add * gamma + reward[i]\n",
    "        discounted_r[i] = running_add\n",
    "\n",
    "    discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "    discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "    return discounted_r\n",
    "\n",
    "def get_gaes(rewards, dones, values, next_values, gamma = 0.99, lamda = 0.9, normalize=True):\n",
    "    deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "    deltas = np.stack(deltas)\n",
    "    gaes = copy.deepcopy(deltas)\n",
    "    for t in reversed(range(len(deltas) - 1)):\n",
    "        gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "    target = gaes + values\n",
    "    if normalize:\n",
    "        gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "    return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "def test_reward():\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "   \n",
    "    limit = 0\n",
    "    while not done:\n",
    "        state_input = K.expand_dims(state, 0)\n",
    "        action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "        action = np.argmax(action_probs)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        #print('test reward',reward)\n",
    "        limit += 1\n",
    "        if limit > 20:\n",
    "            break\n",
    "    print('testing...', total_reward)\n",
    "    return total_reward\n",
    "\n",
    "def critic_predict(state):\n",
    "    return critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "def replay(states, actions, rewards, predictions, dones, next_states):\n",
    "    # reshape memory to appropriate shape for training\n",
    "    states = np.vstack(states)\n",
    "    next_states = np.vstack(next_states)\n",
    "    actions = np.vstack(actions)\n",
    "    predictions = np.vstack(predictions)\n",
    "\n",
    "    # Get Critic network predictions \n",
    "    values = critic_predict(states)\n",
    "    next_values = critic_predict(next_states)\n",
    "\n",
    "    # Compute discounted rewards and advantages\n",
    "    #discounted_r = self.discount_rewards(rewards)\n",
    "    #advantages = np.vstack(discounted_r - values)\n",
    "    advantages, target = get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "    '''\n",
    "        pylab.plot(advantages,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        '''\n",
    "    # stack everything to numpy array\n",
    "    # pack all advantages, predictions and actions to y_true and when they are received\n",
    "    # in custom PPO loss function we unpack it\n",
    "    y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "    # training Actor and Critic networks\n",
    "    print(np.array(states.shape),np.array(y_true).shape)\n",
    " \n",
    "    a_loss = actor.fit(states, y_true, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "    print('actor loss',np.mean(a_loss.history['loss']))\n",
    "    print()\n",
    "\n",
    "    c_loss = critic.fit(states, target, epochs=epochs, verbose=0, shuffle=shuffle)\n",
    "    print('critic loss',np.mean(c_loss.history['loss']))\n",
    "    print()\n",
    "#     self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "#     self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "#     self.replay_count += 1\n",
    "    #replay_count += 1\n",
    "\n",
    "def run_batch(): # train every self.Training_batch episodes\n",
    "    scores_ = []\n",
    "    episodes_ = []\n",
    "    averages_= [] \n",
    "    episode = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1,state_size[0]])\n",
    "    done, score, SAVING = False, 0, ''\n",
    "    while True:\n",
    "        # Instantiate or reset games memory\n",
    "        states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "        for t in range(training_batch):\n",
    "           # env.render()\n",
    "            # Actor picks an action\n",
    "            action, action_onehot, prediction = act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Memorize (state, action, reward) for training\n",
    "            states.append(state)\n",
    "            next_states.append(np.reshape(next_state, [1, state_size[0]]))\n",
    "            actions.append(action_onehot)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "            predictions.append(prediction)\n",
    "            # Update current state\n",
    "            state = np.reshape(next_state, [1, state_size[0]])\n",
    "            score += reward\n",
    "            if done:\n",
    "                episode += 1\n",
    "                SAVING = False\n",
    "                scores_.append(score)\n",
    "                averages_.append(sum(scores_[-50:]) / len(scores_[-50:]))\n",
    "\n",
    "                print('score', averages_[-1])\n",
    "#                     average, SAVING = self.PlotModel(score, self.episode)\n",
    "#                     print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "#                     self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "\n",
    "                state, done, score, SAVING = env.reset(), False, 0, ''\n",
    "                state = np.reshape(state, [1, state_size[0]])\n",
    "   \n",
    "        replay(states, actions, rewards, predictions, dones, next_states)\n",
    "        if episode >= 1000:\n",
    "            break\n",
    "    env.close()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9de909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T15:37:25.546088Z",
     "start_time": "2022-08-02T15:37:17.149691Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score 12.0\n",
      "score 16.5\n",
      "score 22.666666666666668\n",
      "score 29.0\n",
      "score 30.2\n",
      "score 28.333333333333332\n",
      "score 27.428571428571427\n",
      "score 29.125\n",
      "score 30.22222222222222\n",
      "score 30.5\n",
      "score 29.09090909090909\n",
      "score 29.166666666666668\n",
      "score 28.0\n",
      "score 27.785714285714285\n",
      "score 27.066666666666666\n",
      "score 26.8125\n",
      "score 26.0\n",
      "score 26.11111111111111\n",
      "score 26.36842105263158\n",
      "score 25.65\n",
      "score 25.142857142857142\n",
      "score 25.5\n",
      "score 25.217391304347824\n",
      "score 25.333333333333332\n",
      "score 24.88\n",
      "score 25.615384615384617\n",
      "score 25.37037037037037\n",
      "score 24.821428571428573\n",
      "score 24.93103448275862\n",
      "score 24.666666666666668\n",
      "score 24.903225806451612\n",
      "score 26.0\n",
      "score 25.757575757575758\n",
      "score 26.88235294117647\n",
      "score 26.97142857142857\n",
      "score 26.694444444444443\n",
      "[1000    4] (1000, 5)\n",
      "actor loss -0.009604438585042955\n",
      "\n",
      "critic loss 19.00114910106659\n",
      "\n",
      "score 27.37837837837838\n",
      "score 27.07894736842105\n",
      "score 26.82051282051282\n",
      "score 26.85\n",
      "score 26.804878048780488\n",
      "score 26.404761904761905\n",
      "score 26.209302325581394\n",
      "score 26.15909090909091\n",
      "score 25.91111111111111\n",
      "score 25.73913043478261\n",
      "score 25.680851063829788\n",
      "score 25.645833333333332\n",
      "score 25.857142857142858\n",
      "score 26.44\n",
      "score 27.46\n",
      "score 27.72\n",
      "score 27.66\n",
      "score 27.46\n",
      "score 27.38\n",
      "score 27.42\n",
      "score 27.44\n",
      "score 27.86\n",
      "score 28.3\n",
      "score 27.84\n",
      "score 28.38\n",
      "score 28.12\n",
      "score 28.18\n",
      "score 28.46\n",
      "score 28.38\n",
      "score 28.34\n",
      "score 28.46\n",
      "score 28.12\n",
      "[1000    4] (1000, 5)\n",
      "actor loss -0.01983891368873883\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mrun_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    323\u001b[0m         state, done, score, SAVING \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(), \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    324\u001b[0m         state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, state_size[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m--> 326\u001b[0m \u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mreplay\u001b[1;34m(states, actions, rewards, predictions, dones, next_states)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor loss\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39mmean(a_loss\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m--> 276\u001b[0m c_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic loss\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39mmean(c_loss\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    776\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:641\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    638\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    639\u001b[0m   val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:377\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_index, batch_logs)\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    379\u001b[0m   batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\backend.py:4275\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callable_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m feed_arrays \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_arrays \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4270\u001b[0m     symbol_vals \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol_vals \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4271\u001b[0m     feed_symbols \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed_symbols \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetches \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   4272\u001b[0m     session \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session):\n\u001b[0;32m   4273\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[1;32m-> 4275\u001b[0m fetched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marray_vals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4276\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4277\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fetches):])\n\u001b[0;32m   4278\u001b[0m output_structure \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(\n\u001b[0;32m   4279\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_structure,\n\u001b[0;32m   4280\u001b[0m     fetched[:\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs)],\n\u001b[0;32m   4281\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\client\\session.py:1480\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1479\u001b[0m   run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1480\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRunCallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1481\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1482\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1483\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m   1484\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "run_batch() # train as PPO, train every epesode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840f3f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:50:49.315061Z",
     "start_time": "2022-07-28T13:50:49.315061Z"
    }
   },
   "outputs": [],
   "source": [
    "state_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bb5ac6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:50:49.316046Z",
     "start_time": "2022-07-28T13:50:49.316046Z"
    }
   },
   "outputs": [],
   "source": [
    "type(critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69088a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T10:22:39.330059Z",
     "start_time": "2022-08-03T10:22:38.155776Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboardX\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mdisable_eager_execution() \u001b[38;5;66;03m# usually using this for fastest performance\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\tensorboardX\\__init__.py:5\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"A module for visualization with tensorboard\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordWriter\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorchvis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchVis\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter, SummaryWriter\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mglobal_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlobalSummaryWriter\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\tensorboardX\\torchvis.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wraps\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisdom_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisdomWriter\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Supports both TensorBoard and Visdom (no embedding or graph visualization with Visdom)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\tensorboardX\\writer.py:34\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m numpy_compatible \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mndarray\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     numpy_compatible \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\torch\\__init__.py:126\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    124\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    125\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"C:\\Users\\filip\\anaconda3\\envs\\ai_4\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29470b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
