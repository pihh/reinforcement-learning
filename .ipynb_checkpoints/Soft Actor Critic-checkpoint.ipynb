{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb0124b",
   "metadata": {},
   "source": [
    "### Soft Actor Critic\n",
    "\n",
    "#### Introduction:\n",
    "* Maximizes long term rewards and entropy\n",
    "* Similar to Q learning ( epsilon greedy - some % of the time selects random action )\n",
    "* Entropy modeled by reward scaling ( inv. relationship )\n",
    "* Networks:\n",
    "    * 1 actor network\n",
    "    * 1 value network\n",
    "    * 2 critic networks (like ddqn/td3)\n",
    "* Uses a target value function (soft update)\n",
    "* Has replay buffer\n",
    "\n",
    "#### Sampling:\n",
    "* actor outputs mu and sigma and we use a normal dist from them\n",
    "\n",
    "#### Network updates:\n",
    "* Actor:\n",
    "    * sample states from buffer, compute new actions\n",
    "    * get the minimum of two critics\n",
    "    * log is computed according to the previous slide\n",
    "* Value:\n",
    "    * use value fn (current params) for states\n",
    "    * samples states and computes new actions\n",
    "    * uses minimum value of two critics\n",
    "    * log is computed according to the prev slide\n",
    "* Target:\n",
    "    * Uses a small tau ( 0.005 for ex )\n",
    "    * Slowly moving avg of online and target network\n",
    "* Critic:\n",
    "    * target value fort new states\n",
    "    * sample states and actions\n",
    "    * reward is scaled here\n",
    "\n",
    "#### Limitations:\n",
    "* Only works in continous environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db4ca577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T12:56:18.763799Z",
     "start_time": "2022-07-25T12:56:18.732590Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36625620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:40:40.739847Z",
     "start_time": "2022-07-25T13:40:40.730985Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.agents.agent import Agent\n",
    "from src.utils.buffer import Buffer\n",
    "from src.utils.logger import LearningLogger\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d4fee5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:40:41.022623Z",
     "start_time": "2022-07-25T13:40:41.002582Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LearningLogger' object has no attribute 'log_episode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m logger \u001b[38;5;241m=\u001b[39m LearningLogger()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_episode\u001b[49m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LearningLogger' object has no attribute 'log_episode'"
     ]
    }
   ],
   "source": [
    "logger = LearningLogger()\n",
    "logger.log_episode(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8443c26a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T12:56:22.436039Z",
     "start_time": "2022-07-25T12:56:22.422039Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, input_shape, n_actions):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer_counter = 0\n",
    "        self.state_memory = np.zeros((self.buffer_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.buffer_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.buffer_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.buffer_size)\n",
    "        self.done_memory = np.zeros(self.buffer_size, dtype=np.bool)\n",
    "\n",
    "    def remember(self, state, action, reward, state_, done):\n",
    "        index = self.buffer_counter % self.buffer_size\n",
    "\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.done_memory[index] = done\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    def sample(self, batch_size=64):\n",
    "        max_mem = min(self.buffer_counter, self.buffer_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e91f37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T12:56:22.452039Z",
     "start_time": "2022-07-25T12:56:22.439040Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.layers import Dense \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def MultiLayerPerceptron(policy=\"mlp\"):\n",
    "    layers = []\n",
    "    if type(policy) == str:\n",
    "        if policy == \"mlp\":\n",
    "            layers.append(Dense(256, activation='relu', name=\"mlp_dense_layer_0\"))\n",
    "            layers.append(Dense(256, activation='relu', name=\"mlp_dense_layer_1\"))\n",
    "    else:\n",
    "        for i,layer in enumerate(policy):\n",
    "            layer._name = 'mlp_custom_layer_{}'.format(i)\n",
    "            layers.append(layer)\n",
    "            \n",
    "    return layers\n",
    "        \n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                policy=\"mlp\",\n",
    "                n_actions=2,\n",
    "                name='critic'\n",
    "        ):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.model_name = name\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        self.q = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        X = tf.concat([state, action], axis=1)\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "            \n",
    "        q = self.q(X)\n",
    "        return q\n",
    "\n",
    "class ValueNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 policy=\"mlp\",\n",
    "                 name='value',  \n",
    "        ):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        \n",
    "\n",
    "        self.model_name = name\n",
    "\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        self.v = Dense(1, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        X = state\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "\n",
    "        v = self.v(X)\n",
    "\n",
    "        return v\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, \n",
    "            policy=\"mlp\",\n",
    "            n_actions=2,\n",
    "            max_action=1, \n",
    "            name='actor', \n",
    "    ):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.model_name = name\n",
    "        self.max_action = max_action\n",
    "        self.noise = 1e-6\n",
    "\n",
    "        self.fc = MultiLayerPerceptron(policy=policy)\n",
    "        \n",
    "        self.mu = Dense(n_actions, activation=None)\n",
    "        self.sigma = Dense(n_actions, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        X = state\n",
    "        for layer in self.fc:\n",
    "            X = layer(X)\n",
    "\n",
    "        mu = self.mu(X)\n",
    "        sigma = self.sigma(X)\n",
    "        sigma = tf.clip_by_value(sigma, self.noise, 1)\n",
    "\n",
    "        return mu, sigma\n",
    "\n",
    "    def sample_normal(self, state, reparameterize=True):\n",
    "        mu, sigma = self.call(state)\n",
    "        probabilities = tfp.distributions.Normal(mu, sigma)\n",
    "\n",
    "        if reparameterize:\n",
    "            actions = probabilities.sample() # + something else if you want to implement\n",
    "        else:\n",
    "            actions = probabilities.sample()\n",
    "\n",
    "        action = tf.math.tanh(actions)*self.max_action\n",
    "        log_probs = probabilities.log_prob(actions)\n",
    "        log_probs -= tf.math.log(1-tf.math.pow(action,2)+self.noise)\n",
    "        log_probs = tf.math.reduce_sum(log_probs, axis=1, keepdims=True)\n",
    "\n",
    "        return action, log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46258ddf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T12:56:22.484038Z",
     "start_time": "2022-07-25T12:56:22.453040Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.agents.agent import Agent\n",
    "\n",
    "\n",
    "class SoftActorCriticAgent(Agent):\n",
    "    def __init__(self, \n",
    "            environment,\n",
    "            alpha=0.0003, \n",
    "            beta=0.0003, \n",
    "            gamma=0.99, \n",
    "            tau=0.005,\n",
    "            buffer_size=1000000, \n",
    "            policy=\"mlp\", \n",
    "            batch_size=256, \n",
    "            reward_scale=2, \n",
    "            loss_function = keras.losses.MSE, #keras.losses.Huber()\n",
    "    ):\n",
    "        super(SoftActorCriticAgent, self).__init__(environment)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.policy = policy\n",
    "        self.reward_scale = reward_scale\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_function = loss_function\n",
    "        \n",
    "        self.__init_networks()\n",
    "        self.__init_buffers()\n",
    "        \n",
    "    def __init_buffers(self):\n",
    "        self.buffer = ReplayBuffer(self.buffer_size, self.observation_shape, self.n_actions)\n",
    "            \n",
    "    def __init_networks(self):\n",
    "        self.actor = ActorNetwork(n_actions=self.n_actions,policy=self.policy, max_action=self.env.action_space.high)\n",
    "        self.critic_1 = CriticNetwork(n_actions=self.n_actions,policy=self.policy, name='critic_1')\n",
    "        self.critic_2 = CriticNetwork(n_actions=self.n_actions,policy=self.policy, name='critic_2')\n",
    "        self.value = ValueNetwork(name='value',policy=self.policy)\n",
    "        self.target_value = ValueNetwork(name='target_value',policy=self.policy)\n",
    "\n",
    "        self.actor.compile(optimizer=Adam(learning_rate=self.alpha))\n",
    "        self.critic_1.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.critic_2.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.value.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "        self.target_value.compile(optimizer=Adam(learning_rate=self.beta))\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "    \n",
    "    def choose_action(self, observation):\n",
    "        state = tf.convert_to_tensor([observation])\n",
    "        actions, _ = self.actor.sample_normal(state, reparameterize=False)\n",
    "\n",
    "        return actions[0]\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.buffer.remember(state, action, reward, new_state, done)      \n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_value.weights\n",
    "        for i, weight in enumerate(self.value.weights):\n",
    "            weights.append(weight * tau + targets[i]*(1-tau))\n",
    "\n",
    "        self.target_value.set_weights(weights)\n",
    "        \n",
    "    def replay(self):\n",
    "        if self.buffer.buffer_counter < self.batch_size:\n",
    "            return\n",
    "    \n",
    "        state,action, reward, state_, done = self.buffer.sample(self.batch_size)\n",
    "        \n",
    "        states = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        states_ = tf.convert_to_tensor(state_, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(reward, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(action, dtype=tf.float32)\n",
    "        \n",
    "        # Value network update\n",
    "        with tf.GradientTape() as tape:\n",
    "            value = tf.squeeze(self.value(states),1)\n",
    "            value_= tf.squeeze(self.target_value(states_),1)\n",
    "            \n",
    "            current_policy_actions, log_probs = self.actor.sample_normal(states)\n",
    "            log_probs = tf.squeeze(log_probs,1)\n",
    "            \n",
    "            q1_new_policy = self.critic_1(states,current_policy_actions)\n",
    "            q2_new_policy = self.critic_2(states,current_policy_actions)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(q1_new_policy,q2_new_policy))\n",
    "            \n",
    "            value_target = critic_value - log_probs\n",
    "            value_loss = 0.5 *self.loss_function(value,value_target)\n",
    "            \n",
    "            \n",
    "        value_network_gradient = tape.gradient(value_loss,self.value.trainable_variables)\n",
    "        self.value.optimizer.apply_gradients(zip(value_network_gradient, self.value.trainable_variables))\n",
    "        \n",
    "        # Actor network update\n",
    "        with tf.GradientTape() as tape:\n",
    "            # in the original paper, they reparameterize here. \n",
    "            new_policy_actions, log_probs = self.actor.sample_normal(states,reparameterize=True)\n",
    "            \n",
    "            log_probs = tf.squeeze(log_probs, 1)\n",
    "            q1_new_policy = self.critic_1(states, new_policy_actions)\n",
    "            q2_new_policy = self.critic_2(states, new_policy_actions)\n",
    "            critic_value = tf.squeeze(tf.math.minimum(\n",
    "                                        q1_new_policy, q2_new_policy), 1)\n",
    "        \n",
    "            actor_loss = log_probs - critic_value\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        actor_network_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_network_gradient, self.actor.trainable_variables))\n",
    "\n",
    "        # Critic network update\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            \n",
    "            q_hat = self.reward_scale*reward + self.gamma*value_*(1-done)\n",
    "            q1_old_policy = tf.squeeze(self.critic_1(state, action), 1)\n",
    "            q2_old_policy = tf.squeeze(self.critic_2(state, action), 1)\n",
    "            critic_1_loss = 0.5 * self.loss_function(q1_old_policy, q_hat)\n",
    "            critic_2_loss = 0.5 * self.loss_function(q2_old_policy, q_hat)\n",
    "    \n",
    "        critic_1_network_gradient = tape.gradient(critic_1_loss,self.critic_1.trainable_variables)\n",
    "        critic_2_network_gradient = tape.gradient(critic_2_loss,self.critic_2.trainable_variables)\n",
    "\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_network_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(critic_2_network_gradient, self.critic_2.trainable_variables))\n",
    "\n",
    "        self.update_network_parameters()\n",
    "        \n",
    "    def test(self, episodes=10, render=True):\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                \n",
    "                # Sample action, probs and critic\n",
    "                action = self.choose_action(state)\n",
    "\n",
    "                # Step\n",
    "                state,reward,done, info = self.env.step(action)\n",
    "\n",
    "                # Get next state\n",
    "                score += reward\n",
    "            \n",
    "            if render:\n",
    "                self.env.close()\n",
    "\n",
    "            print(\"Test episode: {}, score: {:.2f}\".format(episode,score)) \n",
    "            \n",
    "    def learn(self, timesteps=-1, plot_results=True, reset=False, log_each_n_episodes=100, success_threshold=False):\n",
    "        self.validate_learn(timesteps,success_threshold,reset)\n",
    "        success_threshold = success_threshold if success_threshold else self.env.success_threshold\n",
    " \n",
    "        score = 0\n",
    "        timestep = 0\n",
    "        episode = 0\n",
    "        \n",
    "        while self.learning_condition(timesteps,timestep):  # Run until solved\n",
    "            state = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.choose_action(state)\n",
    "                state_, reward, done, info = self.env.step(action)\n",
    "                score += reward\n",
    "                self.remember(state, action, reward, state_, done)\n",
    "                self.replay()\n",
    "                state = state_\n",
    "            \n",
    "            self.running_reward.step(score)\n",
    "             # Log details\n",
    "            episode += 1\n",
    "            if episode % log_each_n_episodes == 0 and episode > 0:\n",
    "                print('episode {}, running reward: {:.2f}, last reward: {:.2f}'.format(episode,self.running_reward.reward, score))\n",
    "\n",
    "            if self.did_finnish_learning(success_threshold,episode):\n",
    "                break\n",
    "\n",
    "        if plot_results:\n",
    "            self.plot_learning_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a055b87a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:03:00.170680Z",
     "start_time": "2022-07-25T12:56:55.554614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 10, running reward: 10.61, last reward: 19.00\n",
      "episode 20, running reward: 16.35, last reward: 22.00\n",
      "episode 30, running reward: 18.76, last reward: 33.00\n",
      "episode 40, running reward: 22.03, last reward: 21.00\n",
      "episode 50, running reward: 22.25, last reward: 19.00\n",
      "episode 60, running reward: 24.73, last reward: 47.00\n",
      "episode 70, running reward: 33.54, last reward: 47.00\n",
      "episode 80, running reward: 58.48, last reward: 102.00\n",
      "episode 90, running reward: 93.35, last reward: 109.00\n",
      "episode 100, running reward: 115.61, last reward: 173.00\n",
      "Agent solved environment at the episode 103\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "\n",
    "agent= SoftActorCriticAgent(environment)\n",
    "agent.learn(log_each_n_episodes=10, success_threshold=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d437e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:13:29.466260Z",
     "start_time": "2022-07-25T13:13:29.418925Z"
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m p\u001b[38;5;241m.\u001b[39mconnect(p\u001b[38;5;241m.\u001b[39mDIRECT)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironments\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontinuous\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minverted_pendulum\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m environment\n\u001b[1;32m----> 4\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36mSoftActorCriticAgent.test\u001b[1;34m(self, episodes, render)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(episodes):\n\u001b[1;32m--> 142\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\gym\\wrappers\\time_limit.py:26\u001b[0m, in \u001b[0;36mTimeLimit.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:18\u001b[0m, in \u001b[0;36mOrderEnforcing.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\pybullet_envs\\gym_pendulum_envs.py:23\u001b[0m, in \u001b[0;36mInvertedPendulumBulletEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     21\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateId \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#print(\"InvertedPendulumBulletEnv reset p.restoreState(\",self.stateId,\")\")\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_p\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestoreState\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateId\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m   r \u001b[38;5;241m=\u001b[39m MJCFBaseBulletEnv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     25\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateId \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "\n",
    "from src.environments.continuous.inverted_pendulum import environment\n",
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a1de5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T12:56:23.137040Z",
     "start_time": "2022-07-25T12:56:23.137040Z"
    }
   },
   "outputs": [],
   "source": [
    "#env.observation_space, env.action_space\n",
    "# Works well in -inf,inf boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f5f313",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T12:56:23.138042Z",
     "start_time": "2022-07-25T12:56:23.138042Z"
    }
   },
   "outputs": [],
   "source": [
    "#env.observation_space, env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a8dd9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T12:56:23.139042Z",
     "start_time": "2022-07-25T12:56:23.139042Z"
    }
   },
   "outputs": [],
   "source": [
    "agent.env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53e4e41e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:15:48.893401Z",
     "start_time": "2022-07-25T13:15:48.875370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| ---------------------------------\n",
      "| Pendulum-v1\n",
      "| \n",
      "| Action space: Continuous with low state-space\n",
      "| Environment beated threshold: -200\n",
      "| ----------------------------------------------------------   \n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.9656281 , -0.25992772, -0.31559733], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment().reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "447b31ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:17:30.728219Z",
     "start_time": "2022-07-25T13:17:30.722054Z"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d744e47d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:17:30.868486Z",
     "start_time": "2022-07-25T13:17:30.857520Z"
    }
   },
   "outputs": [],
   "source": [
    "...\n",
    "later = datetime.now()\n",
    "difference = (later - now).total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e23b00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:17:31.008639Z",
     "start_time": "2022-07-25T13:17:30.993665Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.136433"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb8f8243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:21:06.095336Z",
     "start_time": "2022-07-25T13:21:06.089864Z"
    }
   },
   "outputs": [],
   "source": [
    "d = {\"a\":1,\"b\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c178c728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T13:21:16.696756Z",
     "start_time": "2022-07-25T13:21:16.680754Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for key in d.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1355fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
