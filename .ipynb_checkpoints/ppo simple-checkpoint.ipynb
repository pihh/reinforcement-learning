{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410d1a66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T12:54:21.264700Z",
     "start_time": "2022-07-28T12:54:21.242134Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747276a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T13:38:52.240233Z",
     "start_time": "2022-07-28T13:11:58.517370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46725884 0.5327412 ]]\n",
      "[[0.50829583 0.49170417]]\n",
      "[[0.4786685  0.52133155]]\n",
      "[[0.47546464 0.52453536]]\n",
      "[[0.45757005 0.54243   ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 4ms/sample - loss: 0.4232\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 180us/sample - loss: 0.4173\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 269us/sample - loss: 0.4159\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 258us/sample - loss: 0.4165\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 250us/sample - loss: 0.4161\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 203us/sample - loss: 0.4156\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.4153\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 0.4150\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 1s 5ms/sample - loss: 66.8828\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 63us/sample - loss: 60.2359\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 63us/sample - loss: 60.1218\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 63us/sample - loss: 60.1217\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 55us/sample - loss: 60.1217\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 72us/sample - loss: 60.1217\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 55us/sample - loss: 60.1217\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 62us/sample - loss: 60.1217\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "iteration 0 total test reward=9.4\n",
      "best reward=9.4\n",
      "[[0.45254132 0.5474587 ]]\n",
      "[[0.44026005 0.55973995]]\n",
      "[[0.45195088 0.5480491 ]]\n",
      "[[0.42821008 0.5717899 ]]\n",
      "[[0.43172446 0.5682756 ]]\n",
      "[[0.43892437 0.5610756 ]]\n",
      "[[0.413731   0.58626896]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 203us/sample - loss: -0.0014\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 250us/sample - loss: -0.0011\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 250us/sample - loss: -0.0015\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 255us/sample - loss: -0.0021\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 141us/sample - loss: -0.0023\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 140us/sample - loss: -0.0023\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: -0.0023\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: -0.0023\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 50.9002\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 50.9002\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 50.9002\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 50.9002\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 50.9002\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 118us/sample - loss: 50.9002\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 106us/sample - loss: 50.9002\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 50.9002\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 8.0\n",
      "testing... 9.0\n",
      "iteration 1 total test reward=9.0\n",
      "[[0.4839703 0.5160297]]\n",
      "[[0.48064336 0.51935667]]\n",
      "[[0.4549743 0.5450257]]\n",
      "[[0.47601128 0.5239887 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 135us/sample - loss: -1.5591e-04\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: -1.6743e-04\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: -2.1135e-04\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: -3.4170e-04\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: -3.4533e-04\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: -2.6300e-04\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: -4.3341e-04\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: -3.5262e-04\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 119us/sample - loss: 103.3510\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 103.3510\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 103.3510\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 103.3510\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 62us/sample - loss: 103.3510\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 103.3510\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 62us/sample - loss: 103.3510\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 103.3510\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 8.0\n",
      "iteration 2 total test reward=9.0\n",
      "[[0.47060403 0.529396  ]]\n",
      "[[0.4252173  0.57478267]]\n",
      "[[0.43808162 0.56191844]]\n",
      "[[0.4708165  0.52918345]]\n",
      "[[0.4270545  0.57294554]]\n",
      "[[0.40583545 0.59416455]]\n",
      "[[0.41516915 0.5848309 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 158us/sample - loss: 8.5732e-04\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 8.2530e-04\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 3.2546e-04\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 3.2097e-04\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 3.5490e-04\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 129us/sample - loss: 2.0392e-05\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 1.1907e-04\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 127us/sample - loss: 2.6632e-04\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 148us/sample - loss: 45.7633\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 157us/sample - loss: 45.7633\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 132us/sample - loss: 45.7633\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 111us/sample - loss: 45.7633\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 45.7634\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 45.7634\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 45.7634\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 45.7634\n",
      "testing... 8.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "iteration 3 total test reward=9.0\n",
      "[[0.42547444 0.5745256 ]]\n",
      "[[0.42857948 0.5714205 ]]\n",
      "[[0.37333313 0.6266669 ]]\n",
      "[[0.4231698  0.57683027]]\n",
      "[[0.37667727 0.62332267]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: -9.3295e-04\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: -0.0015\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -0.0021\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: -0.0023\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: -0.0025\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -0.0025\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 103us/sample - loss: -0.0027\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: -0.0027\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 129us/sample - loss: 77.0454\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 77.0454\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 77.0454\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 77.0454\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 77.0454\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 77.0454\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 87us/sample - loss: 77.0454\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 88us/sample - loss: 77.0454\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "iteration 4 total test reward=9.4\n",
      "[[0.44867775 0.5513222 ]]\n",
      "[[0.45392308 0.54607695]]\n",
      "[[0.4541558 0.5458442]]\n",
      "[[0.43436328 0.56563675]]\n",
      "[[0.44673595 0.5532641 ]]\n",
      "[[0.44152856 0.55847144]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 144us/sample - loss: 6.9067e-06\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: -7.0034e-05\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -1.2909e-04\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -7.9035e-05\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 108us/sample - loss: -2.1428e-04\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: -2.2936e-04\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 110us/sample - loss: -3.3168e-04\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -3.1696e-04\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 140us/sample - loss: 55.5827\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 55.5827\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 55.5827\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 55.5827\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 55.5827\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 55.5827\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 55.5827\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 55.5827\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 8.0\n",
      "testing... 10.0\n",
      "iteration 5 total test reward=9.2\n",
      "[[0.3664512 0.6335488]]\n",
      "[[0.39189345 0.60810655]]\n",
      "[[0.384493 0.615507]]\n",
      "[[0.3490378  0.65096223]]\n",
      "[[0.38561454 0.6143855 ]]\n",
      "[[0.36371467 0.63628536]]\n",
      "[[0.3558859  0.64411414]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 141us/sample - loss: 0.0032\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0026\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 123us/sample - loss: 0.0019\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0020\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0017\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0012\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 113us/sample - loss: 0.0012\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0011\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 50.9555\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 50.9555\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 50.9555\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 50.9555\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 98us/sample - loss: 50.9555\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 88us/sample - loss: 50.9555\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 101us/sample - loss: 50.9555\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 50.9555\n",
      "testing... 8.0\n",
      "testing... 11.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "iteration 6 total test reward=9.8\n",
      "best reward=9.8\n",
      "[[0.31933206 0.680668  ]]\n",
      "[[0.3129194  0.68708056]]\n",
      "[[0.31572127 0.6842787 ]]\n",
      "[[0.3172277 0.6827723]]\n",
      "[[0.31730992 0.6826901 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 162us/sample - loss: 0.0014\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0049\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 7.1480e-04\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 111us/sample - loss: 0.0012\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0015\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0018\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 112us/sample - loss: 0.0022\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0012\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 139us/sample - loss: 85.8041\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 85.8041\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 88us/sample - loss: 85.8041\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 85.8041\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 85.8041\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 85.8041\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 85.8041\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 96us/sample - loss: 85.8041\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 8.0\n",
      "iteration 7 total test reward=9.4\n",
      "[[0.27575475 0.72424525]]\n",
      "[[0.26205128 0.73794866]]\n",
      "[[0.26052174 0.7394783 ]]\n",
      "[[0.27709857 0.72290146]]\n",
      "[[0.28302148 0.71697855]]\n",
      "[[0.2576483 0.7423517]]\n",
      "[[0.24760617 0.75239384]]\n",
      "[[0.275466 0.724534]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 169us/sample - loss: 0.0092\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 118us/sample - loss: 0.0080\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 153us/sample - loss: 0.0081\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 172us/sample - loss: 0.0063\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 210us/sample - loss: 0.0065\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 168us/sample - loss: 0.0067\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 219us/sample - loss: 0.0079\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 172us/sample - loss: 0.0068\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 113us/sample - loss: 42.1668\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 62us/sample - loss: 42.1668\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 54us/sample - loss: 42.1668\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 55us/sample - loss: 42.1668\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 55us/sample - loss: 42.1668\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 63us/sample - loss: 42.1668\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 63us/sample - loss: 42.1668\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 42.1668\n",
      "testing... 8.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "iteration 8 total test reward=9.0\n",
      "[[0.22820045 0.7717995 ]]\n",
      "[[0.2201701  0.77982986]]\n",
      "[[0.220365  0.7796351]]\n",
      "[[0.22046779 0.7795322 ]]\n",
      "[[0.2286574 0.7713426]]\n",
      "[[0.22560363 0.77439636]]\n",
      "[[0.22815003 0.77185   ]]\n",
      "[[0.22601904 0.7739809 ]]\n",
      "[[0.23365512 0.7663449 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 197us/sample - loss: 0.0040\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 195us/sample - loss: 0.0038\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 164us/sample - loss: 0.0033\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 144us/sample - loss: 0.0044\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 219us/sample - loss: 0.0031\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 195us/sample - loss: 0.0036\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 185us/sample - loss: 0.0034\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 141us/sample - loss: 0.0034\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 33.5888\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 102us/sample - loss: 33.5888\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 111us/sample - loss: 33.5888\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 33.5888\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 84us/sample - loss: 33.5888\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 104us/sample - loss: 33.5888\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 33.5888\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 33.5888\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "iteration 9 total test reward=9.8\n",
      "[[0.22054735 0.7794526 ]]\n",
      "[[0.21794099 0.7820591 ]]\n",
      "[[0.2230806  0.77691936]]\n",
      "[[0.21656728 0.7834327 ]]\n",
      "[[0.22021762 0.77978235]]\n",
      "[[0.21673045 0.7832695 ]]\n",
      "[[0.2237165 0.7762835]]\n",
      "[[0.22034445 0.7796556 ]]\n",
      "[[0.22079174 0.7792083 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 143us/sample - loss: 0.0069\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0078\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0059\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0060\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 106us/sample - loss: 0.0064\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 101us/sample - loss: 0.0062\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 108us/sample - loss: 0.0053\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 108us/sample - loss: 0.0072\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 138us/sample - loss: 32.7368\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 32.7368\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 32.7368\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 32.7368\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 32.7368\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 32.7368\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 32.7368\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 32.7368\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "iteration 10 total test reward=9.0\n",
      "[[0.18585497 0.81414497]]\n",
      "[[0.18687369 0.8131263 ]]\n",
      "[[0.18715937 0.81284064]]\n",
      "[[0.18676005 0.81323993]]\n",
      "[[0.1868084 0.8131916]]\n",
      "[[0.18662113 0.8133789 ]]\n",
      "[[0.18845941 0.8115406 ]]\n",
      "[[0.18538958 0.8146104 ]]\n",
      "[[0.1868653 0.8131347]]\n",
      "[[0.18816157 0.81183845]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 157us/sample - loss: 0.0022\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0032\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.0033\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0020\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 107us/sample - loss: 0.0013\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0028\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0034\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0022\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 122us/sample - loss: 33.3156\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 33.3156\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 33.3156\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 33.3156\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 33.3156\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 33.3156\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 89us/sample - loss: 33.3156\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 87us/sample - loss: 33.3156\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "iteration 11 total test reward=9.2\n",
      "[[0.17592788 0.8240722 ]]\n",
      "[[0.17195338 0.8280467 ]]\n",
      "[[0.17295782 0.8270422 ]]\n",
      "[[0.17774005 0.82225996]]\n",
      "[[0.17513812 0.8248619 ]]\n",
      "[[0.17751734 0.8224827 ]]\n",
      "[[0.17558919 0.82441086]]\n",
      "[[0.17223811 0.82776195]]\n",
      "[[0.17220758 0.8277924 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0132\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0088\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0089\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.0087\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 105us/sample - loss: 0.0090\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 106us/sample - loss: 0.0099\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.0085\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0084\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 35.2141\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 35.2141\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 35.2141\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 35.2141\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 35.2141\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 99us/sample - loss: 35.2141\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 82us/sample - loss: 35.2141\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 35.2141\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 8.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "iteration 12 total test reward=9.4\n",
      "[[0.15251715 0.84748286]]\n",
      "[[0.15076281 0.8492372 ]]\n",
      "[[0.154268   0.84573203]]\n",
      "[[0.1521264 0.8478736]]\n",
      "[[0.15394583 0.84605414]]\n",
      "[[0.15217346 0.84782654]]\n",
      "[[0.15225121 0.84774876]]\n",
      "[[0.15166609 0.8483339 ]]\n",
      "[[0.15146947 0.8485306 ]]\n",
      "[[0.15351179 0.84648824]]\n",
      "[[0.15169117 0.84830886]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 147us/sample - loss: -0.0013\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: -0.0012\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: -0.0010\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 113us/sample - loss: -0.0012\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 111us/sample - loss: -0.0020\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -2.3719e-04\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -0.0013\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: -0.0017\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 160us/sample - loss: 23.4870\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 23.4870\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 101us/sample - loss: 23.4870\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 23.4870\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 23.4870\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 23.4870\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 23.4870\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 23.4870\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "iteration 13 total test reward=9.6\n",
      "[[0.17003909 0.82996094]]\n",
      "[[0.16861744 0.8313826 ]]\n",
      "[[0.16976044 0.8302396 ]]\n",
      "[[0.16920733 0.83079267]]\n",
      "[[0.17021075 0.8297892 ]]\n",
      "[[0.16986193 0.830138  ]]\n",
      "[[0.16949745 0.8305025 ]]\n",
      "[[0.17056316 0.82943684]]\n",
      "[[0.17090191 0.8290981 ]]\n",
      "[[0.17049825 0.8295018 ]]\n",
      "[[0.1703739 0.8296261]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 172us/sample - loss: -1.8772e-04\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: -7.9692e-04\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 7.5132e-04\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -4.5132e-04\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: -3.2204e-04\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 4.0121e-04\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 107us/sample - loss: -1.6672e-04\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -2.5184e-04\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 135us/sample - loss: 24.6637\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.6637\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.6637\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.6637\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.6637\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.6637\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.6637\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 89us/sample - loss: 24.6637\n",
      "testing... 8.0\n",
      "testing... 9.0\n",
      "testing... 8.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "iteration 14 total test reward=8.8\n",
      "[[0.14705852 0.8529415 ]]\n",
      "[[0.14266448 0.8573355 ]]\n",
      "[[0.14386694 0.8561331 ]]\n",
      "[[0.14408208 0.855918  ]]\n",
      "[[0.14283158 0.85716844]]\n",
      "[[0.14541829 0.8545817 ]]\n",
      "[[0.1441481  0.85585195]]\n",
      "[[0.14645019 0.8535498 ]]\n",
      "[[0.14541022 0.85458976]]\n",
      "[[0.14486532 0.8551347 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 156us/sample - loss: 0.0075\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0068\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0076\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 113us/sample - loss: 0.0071\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0063\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.0059\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.0071\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0056\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 136us/sample - loss: 32.4189\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 32.4189\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 32.4189\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 32.4189\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 84us/sample - loss: 32.4189\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 32.4189\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 32.4189\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 32.4189\n",
      "testing... 8.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "iteration 15 total test reward=9.4\n",
      "[[0.12614508 0.87385494]]\n",
      "[[0.12710933 0.8728907 ]]\n",
      "[[0.12710321 0.8728968 ]]\n",
      "[[0.12546258 0.87453735]]\n",
      "[[0.12517941 0.8748206 ]]\n",
      "[[0.12708746 0.8729126 ]]\n",
      "[[0.12513459 0.87486535]]\n",
      "[[0.12494525 0.8750547 ]]\n",
      "[[0.12518354 0.8748165 ]]\n",
      "[[0.12620473 0.87379533]]\n",
      "[[0.12406227 0.87593776]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 156us/sample - loss: 0.0043\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0058\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 141us/sample - loss: 0.0032\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0040\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 130us/sample - loss: 0.0032\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0034\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 126us/sample - loss: 0.0028\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0035\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 124us/sample - loss: 23.6898\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 23.6898\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 23.6898\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 23.6898\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 23.6898\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 23.6898\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 70us/sample - loss: 23.6898\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 23.6898\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 8.0\n",
      "testing... 9.0\n",
      "iteration 16 total test reward=9.2\n",
      "[[0.10308966 0.89691037]]\n",
      "[[0.1015261 0.8984739]]\n",
      "[[0.10025099 0.899749  ]]\n",
      "[[0.1050088 0.8949913]]\n",
      "[[0.10189286 0.8981071 ]]\n",
      "[[0.10478243 0.8952176 ]]\n",
      "[[0.10322542 0.8967746 ]]\n",
      "[[0.10470721 0.8952928 ]]\n",
      "[[0.10801866 0.8919813 ]]\n",
      "[[0.10076215 0.8992379 ]]\n",
      "[[0.10653967 0.89346033]]\n",
      "[[0.10609975 0.8939002 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 156us/sample - loss: 0.0107\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0106\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0116\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 108us/sample - loss: 0.0102\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0081\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0084\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 107us/sample - loss: 0.0087\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.0096\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 142us/sample - loss: 21.5706\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 21.5706\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 21.5706\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 21.5706\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 85us/sample - loss: 21.5706\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 21.5706\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 21.5706\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 21.5706\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 11.0\n",
      "testing... 10.0\n",
      "iteration 17 total test reward=10.2\n",
      "best reward=10.2\n",
      "[[0.08758182 0.9124181 ]]\n",
      "[[0.08510575 0.9148943 ]]\n",
      "[[0.08450707 0.91549295]]\n",
      "[[0.08614127 0.9138587 ]]\n",
      "[[0.08451551 0.9154845 ]]\n",
      "[[0.085753 0.914247]]\n",
      "[[0.08596022 0.91403973]]\n",
      "[[0.08459524 0.91540474]]\n",
      "[[0.0853084  0.91469157]]\n",
      "[[0.08441885 0.91558117]]\n",
      "[[0.08550492 0.9144951 ]]\n",
      "[[0.08418323 0.9158168 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 157us/sample - loss: 0.0036\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 118us/sample - loss: 0.0047\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0038\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 107us/sample - loss: 0.0046\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0065\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 115us/sample - loss: 0.0032\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 107us/sample - loss: 0.0092\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 0.0021\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 145us/sample - loss: 19.6861\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 19.6861\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 0s 86us/sample - loss: 19.6861\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 19.6861\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 19.6861\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 87us/sample - loss: 19.6861\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 19.6861\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 82us/sample - loss: 19.6861\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "iteration 18 total test reward=9.8\n",
      "[[0.07885139 0.92114854]]\n",
      "[[0.07918247 0.9208175 ]]\n",
      "[[0.07888367 0.9211163 ]]\n",
      "[[0.08084106 0.9191589 ]]\n",
      "[[0.07901436 0.92098564]]\n",
      "[[0.07977837 0.9202216 ]]\n",
      "[[0.07984465 0.92015535]]\n",
      "[[0.07903548 0.92096454]]\n",
      "[[0.07916529 0.9208347 ]]\n",
      "[[0.07887442 0.9211256 ]]\n",
      "[[0.08146118 0.9185388 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 160us/sample - loss: 0.0067\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0089\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0059\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0047\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0052\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0048\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0058\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0061\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 134us/sample - loss: 23.6229\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 23.6229\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 23.6229\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 23.6229\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 96us/sample - loss: 23.6229\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 23.6229\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 23.6229\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 87us/sample - loss: 23.6229\n",
      "testing... 10.0\n",
      "testing... 11.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "iteration 19 total test reward=9.8\n",
      "[[0.07839615 0.92160386]]\n",
      "[[0.07849512 0.92150486]]\n",
      "[[0.07866615 0.9213338 ]]\n",
      "[[0.07857714 0.9214229 ]]\n",
      "[[0.07878108 0.9212189 ]]\n",
      "[[0.0788587  0.92114127]]\n",
      "[[0.07805534 0.9219447 ]]\n",
      "[[0.07848214 0.9215178 ]]\n",
      "[[0.07829117 0.92170876]]\n",
      "[[0.07882704 0.9211729 ]]\n",
      "[[0.07869775 0.92130226]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 159us/sample - loss: -0.0042\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -0.0039\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: -0.0019\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 107us/sample - loss: -0.0030\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -0.0043\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -0.0043\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 141us/sample - loss: -0.0043\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: -0.0046\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 25.2567\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 25.2567\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 25.2567\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 78us/sample - loss: 25.2567\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 84us/sample - loss: 25.2567\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 25.2567\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 88us/sample - loss: 25.2567\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 25.2567\n",
      "testing... 8.0\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 8.0\n",
      "iteration 20 total test reward=8.8\n",
      "[[0.08374263 0.91625744]]\n",
      "[[0.08396753 0.9160325 ]]\n",
      "[[0.08370103 0.916299  ]]\n",
      "[[0.0833118 0.9166882]]\n",
      "[[0.08398956 0.9160104 ]]\n",
      "[[0.08409184 0.91590816]]\n",
      "[[0.08368357 0.9163165 ]]\n",
      "[[0.0839369  0.91606313]]\n",
      "[[0.08407648 0.9159235 ]]\n",
      "[[0.0841666 0.9158334]]\n",
      "[[0.08372179 0.91627824]]\n",
      "[[0.08367922 0.9163208 ]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 158us/sample - loss: 0.0022\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0017\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0015\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0017\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0019\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0020\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0018\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 0.0018\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 109us/sample - loss: 20.2103\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 20.2103\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 20.2103\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 20.2103\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 20.2103\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 20.2103\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 20.2103\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 20.2103\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 8.0\n",
      "iteration 21 total test reward=9.6\n",
      "[[0.08313361 0.9168664 ]]\n",
      "[[0.08216566 0.9178344 ]]\n",
      "[[0.08301196 0.9169881 ]]\n",
      "[[0.082073   0.91792697]]\n",
      "[[0.08305372 0.9169463 ]]\n",
      "[[0.08253667 0.91746336]]\n",
      "[[0.08290569 0.91709435]]\n",
      "[[0.08288396 0.91711605]]\n",
      "[[0.08284584 0.91715413]]\n",
      "[[0.08336349 0.91663647]]\n",
      "[[0.08303883 0.91696113]]\n",
      "[[0.08182375 0.91817623]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 188us/sample - loss: 0.0083\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0070\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 141us/sample - loss: 0.0068\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0061\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0070\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0061\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0060\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0078\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 134us/sample - loss: 24.0148\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.0148\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 94us/sample - loss: 24.0148\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 24.0148\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.0148\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.0148\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 24.0148\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.0148\n",
      "testing... 9.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 8.0\n",
      "testing... 10.0\n",
      "iteration 22 total test reward=9.2\n",
      "[[0.06597725 0.9340228 ]]\n",
      "[[0.06482065 0.9351793 ]]\n",
      "[[0.06538787 0.9346121 ]]\n",
      "[[0.0668031 0.9331969]]\n",
      "[[0.0652891 0.9347109]]\n",
      "[[0.06588219 0.93411785]]\n",
      "[[0.06559776 0.9344022 ]]\n",
      "[[0.06604936 0.9339506 ]]\n",
      "[[0.06633272 0.93366724]]\n",
      "[[0.06610427 0.9338957 ]]\n",
      "[[0.0669378 0.9330622]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 173us/sample - loss: 0.0100\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0122\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 133us/sample - loss: 0.0098\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 117us/sample - loss: 0.0116\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0096\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 122us/sample - loss: 0.0106\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 125us/sample - loss: 0.0100\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 121us/sample - loss: 0.0097\n",
      "\n",
      "critic loss\n",
      "Train on 128 samples\n",
      "Epoch 1/8\n",
      "128/128 [==============================] - 0s 166us/sample - loss: 24.2346\n",
      "Epoch 2/8\n",
      "128/128 [==============================] - 0s 88us/sample - loss: 24.2346\n",
      "Epoch 3/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 24.2346\n",
      "Epoch 4/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 24.2346\n",
      "Epoch 5/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.2346\n",
      "Epoch 6/8\n",
      "128/128 [==============================] - 0s 86us/sample - loss: 24.2346\n",
      "Epoch 7/8\n",
      "128/128 [==============================] - 0s 102us/sample - loss: 24.2346\n",
      "Epoch 8/8\n",
      "128/128 [==============================] - 0s 103us/sample - loss: 24.2346\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 10.0\n",
      "testing... 9.0\n",
      "testing... 9.0\n",
      "iteration 23 total test reward=9.6\n",
      "[[0.05440584 0.9455942 ]]\n",
      "[[0.0550032 0.9449968]]\n",
      "[[0.05367869 0.94632125]]\n",
      "[[0.05449719 0.9455028 ]]\n",
      "[[0.0537268 0.9462732]]\n",
      "[[0.05426041 0.94573957]]\n",
      "[[0.05511243 0.9448876 ]]\n",
      "[[0.05443823 0.94556177]]\n",
      "[[0.05497428 0.94502574]]\n",
      "[[0.05372342 0.94627655]]\n",
      "[[0.05499101 0.94500905]]\n",
      "[[0.05374043 0.94625956]]\n",
      "\n",
      "actor loss\n",
      "Train on 128 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 218>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m actor_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_actor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensor_board\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcritic loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_v1.py:777\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    776\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:641\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    638\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    639\u001b[0m   val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 641\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:210\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Configure callbacks.\u001b[39;00m\n\u001b[0;32m    209\u001b[0m count_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_steps \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 210\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m \u001b[43mcbks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure_callbacks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples_or_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcount_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcount_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Find beforehand arrays that need sparse-to-dense conversion.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_steps:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\callbacks.py:100\u001b[0m, in \u001b[0;36mconfigure_callbacks\u001b[1;34m(callbacks, model, do_validation, batch_size, epochs, steps_per_epoch, samples, verbose, count_mode, mode)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# Set callback model\u001b[39;00m\n\u001b[0;32m     99\u001b[0m callback_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_get_callback_model()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m \u001b[43mcallback_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallback_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m set_callback_parameters(\n\u001b[0;32m    103\u001b[0m     callback_list,\n\u001b[0;32m    104\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    110\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m    113\u001b[0m callback_list\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\callbacks.py:287\u001b[0m, in \u001b[0;36mCallbackList.set_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    285\u001b[0m   model\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m--> 287\u001b[0m   \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\callbacks_v1.py:228\u001b[0m, in \u001b[0;36mTensorBoard.set_model\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m\"\"\"Sets Keras model and creates summary ops.\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# histogram summaries only enabled in graph mode\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ai_4\\lib\\site-packages\\keras\\callbacks_v1.py:165\u001b[0m, in \u001b[0;36mTensorBoard._init_writer\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    163\u001b[0m       tf\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mgraph(backend\u001b[38;5;241m.\u001b[39mget_graph())\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_graph:\n\u001b[1;32m--> 165\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFileWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39msummary\u001b[38;5;241m.\u001b[39mFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py:422\u001b[0m, in \u001b[0;36mFileWriter.__init__\u001b[1;34m(self, logdir, graph, max_queue, flush_secs, graph_def, filename_suffix, session)\u001b[0m\n\u001b[0;32m    418\u001b[0m   event_writer \u001b[38;5;241m=\u001b[39m EventFileWriter(logdir, max_queue, flush_secs,\n\u001b[0;32m    419\u001b[0m                                  filename_suffix)\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mFileWriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevent_writer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py:80\u001b[0m, in \u001b[0;36mSummaryToEventTransformer.__init__\u001b[1;34m(self, event_writer, graph, graph_def)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session_run_tags \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m graph_def \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m   \u001b[38;5;66;03m# Calling it with both graph and graph_def for backward compatibility.\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m   \u001b[38;5;66;03m# Also export the meta_graph_def in this case.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m   \u001b[38;5;66;03m# graph may itself be a graph_def due to positional arguments\u001b[39;00m\n\u001b[0;32m     83\u001b[0m   maybe_graph_as_def \u001b[38;5;241m=\u001b[39m (graph\u001b[38;5;241m.\u001b[39mas_graph_def(add_shapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(graph, ops\u001b[38;5;241m.\u001b[39mGraph) \u001b[38;5;28;01melse\u001b[39;00m graph)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py:210\u001b[0m, in \u001b[0;36mSummaryToEventTransformer.add_graph\u001b[1;34m(self, graph, global_step, graph_def)\u001b[0m\n\u001b[0;32m    207\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe passed graph must be an instance of `Graph` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor the deprecated `GraphDef`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Finally, add the graph_def to the summary writer.\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_add_graph_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_graph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\summary\\writer\\writer.py:156\u001b[0m, in \u001b[0;36mSummaryToEventTransformer._add_graph_def\u001b[1;34m(self, graph_def, global_step)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_add_graph_def\u001b[39m(\u001b[38;5;28mself\u001b[39m, graph_def, global_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    155\u001b[0m   graph_bytes \u001b[38;5;241m=\u001b[39m graph_def\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m--> 156\u001b[0m   event \u001b[38;5;241m=\u001b[39m \u001b[43mevent_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEvent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_event(event, global_step)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import gfootball.env as football_env\n",
    "# import numpy as np\n",
    "\n",
    "# import gym\n",
    "# import tensorflow as tf\n",
    "# from keras.callbacks import TensorBoard\n",
    "# from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten\n",
    "# from keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from keras import backend as K\n",
    "# from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "# from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "# disable_eager_execution()\n",
    "# clipping_val = 0.2\n",
    "# critic_discount = 0.5\n",
    "# entropy_beta = 0.001\n",
    "# gamma = 0.99\n",
    "# lmbda = 0.95\n",
    "\n",
    "\n",
    "# def get_advantages(values, masks, rewards):\n",
    "#     returns = []\n",
    "#     gae = 0\n",
    "#     for i in reversed(range(len(rewards))):\n",
    "#         delta = rewards[i] + gamma * values[i + 1] * masks[i] - values[i]\n",
    "#         gae = delta + gamma * lmbda * masks[i] * gae\n",
    "#         returns.insert(0, gae + values[i])\n",
    "\n",
    "#     adv = np.array(returns) - values[:-1]\n",
    "#     return returns, (adv - np.mean(adv)) / (np.std(adv) + 1e-10)\n",
    "\n",
    "\n",
    "# def ppo_loss_print(oldpolicy_probs, advantages, rewards, values):\n",
    "#     def loss(y_true, y_pred):\n",
    "#         y_true = tf.Print(y_true, [y_true], 'y_true: ')\n",
    "#         y_pred = tf.Print(y_pred, [y_pred], 'y_pred: ')\n",
    "#         newpolicy_probs = y_pred\n",
    "#         # newpolicy_probs = y_true * y_pred\n",
    "#         newpolicy_probs = tf.Print(newpolicy_probs, [newpolicy_probs], 'new policy probs: ')\n",
    "\n",
    "#         ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))\n",
    "#         ratio = tf.Print(ratio, [ratio], 'ratio: ')\n",
    "#         p1 = ratio * advantages\n",
    "#         p2 = K.clip(ratio, min_value=1 - clipping_val, max_value=1 + clipping_val) * advantages\n",
    "#         actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "#         actor_loss = tf.Print(actor_loss, [actor_loss], 'actor_loss: ')\n",
    "#         critic_loss = K.mean(K.square(rewards - values))\n",
    "#         critic_loss = tf.Print(critic_loss, [critic_loss], 'critic_loss: ')\n",
    "#         term_a = critic_discount * critic_loss\n",
    "#         term_a = tf.Print(term_a, [term_a], 'term_a: ')\n",
    "#         term_b_2 = K.log(newpolicy_probs + 1e-10)\n",
    "#         term_b_2 = tf.Print(term_b_2, [term_b_2], 'term_b_2: ')\n",
    "#         term_b = entropy_beta * K.mean(-(newpolicy_probs * term_b_2))\n",
    "#         term_b = tf.Print(term_b, [term_b], 'term_b: ')\n",
    "#         total_loss = term_a + actor_loss - term_b\n",
    "#         total_loss = tf.Print(total_loss, [total_loss], 'total_loss: ')\n",
    "#         return total_loss\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def ppo_loss(oldpolicy_probs, advantages, rewards, values):\n",
    "#     def loss(y_true, y_pred):\n",
    "#         newpolicy_probs = y_pred\n",
    "#         ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(oldpolicy_probs + 1e-10))\n",
    "#         p1 = ratio * advantages\n",
    "#         p2 = K.clip(ratio, min_value=1 - clipping_val, max_value=1 + clipping_val) * advantages\n",
    "#         actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "#         critic_loss = K.mean(K.square(rewards - values))\n",
    "#         total_loss = critic_discount * critic_loss + actor_loss - entropy_beta * K.mean(\n",
    "#             -(newpolicy_probs * K.log(newpolicy_probs + 1e-10)))\n",
    "#         return total_loss\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def get_model_actor_image(input_dims, output_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "#     oldpolicy_probs = Input(shape=(1, output_dims,))\n",
    "#     advantages = Input(shape=(1, 1,))\n",
    "#     rewards = Input(shape=(1, 1,))\n",
    "#     values = Input(shape=(1, 1,))\n",
    "\n",
    "#     feature_extractor = MobileNetV2(include_top=False, weights='imagenet')\n",
    "\n",
    "#     for layer in feature_extractor.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Flatten(name='flatten')(feature_extractor(state_input))\n",
    "#     x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "#     out_actions = Dense(n_actions, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input, oldpolicy_probs, advantages, rewards, values],\n",
    "#                   outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-4), loss=[ppo_loss(\n",
    "#         oldpolicy_probs=oldpolicy_probs,\n",
    "#         advantages=advantages,\n",
    "#         rewards=rewards,\n",
    "#         values=values)])\n",
    "#     model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def get_model_actor_simple(input_dims, output_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "#     oldpolicy_probs = Input(shape=(1, output_dims,))\n",
    "#     advantages = Input(shape=(1, 1,))\n",
    "#     rewards = Input(shape=(1, 1,))\n",
    "#     values = Input(shape=(1, 1,))\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Dense(128, activation='relu', name='fc1')(state_input)\n",
    "#     x = Dense(128, activation='relu', name='fc2')(x)\n",
    "#     out_actions = Dense(n_actions, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input, oldpolicy_probs, advantages, rewards, values],\n",
    "#                   outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-2), loss=[ppo_loss(\n",
    "#         oldpolicy_probs=oldpolicy_probs,\n",
    "#         advantages=advantages,\n",
    "#         rewards=rewards,\n",
    "#         values=values)])\n",
    "#     # model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def get_model_critic_image(input_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "\n",
    "#     feature_extractor = MobileNetV2(include_top=False, weights='imagenet')\n",
    "\n",
    "#     for layer in feature_extractor.layers:\n",
    "#         layer.trainable = False\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Flatten(name='flatten')(feature_extractor(state_input))\n",
    "#     x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "#     out_actions = Dense(1, activation=Nonw)(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input], outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-2), loss='mse')\n",
    "#     model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def get_model_critic_simple(input_dims):\n",
    "#     state_input = Input(shape=input_dims)\n",
    "\n",
    "#     # Classification block\n",
    "#     x = Dense(128, activation='relu', name='fc1')(state_input)\n",
    "#     x = Dense(128, activation='relu', name='fc2')(x)\n",
    "#     out_actions = Dense(1, activation='tanh')(x)\n",
    "\n",
    "#     model = Model(inputs=[state_input], outputs=[out_actions])\n",
    "#     model.compile(optimizer=Adam(lr=1e-2), loss='mse')\n",
    "#     # model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def test_reward():\n",
    "#     state = env.reset()\n",
    "#     done = False\n",
    "#     total_reward = 0\n",
    "   \n",
    "#     limit = 0\n",
    "#     while not done:\n",
    "#         state_input = K.expand_dims(state, 0)\n",
    "#         action_probs = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "#         action = np.argmax(action_probs)\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "#         #print('test reward',reward)\n",
    "#         limit += 1\n",
    "#         if limit > 20:\n",
    "#             break\n",
    "#     print('testing...', total_reward)\n",
    "#     return total_reward\n",
    "\n",
    "\n",
    "# def one_hot_encoding(probs):\n",
    "#     one_hot = np.zeros_like(probs)\n",
    "#     one_hot[:, np.argmax(probs, axis=1)] = 1\n",
    "#     return one_hot\n",
    "\n",
    "\n",
    "# image_based = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# env = gym.make('CartPole-v1')\n",
    "    \n",
    "# state = env.reset()\n",
    "# state_dims = env.observation_space.shape\n",
    "# n_actions = env.action_space.n\n",
    "\n",
    "# dummy_n = np.zeros((1, 1, n_actions))\n",
    "# dummy_1 = np.zeros((1, 1, 1))\n",
    "\n",
    "# tensor_board = TensorBoard(log_dir='./logs')\n",
    "\n",
    "# if image_based:\n",
    "#     model_actor = get_model_actor_image(input_dims=state_dims, output_dims=n_actions)\n",
    "#     model_critic = get_model_critic_image(input_dims=state_dims)\n",
    "# else:\n",
    "#     model_actor = get_model_actor_simple(input_dims=state_dims, output_dims=n_actions)\n",
    "#     model_critic = get_model_critic_simple(input_dims=state_dims)\n",
    "\n",
    "# ppo_steps = 128\n",
    "# target_reached = False\n",
    "# best_reward = 0\n",
    "# iters = 0\n",
    "# max_iters = 50\n",
    "\n",
    "# while not target_reached and iters < max_iters:\n",
    "\n",
    "#     states = []\n",
    "#     actions = []\n",
    "#     values = []\n",
    "#     masks = []\n",
    "#     rewards = []\n",
    "#     actions_probs = []\n",
    "#     actions_onehot = []\n",
    "#     state_input = None\n",
    "\n",
    "#     for itr in range(ppo_steps):\n",
    "#         state_input = K.expand_dims(state, 0)\n",
    "#         action_dist = model_actor.predict([state_input, dummy_n, dummy_1, dummy_1, dummy_1], steps=1)\n",
    "#         q_value = model_critic.predict([state_input], steps=1)\n",
    "#         action = np.random.choice(n_actions, p=action_dist[0, :])\n",
    "#         action_onehot = np.zeros(n_actions)\n",
    "#         action_onehot[action] = 1\n",
    "        \n",
    "        \n",
    "\n",
    "#         observation, reward, done, info = env.step(action)\n",
    "#         #print('itr: ' + str(itr) + ', action=' + str(action) + ', reward=' + str(reward) + ', q val=' + str(q_value))\n",
    "#         mask = not done\n",
    "\n",
    "#         states.append(state)\n",
    "#         actions.append(action)\n",
    "#         actions_onehot.append(action_onehot)\n",
    "#         values.append(q_value)\n",
    "#         masks.append(mask)\n",
    "#         rewards.append(reward)\n",
    "#         actions_probs.append(action_dist)\n",
    "\n",
    "#         state = observation\n",
    "#         if done:\n",
    "#             print(action_dist)\n",
    "#             env.reset()\n",
    "\n",
    "#     q_value = model_critic.predict(state_input, steps=1)\n",
    "#     values.append(q_value)\n",
    "#     returns, advantages = get_advantages(values, masks, rewards)\n",
    "\n",
    "\n",
    "#     X_actor = [states, actions_probs, advantages, np.reshape(rewards, newshape=(-1, 1, 1)), values[:-1]]\n",
    "#     y_actor = [np.reshape(actions_onehot, newshape=(-1, n_actions))]\n",
    "\n",
    "#     X_critic = [states]\n",
    "#     y_critic = [np.reshape(returns, newshape=(-1, 1))]\n",
    " \n",
    "#     print()\n",
    "#     print('actor loss')\n",
    "#     actor_loss = model_actor.fit(X_actor,y_actor, verbose=True, shuffle=True, epochs=8,callbacks=[tensor_board])\n",
    "#     print()\n",
    "#     print('critic loss')\n",
    "#     critic_loss = model_critic.fit(X_critic, y_critic, shuffle=True, epochs=8,verbose=True, callbacks=[tensor_board])\n",
    "\n",
    "#     avg_reward = np.mean([test_reward() for _ in range(5)])\n",
    "#     print('iteration', iters, 'total test reward=' + str(avg_reward))\n",
    "#     if avg_reward > best_reward:\n",
    "#         print('best reward=' + str(avg_reward))\n",
    "#         model_actor.save('model_actor_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "#         model_critic.save('model_critic_{}_{}.hdf5'.format(iters, avg_reward))\n",
    "#         best_reward = avg_reward\n",
    "#     if best_reward > 450 or iters > max_iters:\n",
    "#         target_reached = True\n",
    "#     iters += 1\n",
    "#     env.reset()\n",
    "\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52843701",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T12:59:28.072288Z",
     "start_time": "2022-07-28T12:59:28.072288Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "# gym.make('CartPole-v1').action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4195588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T12:59:28.073288Z",
     "start_time": "2022-07-28T12:59:28.073288Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import gfootball.env as football_env\n",
    "\n",
    "# env = football_env.create_environment(env_name='academy_empty_goal', render=False)\n",
    "\n",
    "# state = env.reset()\n",
    "# total_reward = 0\n",
    "# while True:\n",
    "#     observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#     total_reward += reward\n",
    "#     if reward != 0:\n",
    "#         print('reward',reward)\n",
    "#     if done:\n",
    "#         env.reset()\n",
    "#         print('done')\n",
    "# #         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650ebf8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T12:59:28.074289Z",
     "start_time": "2022-07-28T12:59:28.074289Z"
    }
   },
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86ef5aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T05:25:11.504662Z",
     "start_time": "2022-08-09T05:25:11.487827Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()\n",
    "\n",
    "from src.environments.continuous.pendulum import environment\n",
    "from src.agents.ppo import PpoAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd56e30d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-09T05:25:12.186854Z",
     "start_time": "2022-08-09T05:25:11.974663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    | ---------------------------------\n",
      "    | Pendulum-v1\n",
      "    | \n",
      "    | Action space: Continuous with low state-space\n",
      "    | Environment beated threshold: -200\n",
      "    | ----------------------------------------------------------   \n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "agent = PpoAgent(environment,batch_size=2000, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
