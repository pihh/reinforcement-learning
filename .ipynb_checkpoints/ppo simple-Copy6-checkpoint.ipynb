{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "602e667f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T06:21:09.031392Z",
     "start_time": "2022-08-03T06:21:09.011393Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutup\n",
    "shutup.please()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca80c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T06:21:09.125394Z",
     "start_time": "2022-08-03T06:21:09.032393Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c0862e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T18:45:48.741797Z",
     "start_time": "2022-08-02T18:45:48.550797Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb52acf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-03T06:21:13.061764Z",
     "start_time": "2022-08-03T06:21:09.126395Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import random\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, Adagrad, Adadelta\n",
    "from tensorflow.keras import backend as K\n",
    "import copy\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "\n",
    "# Helpers\n",
    "\n",
    "# gaussian_likelihood - ver se consigo uma função global\n",
    "# Keras loss\n",
    "# def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "#     log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "#     pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "#     return K.sum(pre_sum, axis=1)\n",
    "#\n",
    "# # Agent\n",
    "# def gaussian_likelihood(self, action, pred, log_std):\n",
    "#     # https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py\n",
    "#     pre_sum = -0.5 * (((action-pred)/(np.exp(log_std)+1e-8))**2 + 2*log_std + np.log(2*np.pi))\n",
    "#     return np.sum(pre_sum, axis=1)\n",
    "\n",
    "\n",
    "# Continuous\n",
    "class PpoActorContinuous:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer,loss_clipping = 0.2):\n",
    "\n",
    "        self.loss_clipping = loss_clipping\n",
    "\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"tanh\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(learning_rate=lr))\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        advantages, actions, logp_old_ph, = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space]\n",
    "\n",
    "        logp = self.gaussian_likelihood(actions, y_pred)\n",
    "\n",
    "        ratio = K.exp(logp - logp_old_ph)\n",
    "\n",
    "        p1 = ratio * advantages\n",
    "        p2 = tf.where(advantages > 0, (1.0 + self.loss_clipping)*advantages, (1.0 - self.loss_clipping)*advantages) # minimum advantage\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        return actor_loss\n",
    "\n",
    "    def gaussian_likelihood(self, actions, pred): # for keras custom loss\n",
    "        log_std = -0.5 * np.ones(self.action_space, dtype=np.float32)\n",
    "        pre_sum = -0.5 * (((actions-pred)/(K.exp(log_std)+1e-8))**2 + 2*log_std + K.log(2*np.pi))\n",
    "        return K.sum(pre_sum, axis=1)\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "# Discrete\n",
    "class PpoActorDiscrete:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer,loss_clipping=0.2,loss_entropy=0.001):\n",
    "\n",
    "        self.loss_clipping = loss_clipping\n",
    "        self.loss_entropy = loss_entropy\n",
    "\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(learning_rate=lr))\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        #advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        advantages,  actions, prediction_picks = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "\n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = K.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = K.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = K.exp(K.log(prob) - K.log(old_prob))\n",
    "\n",
    "        p1 = ratio * advantages\n",
    "        p2 = K.clip(ratio, min_value=1 - self.loss_clipping, max_value=1 + self.loss_clipping) * advantages\n",
    "\n",
    "        actor_loss = -K.mean(K.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * K.log(y_pred + 1e-10))\n",
    "        entropy = self.loss_entropy * K.mean(entropy)\n",
    "\n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)\n",
    "\n",
    "\n",
    "# PPO Critic for discrete or continuous only differs in the initializer\n",
    "class PpoCritic:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer,loss_clipping=0.2,kernel_initializer=False,continuous=False):\n",
    "\n",
    "        self.loss_clipping = loss_clipping\n",
    "\n",
    "        if kernel_initializer == False:\n",
    "            if continuous == False:\n",
    "                kernel_initializer = 'he_uniform'\n",
    "            else:\n",
    "                kernel_initializer=tf.random_normal_initializer(stddev=0.01)\n",
    "\n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        V = Dense(512, activation=\"relu\", kernel_initializer=kernel_initializer)(X_input)\n",
    "        V = Dense(256, activation=\"relu\", kernel_initializer=kernel_initializer)(V)\n",
    "        V = Dense(64, activation=\"relu\", kernel_initializer=kernel_initializer)(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[self.ppo_loss(old_values)], optimizer=optimizer(learning_rate=lr))\n",
    "\n",
    "    def ppo_loss(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "\n",
    "            clipped_value_loss = values + K.clip(y_pred - values, -self.loss_clipping, self.loss_clipping)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "\n",
    "            value_loss = 0.5 * K.mean(K.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = K.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])\n",
    "\n",
    "\n",
    "# PPO PPOAgent\n",
    "\n",
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name, training_batch=1024, epochs=10, episodes=1000, continuous=False):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name\n",
    "        self.env = gym.make(env_name)\n",
    "        if continuous:\n",
    "            self.action_size = self.env.action_space.shape[0]\n",
    "        else:\n",
    "            self.action_size = self.env.action_space.n\n",
    "            \n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = episodes # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = 0.001\n",
    "        self.epochs = epochs # training epochs\n",
    "        self.shuffle = True\n",
    "        self.Training_batch = training_batch\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "        self.replay_count = 0\n",
    "        self.continuous_action_space=continuous\n",
    "\n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        if continuous:\n",
    "            self.Actor= PpoActorContinuous(self.state_size, self.action_size, lr=self.lr, optimizer = self.optimizer,loss_clipping = 0.2)\n",
    "        else:\n",
    "            self.Actor= PpoActorDiscrete(self.state_size, self.action_size, lr=self.lr, optimizer = self.optimizer,loss_clipping=0.2,loss_entropy=0.001)\n",
    "\n",
    "        self.Critic = PpoCritic(self.state_size, self.action_size, lr=self.lr, optimizer = self.optimizer,loss_clipping=0.2,kernel_initializer=False,continuous=continuous)\n",
    "\n",
    "        # do not change bellow\n",
    "        self.log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "        self.std = np.exp(self.log_std)\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.continuous_action_space:\n",
    "            # Use the network to predict the next action to take, using the model\n",
    "            prediction = self.Actor.predict(state)\n",
    "\n",
    "            low, high = -1.0, 1.0 # -1 and 1 are boundaries of tanh\n",
    "            action = prediction + np.random.uniform(low, high, size=prediction.shape) * self.std\n",
    "            action = np.clip(action, low, high)\n",
    "\n",
    "            logp_t = self.gaussian_likelihood(action, prediction, self.log_std)\n",
    "\n",
    "            return action[0], action , logp_t[0]\n",
    "        else:\n",
    "            prediction = self.Actor.predict(state)[0]\n",
    "            action = np.random.choice(self.action_size, p=prediction)\n",
    "            action_onehot = np.zeros([self.action_size])\n",
    "            action_onehot[action] = 1\n",
    "            return action, action_onehot, prediction\n",
    "\n",
    "\n",
    "    def run_batch(self):\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score = False, 0\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            #states, next_states, actions, rewards, logp_ts , dones = [], [], [], [], [], []    #C\n",
    "            #states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], [] #D\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "\n",
    "            for t in range(self.Training_batch):\n",
    "                #self.env.render()\n",
    "                # Actor picks an action\n",
    "                #action, logp_t = self.act(state)                        #C\n",
    "                #action, action_onehot, prediction = self.act(state)     #D\n",
    "                action, action_data, prediction = self.act(state)\n",
    "\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                # next_state, reward, done, _ = self.env.step(action[0])  #C\n",
    "                # next_state, reward, done, _ = self.env.step(action)     #D\n",
    "                next_state, reward, done, _ = self.env.step(action) \n",
    "\n",
    "                # Memorize (state, next_states, action, reward, done, logp_ts) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                # actions.append(action)          #C\n",
    "                # actions.append(action_onehot)   #D\n",
    "                actions.append(action_data)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                # logp_ts.append(logp_t[0])       #C\n",
    "                # predictions.append(prediction)  #D\n",
    "                predictions.append(prediction)\n",
    "\n",
    "                # Update current state shape\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average = self.checkpoint(score, self.episode)\n",
    "                    if str(self.episode)[-2:] == \"00\":\n",
    "                        print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, ''))\n",
    "                    state, done, score = self.env.reset(), False, 0\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            # self.replay(states, actions, rewards, dones, next_states, logp_ts)      #C\n",
    "            # self.replay(states, actions, rewards, predictions, dones, next_states)  #D\n",
    "            self.replay(states, actions, rewards, dones, next_states, predictions)\n",
    "\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def replay(self, states, actions, rewards, dones, next_states, predictions):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Get Critic network predictions\n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "\n",
    "        # stack everything to numpy array\n",
    "        # pack all advantages, predictions and actions to y_true and when they are received\n",
    "        # in custom loss function we unpack it\n",
    "        # y_true = np.hstack([advantages, actions, predictions]) #C\n",
    "        # y_true = np.hstack([advantages, predictions, actions]) #D\n",
    "        y_true = np.hstack([advantages, actions, predictions])\n",
    "\n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "\n",
    "        # calculate loss parameters (should be done in loss, but couldn't find working way how to do that with disabled eager execution)\n",
    "        # pred = self.Actor.predict(states)\n",
    "        # log_std = -0.5 * np.ones(self.action_size, dtype=np.float32)\n",
    "        # logp = self.gaussian_likelihood(actions, pred, log_std)\n",
    "        # approx_kl = np.mean(predictions - logp)\n",
    "        # approx_ent = np.mean(-logp)\n",
    "\n",
    "        self.replay_count += 1\n",
    "\n",
    "    ### Equal fns\n",
    "    def gaussian_likelihood(self, action, pred, log_std):\n",
    "        # for continuous only\n",
    "        # https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/sac/policies.py\n",
    "        pre_sum = -0.5 * (((action-pred)/(np.exp(log_std)+1e-8))**2 + 2*log_std + np.log(2*np.pi))\n",
    "        return np.sum(pre_sum, axis=1)\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.90, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def load(self):\n",
    "#         self.Actor.Actor.load_weights(self.Actor_name)\n",
    "#         self.Critic.Critic.load_weights(self.Critic_name)\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "#         self.Actor.Actor.save_weights(self.Actor_name)\n",
    "#         self.Critic.Critic.save_weights(self.Critic_name)\n",
    "        pass\n",
    "\n",
    "    def checkpoint(self, score, episode):\n",
    "        self.scores_.append(score)\n",
    "        self.episodes_.append(episode)\n",
    "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n",
    "        saving = False\n",
    "        # saving best models\n",
    "        if self.average_[-1] >= self.max_average:\n",
    "            self.max_average = self.average_[-1]\n",
    "            self.save()\n",
    "            # decreaate learning rate every saved model\n",
    "            self.lr *= 0.95\n",
    "            K.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
    "            K.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
    "            saving = True\n",
    "\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            # Do some logging\n",
    "            pass\n",
    "\n",
    "        return self.average_[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e1c27a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-03T06:21:09.013Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 100/3000, score: 56.0, average: 53.92 \n",
      "episode: 200/3000, score: 130.0, average: 157.42 \n",
      "episode: 300/3000, score: 114.0, average: 105.08 \n",
      "episode: 400/3000, score: 101.0, average: 99.30 \n",
      "episode: 500/3000, score: 94.0, average: 82.38 \n",
      "episode: 600/3000, score: 98.0, average: 86.54 \n",
      "episode: 700/3000, score: 96.0, average: 89.78 \n",
      "episode: 800/3000, score: 101.0, average: 80.92 \n",
      "episode: 900/3000, score: 94.0, average: 88.36 \n",
      "episode: 1000/3000, score: 95.0, average: 88.64 \n",
      "episode: 1100/3000, score: 90.0, average: 89.24 \n",
      "episode: 1200/3000, score: 93.0, average: 91.10 \n",
      "episode: 1300/3000, score: 105.0, average: 92.12 \n",
      "episode: 1400/3000, score: 104.0, average: 99.78 \n",
      "episode: 1500/3000, score: 135.0, average: 107.36 \n",
      "episode: 1600/3000, score: 128.0, average: 125.70 \n",
      "episode: 1700/3000, score: 122.0, average: 143.04 \n",
      "episode: 1800/3000, score: 185.0, average: 254.36 \n",
      "episode: 1900/3000, score: 155.0, average: 190.76 \n",
      "episode: 2000/3000, score: 141.0, average: 177.94 \n",
      "episode: 2100/3000, score: 82.0, average: 138.82 \n",
      "episode: 2200/3000, score: 92.0, average: 135.80 \n"
     ]
    }
   ],
   "source": [
    "discrete_agent = PPOAgent('CartPole-v1', training_batch=1024, epochs=10, episodes=3000, continuous=False)\n",
    "discrete_agent.run_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a1a7d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-03T06:21:09.014Z"
    }
   },
   "outputs": [],
   "source": [
    "continuous_agent = PPOAgent('InvertedPendulumBulletEnv-v0',training_batch=512, epochs=5,episodes=3000, continuous=True)\n",
    "continuous_agent.run_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd120d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
